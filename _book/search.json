[
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "3  What Is Statistical Learning?",
    "section": "",
    "text": "Question: How to improve sales of our product?\nWe have a data set:\n\nadvertising = read_csv(\"./data/Advertising.csv\") %&gt;% as_tibble %&gt;% select(-1)\n\nNew names:\nRows: 200 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(5): ...1, TV, radio, newspaper, sales\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nadvertising\n\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n\n\n\\(n=200\\), independent variables (predictors) are TV, radio, and newspaper advertising spendings in thousands of dollars. We want to explore their relationship with sales; quantity of product sold for each advertising mixture. If we determine association between advertising and sales, we can provide adjustment of advertisement budgeds based on most effective media to increase sales; we want to develop an accurate model that ca be used to predict sales on the basis of three media budgets.\nWe denote all input variables (actual–realized) as \\(X_1, X_2, ..., X_p\\) and use \\(X\\) to refer all of them. In this case \\(X = (X_1, X_2, X_3)\\). Sales is denoted with \\(Y\\).\nThis means we assume a relationship between \\(Y\\) and \\(X\\) in a form of\n\\[\nY = f(X) + \\epsilon\n\\] (2.1)\n\nHere \\(f\\) is some fixed, but unknown function of \\(X\\).\n\\(\\epsilon\\) is a random error term =&gt; independent of \\(X\\) and has a mean zero.\n\nSo \\(f\\) represents systematic information that \\(X\\) provides about \\(Y\\).\n\\(f\\) is generally unknown. We will need to estimate \\(f\\) baed on the observed points =&gt; \\(\\hat{f}\\)."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Chapter2.html#why-estimate-f",
    "href": "Chapter2.html#why-estimate-f",
    "title": "3  What Is Statistical Learning?",
    "section": "3.1 Why estimate \\(f\\)?",
    "text": "3.1 Why estimate \\(f\\)?\nTwo reasons: * prediction * inference\nPrediction\nMost of the time we have \\(X\\) but we might not have \\(Y\\). In this setting, since the error term averages to zero, we can predict \\(Y\\) using\n\\[\n\\hat{Y} = \\hat{f}X\n\\] (2.2)\nHere \\(\\hat{f}\\) is treated as a black box. We are not concerned with the exact form of \\(\\hat{f}\\), we just want to have accurate predictions of \\(Y\\).\nImagine we have \\(X = (X_1, X_2, \\dots, X_p)\\); blood sample characteristics of patients. \\(Y\\) is a variable showing the patient’s risk for a adverse reaction to a drug. We don’t want to give the drug and see the reaction, so we want to predict reactions.\nThe accuracy of our predictions \\(\\hat{Y}\\) of \\(Y\\), depends on two quantities:\n\nreducible error\nGenerally \\(\\hat{f}\\) will not be a perfect estimate for \\(f\\). This inaccuracy will introduce some error, which we call reducible error since we can improve our accuracy of \\(\\hat{f}\\) using the most appropriate statistical leraning method.\nirreducible error\nEven if we estimate \\(f\\) perfectly, our estimated response would take the form \\(\\hat{Y} = f(X)\\); our predictions would still get some error. This is because \\(Y\\) is not just a function of \\(X\\) but also a function of \\(\\epsilon\\), which cannot be predicted by \\(X\\). So the level of \\(\\epsilon\\) would also effect our prediciton accuracy. And we cannot remove this error; thus, irreducible.\n\\(\\epsilon\\) is larger than zero; because \\(\\epsilon\\) may contain some variables we don’t include in our model, but effect \\(Y\\).\n\nInference\nHere we want to understand the way that \\(Y\\) is affected by \\(X\\). In this setting, we wish to estimate \\(f\\) but we are not concerned with predicting. We want to understand the relationship between \\(X\\) and \\(Y\\); how \\(Y\\) changes as \\(X\\) changes. We don’t treate \\(\\hat{f}\\) as a black box now since we need to know its exact form. In this setting we are interested in answering questions such as\n\nWhich predictors are associated with the response?\nUsually not all predictors are associated with \\(Y\\). We need to identify the important predictors among a large set of possible predictors.\nWhat is the relationship between the response and each predictor?\nSome predictors have positive some negative association with \\(Y\\). Depending on the complexity of \\(f\\), the relationship between \\(Y\\) and \\(X_i\\) may also depend on the values of other predictors(\\(X_j\\)) =&gt; synergy\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\nSometimes we are interested with prediction: Identifying individuals who will respond positively to a mailing, based on observations of demographic variables. Here we are not interested with understanding the relationship of demographic variables and response, we just want an accurate model to predict the response using the predictors. This is prediction.\nBut often we are interested to answer questions like: Which media contribute to sales?, Which media generate the biggest boost in sales?, or How much increase in sales is associated with a given increase in TV advertising?. This is inference.\nAnd sometimes we want a combination of both: Values of homes based on crime rate, zoning, distance from a river, air quality, schools, size of houses etc. and How does air quality effect valeus of homes?.\nWe use different models for prediction, inference, or combination of the two.\n\n3.1.1 How Do We Estimate \\(f\\)?\nThere are many linear and non-linaer approaches we will discuss. But generally these models share certain characteristics. Here are they:\n\nWe will always assume that we have observed a set of n different data points. These data points, observations, are called training data; which we will use these observations to train, or teach, our model on how to estimate \\(f\\). Our training data will consist of \\(\\{(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\\}\\), where \\(x_i = (x_{i1}, x_{i2}, \\dots, x_{ip})^T\\)\n\nWe want to apply a statistical learning method to the training data to estimate the unknown function \\(f\\). We want to find a function \\(\\hat{f}\\) such that \\(Y \\approx \\hat{f}(X)\\) for any obsrvation \\((X,Y)\\).\nThese statistical learning methods can be charactarized as either parametric or non-parametric.\nParametric Methods\nParametric methods involve a two step model-based approach:\n\nSelect a model =&gt; Make an assumption about the functional form of \\(f\\): is it linear, non linear?\n\nFor example a linear \\(f\\) assumption would yield a linear model\n\\[\n  f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n  \\] (2.4)\n\nFit or train the model\n\nAfter we select a model, we need a procedure that uses training data to fit or train the model.\nFor linear model, we need to estimate the parameters of the model (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)). That is we want to find values of these parameters such that \\[\n  Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n  \\] The most common approach to fitting the model (2.4) is called ordinary least squares. Chapter3. But there are other approaches as well.\nThis model-based approach is called parametric: we estimate \\(f\\) via estimating a set of parameters.\nDisadvantage (potential): model we choose will usually not match the true unknown form of \\(f\\) =&gt; our estimates will be poor. =&gt; solution: choose a flexible model that can fit different possible functional forms for \\(f\\) =&gt; you will need to estimate more parametrs =&gt; overfitting the data.\nNon-parametric Methods\n=&gt; No explicit assumptions about the functional form of \\(f\\). The goal is to get an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly =&gt; advantage over parametric approach: no assumption about the functional form of \\(f\\)–potentially accurately fit a wider range of possible shapes for \\(f\\).\nDisadvantage =&gt; lots of parameters to estimate =&gt; very large of observations required to obtain an accurate estimate for \\(f\\).\n\n\n3.1.2 The Trade-Off Between Prediction Accuracy and Model Interpretability\nSome models are flexible some restrictive; in the sense that they can produce just a small range of functional forms to estimate \\(f\\). Linear regression for instance is a relatively inflexible approach. Other metgods such as thin plate splines (non-parametric) are more flexible because they can generate a much wider range of possible functional forms to estimate \\(f\\).\nWhy would be ever choose to use a more restrictive method instead of a very flexible approach? :\n\nIf we are mainly interested in inference, restrictive models are more interpretable. They give more information about each predictors effect on predicted.\nIf we are mainly interested in prediction, flexible models give better fit. =&gt; but may yield less accurate fits due to overfitting!\n\n\n\n3.1.3 Supervised vs Unsupervised Learning\nMost statistical learning problems fall into these two categories: supervised or unsupervised.\nIn supervised learning for each observation of the predictor values \\(x_i, i = 1,\\dots, n\\) there is an associated response value \\(y_i\\). We wish to fit a model that relates the response to the predictors with the aims of either accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Linear regression, GAM, boosting, support vector machines operate in the supervised learning domain.\nUnsupervised leraning describes a situtaion in which for every observation \\(i=1,\\dots,n\\) we obser a vector of values \\(x_i\\) but no associated response \\(y_i\\). We cannot use a linear regression model since we dont have \\(y_i\\) values. Here we can seek to understand the relationships between the variables or between the observations; like cluster analysis, or clustering: to assert on the basis of \\(x_i,\\dots,x_n\\) whether the observations fall into relatively distinct groups."
  },
  {
    "objectID": "Chapter2.html#regression-vs-classisfication-problems",
    "href": "Chapter2.html#regression-vs-classisfication-problems",
    "title": "3  What Is Statistical Learning?",
    "section": "3.2 Regression vs Classisfication Problems",
    "text": "3.2 Regression vs Classisfication Problems\nVariables can be characterized as either quantitative or qualitative(also known as categorical). Quantitative varaibles take on numberical values: a person’s age, height, or income, the value of a house, price of stock. Qualitative varaibles take on values n one of \\(K\\) different classes, or categories: aperson’s gender(male or female), the brand of a good (A,B, or C), a person’s race etc.\nWe refer to problems with a quantitative response as regression problems, and probmes with a qualitative response as classification problems. However, the distinction is not clear-cut.\nLeast squares regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative response. Some statistical methods, such as K-nearest negihbors and boosting, can be used in the case of either quantitative or qualitative.\nWe usually select statistical learning methods based on whether the response is quantitative or qualitative: we might use linear regression wjen quantitative and logistic regression when qualitative. But whether the predictors are qualitative or quantitative is usually not that important. Most of the statistical learning methods can be applied regardless of the predictor varible type."
  },
  {
    "objectID": "Chapter2.html#assessing-model-accuracy",
    "href": "Chapter2.html#assessing-model-accuracy",
    "title": "3  What Is Statistical Learning?",
    "section": "3.3 Assessing Model Accuracy",
    "text": "3.3 Assessing Model Accuracy\nThere is no one method that dominates all others over all possible data sets. On a particular data set, one metghod may work best, but some other method may work better on a similar but differet data set. So it is important to assess the model accuracies of the methods.\nHere some ways to asses the model accuracy\n\n3.3.1 Measuring the Quality of Fit\nSo, to evaluate the performance of a statistical learning method on a given data set, we need to measure how well its predictions actually match the observed data.\nIn the regression setting, the most commonly-used measure is the mean squared error (MSE), given by\n\\[\n\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{f}(x_i))^2\n\\]\nMSE will be small if the predicted responses are veryt close to the true responses, and large if predicted and true responses differ substantially on average.\nHere since MSE is computed using the training data it is best to refer it as training MSE. But in general, we do not really care how well the method works on the training data =&gt; we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.\nImagine: stock price prediction =&gt; we have training and test data =&gt; we already know the stock prices of the past, we dont care about the training data accuracy of the model, we want our model to predict the future prices of stocks best.\nOr we have blood characteristics of diabetes patients. We don’t want our model to explain our existing patient’s classification of diabetes or not, we want our model to predict our future patien’s situation the best.\nMathematically:\nWe fit our statistical learning method on our training observations \\(\\{(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\\}\\), and we obtain the estimate \\(\\hat{f}\\). We can then compute \\(\\hat{f}(x_1),\\dots, \\hat{f}(x_n)\\). If these are approximately equal to \\(y_1, \\dots, y_n\\) then our training MSE will be small. Howeer, we are not interested in whether \\(y_i \\approx \\hat{f}(x_i)\\), we want to know whether \\(\\hat{f}(x_0)\\) is approximately equal to \\(y_0\\), where \\((x_0,y_0)\\) is a previously unseen test obsrevation not used to train the statistical learning method.\nThat is, we want to choose the method that gives the lowest test MSE!.\nSo with our test data we can compute test MSE\n\\[\n\\text{MSE}_{test} =\\frac{1}{n_{test}} \\sum(y_{test_{i}} - \\hat{f}(X_{test_i}))\n\\] (2.6)\nWe want the test MSE to be small as possible. We can compute test MSE via (2.6) if we have test data for different models and select the model with minimum test MSE.\nIf we don’t have a test data, you might think our goal would be to minimize the training MSE since test and training data are colesly related. But no; minimal training MSE doesn’t guarantee minimal training MSE\nUsually as the level of flexibility increases, the curves fit the observed data more closely =&gt; lower training MSE. The level of flexibility is quantified by degrees of freedom. More restricted models have lower degrees of freedom. and usually the training MSE declines as flexibility increases.\n\nAs the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a \\(U\\)-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. AS model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learninig procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random change rather than by true properties of the unknown function \\(f\\). When we overfit the trainin data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.\nNote that regardless of whether or not overfitting has occured, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or inderectly seek to mimizie the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.\nIn practice, training MSE is computed easily, but estimating test MSE is hard because usually no test data are available. We will learn approaches that can be used in practice to estimate the mininmum test MSE. One important method is cross-validation( Chapter 5), which is a method for estimating test MSE using the training data.\n\n\n3.3.2 The Bias-Variance Trade-Off\nThe U-shape in the test MSE result of two competing properties of statistical learnig methods. The expected test MSE, for a given value \\(x_0\\) can always be decomposed into sum of variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\) and the variance of the error terms \\(\\epsilon\\). That is\nThis means that to minimize the expected test error, we need to simultaneously have low variance and low bias. Since variance is always bigger than zero; \\(\\text{Var}(\\epsilon)\\), and \\(\\text{Bias}(\\hat{f}(x_0))\\) are nonnegative. So, the expected test MSE can never lie below \\(\\text{Var}(\\epsilon)\\), the irreducible error from (2.3).\nWhat do we mean by the variance and bias of statistical learning method?\nVarince refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set; different training data sets will result in a different \\(\\hat{f}\\). But ideally, \\(\\hat{f}\\) should not vary too much between training sets. If a method has high variance small changes in the training data can result in large changes in \\(\\hat{f}\\).\nFlexible methods have hiher variance, because they fit better to the data points and changing any of the data points may cause the estiamte \\(\\hat{f}\\) to chance considerably. But for example, least squares method is relatively inflexible and has low variance, because mooving any single observation will cause only a small shift in the position of the line.(2.9)\nbias refers to the error that is due to functional form of \\(\\hat{f}\\). In real life, linear relationships are very rare. So performing linear regression will result in some bias in the estimate of \\(f\\). If your \\(f\\) is non-linear performing linear regression on different data sets will not produce an accurate estimate; so linear regression will result in high bias.\nFor example in 2.9 true \\(f\\) is non linear; so linear regression have high bias, low variance. \nIn 2.10 true \\(f\\) is very close to linear, so linaer regression have low bias, low variance.\nGenerally more flexible methods result in less bias.\nAs a general rule, as we use more flexible methods the variance will increase and bias will decrease. The relative rate of change of these two quantities determines whether test MSE increases or decreases. As we increase the flexibility, the bias tends to initially decrease faster than the variance increases =&gt; test MSE declines. However, at some point increasing flexibility has little impact on thebias but starts to significantly increase the variance =&gt; test MSE increases.\n\nFigure 2.12 shows bias and variance effect to the test MSE for different \\(f\\)s. Horizontal dashed line represents \\(\\text{Var}(\\epsilon)\\), the irreducible error; the red curve test MSE is the sum of squared bias, variance, and variance of irreducible error. In all cases bias decreases as flexibility increaes. However, the optimal flexibility is different for each \\(f\\). In the left panel the bias initially decreaes rapidly, decreasing test MSE. In center panel true \\(f\\) is closer to linear so there is only a small decrease in bias as flexibility increaes, and the test MSE only declines slightly before increasing rapidly as the variance increaes. Right hand panel, as flexibility increaess bias dramatically decreases because true \\(f\\) is very non-linear. There is alsso very little increase in variance as flexibility increases =&gt; tets MSE decreases before increasing.\nThis is called bias-variance trade off. Good test set performance of a method requires low variance as well as low squared bias. This is a trade off because it is easy to obtain a method with extremely low bias but high variance(for instance, drawing a curve that passes through every single training observation, or a method with low variance but high bias(by fitting a horizontal line to the data). Challange is finding a method wihch both the variance and squareed bias are low.\nIn real life, it is not possible to explicitly compute the test MSE, bias, or variance for methods. But we should keep this in mind.\n\n\n3.3.3 The Classification Setting\nSo far we focused on regression setting. Problems such as bias-variance trade of also occurs in classification but in a modificated way because \\(y_i\\) is no longer numerical.\nSuppose that we seek to estiamte \\(f\\) on the basis of training observations \\(\\{(x_1,y_1), \\dots, (x_n,y_n)\\}\\) where now \\(y_1,\\dots, y_n\\) are qualitative.\nWe need to quantify the accuracy of our estimate \\(\\hat{f}\\). We can use the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations:\n\\[\n\\text{error rate}=\\frac{1}{n}\\sum^n_{i=1}I(y_i \\neq \\hat{y_i})\n\\] (2.8)\n\\(\\hat{y_i}\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\). \\(I(y_i \\neq \\hat{y_i})\\) is an indicator variable that equals 1 if \\(y_i \\neq \\hat{y_i}\\) and zero if \\({y_i = \\hat{y_i}}\\). If \\(I(y_i \\neq \\hat{y_i}) = 0\\) then \\(i\\)th observation was classified correctly, otherwise it was misclassified. So (2.8) computes the fraction of incorrect classifications.\n(2.8) is training error. But as the regression setting we are more interested in test error rate. The test error rate associated with a set of test observations of the form\n\\[\n\\text{error rate}_{test} = \\frac{1}{n_{test}}\\sum_{i=1}^{n_{test}}(I(y_{test_i} \\neq \\hat{y}_{test_i}))\n\\] (2.9)\nA good classifier is one for which the test error is smallest.\nThe Bayes Classifier\nWe can minimize test error rate by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector \\(x_0\\) to the class \\(j\\) for which\n\\[\nPr(Y = j | X = x_0)\n\\] (2.10)\nis largest. This is conditional probability: it is the probabilty that \\(Y=j\\) given the observed predictor vector \\(x_0\\). This classifier is called Bayes classifier.\nIn a two-class problem where there are only two possible response values, class 1 or class2, the Bayes classifier corresponds to predicting class one if \\(Pr(Y=1 | X = x_0) &gt; 0.5\\), and class two otherwise.\nImagine having \\(X = (X_1, X_2)\\). For each value of \\(X_1\\) and \\(X_2\\) there will be a different probability of the response being class 1 or 2. For \\(Pr(Y=class1 | X = (X_2,X_2)) &gt; 0.5\\) and \\(Pr(Y=class2 | X = (X_1,X_2))\\).\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. Since the Bayes classifer will always choose the class for which \\(Pr(Y = j | X = x_0)\\) is largest, the error rate at \\(X=x_0\\) will be \\(1-\\max_jPr(Y =j | X = x_0)\\). In general, the overall Bayes error rate is given by\n\\[\n\\text{Bayes error rate} = 1 - E(\\max_jPr(Y =j | X))\n\\] (2.11)\nwhere the expectation averages the probabilty over all possible values of X. The Bayes eror rate is analogous to the irreducible eror.\nK-Nearest Neighbors\nIn theory we always want to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of \\(Y\\) given \\(X\\), and so computing the Bayes classfier is impossible. So Bayes classifier is like a gold standard to compare other methods.\nMany approaches attemp to estiamte the conditional distribution of \\(Y\\) given \\(X\\), and then classify a given observation to the class with highest estimated probability. One of them is K-nearest neighbors(KNN) classfier.\nGiven a positive integer K and a test observation \\(x_0\\), the KNN clasffier first identifies the K points in the training data that are closest to \\(x_0\\), represented by \\(N_0\\). It then esitmates the conditional probability for class j as the fraction points in \\(N_0\\) whose response values equal to \\(j\\).\n\\[\nPr(Y = j | X = x_0) = \\frac{1}{K}\\sum_{i \\in N_0}I(y_i=j)\n\\] (2.12)\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_0\\) to the class with the largest probability.\n\nFigure 2.14 provides an illustrative example of the KNN approach. Left panel =&gt; Our goal is to make prediction for the black cross point. When \\(K=3\\) KNN will identify the 3 oservations that are closest to the cross. There are two blue and one orange points; \\(Pr(Y=orange | X = x_{cross}) = 1/3\\), and \\(Pr(Y=blue | X = x_{cross}) = 2/3\\) =&gt; KNN will predict that the black cross belongs to the blue class.\nThe choice of K is very important. as K increases flexibility decreases =&gt; high bias, but low variance.\nJust like in regression setting there is not a strong relationship between the training error rate and the test error rate.\n\nas in the regression setting, the training error rate consistently declines as the flexibility(\\(1/K\\)) increases. However, the test error rate again have a characteristic U-shape.\nIn both regression and classification settings, choosing the correct level of flexibility is critical. The bias-variance tradeoff =&gt; U-shape in the test error, can make this a difficult task."
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "4  Linear Regression",
    "section": "",
    "text": "We will predict quantitative response.\n\nadvertising = read_csv(\"./data/Advertising.csv\") %&gt;% as_tibble %&gt;% select(-1)\nadvertising\n\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n\n\nWe are asked to suggest a marketing plan for next year which will yeild high product sales. We may want to inquire the following questions:\n\nIs there a relationship between advertising budget and sales?\nFirst we should determine if there is an association between adveritisng expenditure and sales. If not, no money shpuld be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nIf there is a relationship between advertising and sales, what is the strength of this relationship? Given a certain advertising budget, can we predict sales with a high level of accuracy? =&gt; strong relationsihp.\nWhich media contribute to sales?\nDo all variables–tv,radio,newspaper– contribute to sales, or just one or the two?\nHow accurately can we estimate the effect of each medium on sales?\nFor every ollar spent on advertising in a particular medium, by what amount will sales increase? How accuretly can we predict this amount of increase?\nHow accurately can we predict future sales?\nFor any given level of media advertisig, what is our prediction for sales, and what is the accuracy of this prediciton?\nIs the relationship linear?\nIf so linear regresion is appropriate tool, if not we may need to transform the predictor or the repsonse so that liner regression can be used.\nIs there synergy among the advertising media?\nDoes the effect of a medium on sales depend on other medium levels? Does dividing advertisement budget to two or three medium yeild a higher sales?\n\nWe can answer each of these questions using Linear regression."
  },
  {
    "objectID": "Chapter3.html#simple-linear-regression",
    "href": "Chapter3.html#simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.1 Simple Linear Regression",
    "text": "2.1 Simple Linear Regression\nPredicting a quantitative response \\(Y\\) on the basis of a single predictor variable \\(X\\).\nOur assumption is that there is approximately a linear relationship between \\(X\\) and \\(Y\\); we can write this linear relationship as\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\epsilon\n\\]\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\] (3.1)\nFor example lets say \\(X\\) is TV, and \\(Y\\) is sales\n\\[\nsales = \\beta_0 + \\beta_1 \\times TV + \\epsilon\n\\] or\n\\[\nsales = \\beta_0 + \\beta_1 \\times TV\n\\]\nOn (3.1) \\(\\beta_0\\) and \\(\\beta_1\\) are unknown constants that represent the intercept and slope in the linear model. Together they are known as coefficients or parameters.\nWe are going to use or training data to produce estimates for \\(\\beta_0\\) =&gt; \\(\\hat{\\beta_0}\\) and \\(\\beta_1\\) =&gt; \\(\\hat{\\beta_1}\\). Using these predicted coefficients we can predict sales;\n\\[\n\\hat{sales} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times TV\n\\]\nor as in general form\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\n\\] (3.2)\n\n2.1.1 Estimating the coefficients\nSince, \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, before we can use (3.1) to make predictions we must use data to estimate the coefficients. We have \\(n\\) observations :\n\\[\n(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\n\\]\nWe want our estimated coefficients to give such predictions that will fit the avaible data as well =&gt; \\(y_i \\approx \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\) for \\(i = 1,\\dots, n\\). This coefficients will allow us to draw a regression line and we want this regression line to be close as possible to the \\(n\\) data points we have.\nThere are different ways to measure closeness. The most common approach is minimizing the least squares criterion. Alternative approaches will be considered in Chapter 6.\nOur predictions come from \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\).\nThen for each data we have a residual: difference between \\(y\\) and \\(\\hat{y}\\):\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\nWe need to take the squares to get the distances–because of the negative residuals, and sum them to get the residual sum of squares(RSS)\n\\[\nRSS = e_1^2 + e_2^2 + \\dots + e_n^2\n\\]\nthis is equal to\n\\[\nRSS = (y_i - \\hat{beta_0} - \\hat{\\beta_1}x_1)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2) + \\dots + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1}x_n)\n\\] (3.3)\nThe least squares approach chooses \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize RSS. These minimizers are\n\\[\n\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y_i} - \\hat{\\beta_1}\\bar{x}\n\\end{align}\n\\] (3.4)\n\\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i\\) are the sample means.\nSo (3.4) defines the least squares coefficient estimates for simple linear regression.\nLets calculate them with R\n\nbeta_1_hat_adv = sum((advertising$TV - mean(advertising$TV)) * ((advertising$sales - mean(advertising$sales)))) / sum((advertising$TV - mean(advertising$TV))^2)\nbeta_1_hat_adv\n\n[1] 0.04753664\n\n\nSo; our $ = 0.0475 $\n\nbeta_0_hat_adv = mean(advertising$sales) - beta_1_hat_adv * mean(advertising$TV)\nbeta_0_hat_adv\n\n[1] 7.032594\n\n\nour \\(\\hat{\\beta_0} = 7.032\\)\nLets compare them with r function\n\nsummary(lm(sales ~ TV, data = advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nCalculating the predicted values\n\ny_hat_adv = beta_0_hat_adv + beta_1_hat_adv * advertising$TV\ny_hat_adv[1:10]\n\n [1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162  9.765950\n [8] 12.746498  7.441409 16.530414\n\n\nOur \\(y_i\\) values are as above.\n\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n\n[1] 2102.531\n\n\nOur \\(\\text{RSS} = 2102.531\\)\nSo we can draw our regression line\n\nadvertising %&gt;%\n  ggplot() + aes(x=TV, y = sales) +  geom_abline(intercept = beta_0_hat_adv, slope = beta_1_hat_adv, color = \"#262B70\", size =1.2) +\n  geom_segment(aes(xend=TV, yend=y_hat_adv), color = \"#939393\") +\n  geom_point(color = \"#AA1D2E\", size =2) + theme_par()\n\n\n\n\nFor the advertising data, the least squares fit for the regression of sales onto TV. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, adn the fit make a comprimise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot\n\n\n\n\nSo we have\n\\[\n\\hat{y_i} = 7.032 + 0.0475x_i\n\\]\nAccording to this approximation an additional $1,000 spent on TV increases sales by 47.5 units.\n\n\n2.1.2 Assessing the Accuracy of the Coefficient Estimates\nWe assumed that true relationship is linear: \\(Y = f(X) + \\epsilon\\). We don’t know \\(f\\), and \\(\\epsilon\\) is a mean-zero random error term.\nWe said \\(f\\) is approximatly linear, so that \\(f(X) = \\beta_0 + \\beta_1 X\\); which means\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\] (3.5)\nerror term captures: * the true relationship may not be linear * other variables that affect Y * measurement error\nand is independent of \\(X\\).\n(3.5) is the population regression line: the best linar approximation to the true relationship between \\(X\\) and \\(Y\\).\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}X\n\\] is the least squares line. They are different of course! But we don’t know the population regression line. If we did:\nFor example, lets create a data;\n\nFirst we create random x values from 100 random numbers\n\n\nseq(-2,2,length.out=100)[1:10]\n\n [1] -2.000000 -1.959596 -1.919192 -1.878788 -1.838384 -1.797980 -1.757576\n [8] -1.717172 -1.676768 -1.636364\n\n\n\nset.seed(11)\nx = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\nx[1:10]\n\n [1] -0.6666667  0.2222222 -1.0303030 -1.3939394 -0.5454545  0.3838384\n [7] -1.5555556  1.3939394  1.4343434  0.4646465\n\n\nlets define our \\(f\\)–population parameters \\(\\beta_0\\) and \\(\\beta_1\\)\n\\[\nf(X) = 2 + 3\\times X\n\\] Now lets create our \\(Y\\) values from this function but we also want to add random error values as well\n\nset.seed(11)\ny = 2 + 3*x + rnorm(100)\ny[1:10]\n\n [1] -0.5910311  2.6932610 -2.6074622 -3.5444715  1.5421255  2.2173638\n [7] -1.3430610  6.8067360  6.2573073  2.3898188\n\n\nSo we have a data set\n\ndata = tibble(\n  y = y, x = x\n)\n\nNow lets plot this data points\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() + theme_par()\n\n\n\n\nNow, even though we already know \\(f\\) and population parameters \\(\\beta_0 = 2\\) and \\(\\beta_1 = 3\\), lets estimate them:\n\nsummary(lm(y ~ x, data = data))\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.88077    0.09180   20.49   &lt;2e-16 ***\nx            3.05893    0.07608   40.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSo our least squares estimation is\n\\[\n\\hat{y_i} = 1.88 + 3.06 x_i\n\\] Lets draw this least squares regression line to our plot\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept=1.88, slope = 3.06, size = 1.2) + theme_par()\n\n\n\n\nWhat about the population regression line\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept = 1.9, slope = 3.06, size = 1.2) + geom_abline(intercept = 2, slope = 3, color =\"red\") + theme_par()\n\n\n\n\nThey are not the same! If we were to have another data from the same data generation process other estimates of parameters would result with different least squares regression lines:\n\nset.seed(111)\nx_r1 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r1 = 2 + 3*x_r1 + rnorm(100)\nset.seed(1111)\nx_r2 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r2 = 2 + 3*x_r2 + rnorm(100)\nset.seed(11111)\nx_r3 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r3 = 2 + 3*x_r3 + rnorm(100)\nset.seed(111111)\nx_r4 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r4 = 2 + 3*x_r4 + rnorm(100)\nset.seed(1111111)\nx_r5 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r5 = 2 + 3*x_r5 + rnorm(100)\nset.seed(11111111)\nx_r6 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r6 = 2 + 3*x_r6 + rnorm(100)\n\nLets now estimate population parameters for each of these data and plot them\n\ndata_r = tibble(\n  y_r1,x_r1,y_r2,x_r2,y_r3,x_r3,y_r4,x_r4, y_r5,x_r5,y_r6,x_r6\n)\ndata_r\n\n# A tibble: 100 × 12\n     y_r1   x_r1   y_r2    x_r2   y_r3   x_r3  y_r4   x_r4   y_r5    x_r5  y_r6\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1  5.99   1.11   0.807 -0.263   8.82   1.80  -3.05 -1.52   7.64   1.68   6.49 \n 2  6.53   1.35   3.74   0.141   6.23   1.92   5.11  1.27   8.71   2      7.75 \n 3  6.28   1.31   2.16  -0.0202 -0.272 -0.707  2.39 -0.101  4.59   0.909  1.21 \n 4  1.52  -0.141 -0.178 -0.990   5.45   0.788 -4.55 -1.84  -1.41  -1.11   0.478\n 5  0.708 -1.03  -0.464 -0.586   0.361 -0.343 -3.31 -1.15   9.59   1.88   2.64 \n 6  2.05   0.343  4.12   0.788  -1.09  -1.11  -5.13 -1.96   0.723 -0.586  4.10 \n 7  4.54   0.747  5.44   1.60    0.479 -0.707  2.27  0.182  2.29  -0.263  3.68 \n 8  1.69  -0.626  8.02   1.80   -2.23  -1.64   1.55  0.222 -1.45  -0.990  5.08 \n 9  2.84   0.869  1.02   0.384   4.04   0.949  3.34 -0.747 -2.21  -0.828  8.33 \n10 -0.933 -0.990  3.52   0.505  -0.471 -0.505 -1.07 -0.869  1.47   0.0606 2.45 \n# ℹ 90 more rows\n# ℹ 1 more variable: x_r6 &lt;dbl&gt;\n\n\n\ndata %&gt;% \n  ggplot() + aes(x,y) + geom_point(size = 0, color = \"white\")  + \n  geom_abline(intercept = lm(y_r1 ~ x_r1)$coefficients[1], slope = lm(y_r1 ~ x_r1)$coefficients[2], color =\"#29019F\") +\n  geom_abline(intercept = lm(y_r2 ~ x_r2)$coefficients[1], slope = lm(y_r2 ~ x_r2)$coefficients[2], color =\"#0A04BF\") +\n  geom_abline(intercept = lm(y_r3 ~ x_r3)$coefficients[1], slope = lm(y_r3 ~ x_r3)$coefficients[2], color =\"#0930DF\") +\n  geom_abline(intercept = lm(y_r4 ~ x_r4)$coefficients[1], slope = lm(y_r4 ~ x_r4)$coefficients[2], color =\"#0E6DFF\") +\n  geom_abline(intercept = lm(y_r5 ~ x_r5)$coefficients[1], slope = lm(y_r5 ~ x_r5)$coefficients[2], color =\"#2BA8FF\") +\n  geom_abline(intercept = lm(y_r6 ~ x_r6)$coefficients[1], slope = lm(y_r6 ~ x_r6)$coefficients[2], color =\"#48D9FF\") +\n  geom_abline(intercept =2, slope =3, color = \"red\") +\n  \n  theme_par()\n\n\n\n\nSo, different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.\nThis is because we are using a sample, and estimating characteristics of the population. Usually these characteristics are different, but generally sample characteristics will provide a good estimate to the population characteristics.\nComputing \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) from different sets of sample data provide different but similar results. And we are trying to estimate population parameters \\(\\beta_0\\) and \\(\\beta_1\\) with these. Some of these \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) will overestimate, some will underestimate \\(\\beta_0\\), and \\(\\beta_1\\). But if we could average all these estimated parameters and take the average, than this average should be equal to population parameters; if this is the case this estimator is called unbiased estimator. So an unbiased estimator does not systematically over- or under-estimate the true parameter.\nOkay but how close \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are to the true values \\(\\beta_0\\) and \\(\\beta_1\\). We want to compute the standard errors associated with \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). Standard error telss us the average amount of estimate differes from the actual value.\n\\[\n\\begin{align}\n\\text{SE}(\\hat{\\beta_0})^2 &= \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x}^2)}\\right] \\\\\n\\text{SE}(\\hat{\\beta_1})^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\end{align}\n\\] (3.8)\nWhere \\(\\sigma^2=\\text{Var}(\\epsilon)\\).\nNotice that formula of \\(\\text{SE}(\\hat{\\beta_1})\\) is smaller when \\(x_i\\) are more spread out; intutively we have more leverage to estimate a slope when this is the case.\nIn general \\(\\sigma^2\\) is not known, but can be estimated from the data. The estimate of \\(\\sigma\\) is known as the residual standard error, and given by the formula\n\\[\n\\text{RSE} = \\sqrt{\\text{RSS}/(n-2)}\n\\] So, when \\(\\sigma^2\\) is estimated fro mthe data we should write \\(\\hat{\\text{SE}}(\\hat{\\beta_1})\\) to indicate that an estimate has been made, but usually we drop this extra hat.\nStandard errors can be used to compute confidence intervals. A 95% confidence interval is defines as a range of values such that with 95% probability, the rage will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for \\(\\beta_1\\) approximately takes the form\n\\[\n\\hat{\\beta_1} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_1})\n\\] (3.9)\nSo there is approximately a 95% chance that the interval \\[[\\hat{\\beta_1} - 1.96 \\cdot \\text{SE}(\\hat{\\beta_1}), \\hat{\\beta_1} + 1.96 \\cdot \\text{SE}(\\hat{\\beta-1})]\\] (3.10) will contain the true value of \\(\\beta_1\\). Same is true for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_0})\n\\] (3.11)\nLets calculate the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) from our original data and model\n\\[\n\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\cdot TV\n\\] We first need to calculate RSS and RSE:\n\\[\n\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\n\\]\n\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n\n[1] 2102.531\n\n\n\\[\n\\text{RSE} = \\sigma = \\sqrt{RSS/(n-2)}\n\\]\n\nRSE = sqrt((RSS / (length(advertising$sales) -2)))\nRSE\n\n[1] 3.258656\n\n\nFor \\(\\beta_0\\)\n\\[\n\\text{SE}(\\hat{\\beta_0}) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]\n\\]\n\nse_beta_0_adv =  sqrt(RSE^2 * (1/length(advertising$sales) + (mean(advertising$TV)^2 / sum((advertising$TV - mean(advertising$TV))^2))))\nse_beta_0_adv\n\n[1] 0.4578429\n\n\nSo we can calculate the confidence interval for \\(\\beta_0\\)\n\ncat(\"In the absence of any advertising, sales will on average, fall somewhere between\",beta_0_hat_adv - 1.96 * se_beta_0_adv, \"and\", beta_0_hat_adv + 1.96 * se_beta_0_adv)\n\nIn the absence of any advertising, sales will on average, fall somewhere between 6.135221 and 7.929966\n\n\nFor \\(\\hat{\\beta_1}\\):\n\\[\n\\text{SE}(\\hat{\\beta_1})^2 =\\frac{\\sigma^2}{\\sum(x_i - \\bar{x})^2}\n\\]\n\nse_beta_1_adv = sqrt(RSE^2 / (sum((advertising$TV - mean(advertising$TV))^2)))\nse_beta_1_adv\n\n[1] 0.002690607\n\n\n\ncat(\"For each $1,000 increase in TV advertising, average increase in sales will be between\",(beta_1_hat_adv - 1.96 * se_beta_1_adv) * 1000, \"and\", (beta_1_hat_adv + 1.96 * se_beta_1_adv)*1000, \"by 95% confidence\")\n\nFor each $1,000 increase in TV advertising, average increase in sales will be between 42.26305 and 52.81023 by 95% confidence\n\n\nLets confirm our results\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nconfint(lm(sales ~ TV, advertising))\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\nSo standard errors of our estimated parameters tells us the average amount of difference from the true population parameters. And using the confidence intervals we can tell a range of the true population parameters’ interval with a percentage (usually 95%).\nLets do this for our data as well, which we know has the form\n\\[\ny_i = 2 + 3 x_i + \\epsilon_i\n\\] Lets calculate \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) first\n\nbeta_1_hat_data = sum((data$x - mean(data$x)) * (data$y - mean(data$y))) / sum((data$x - mean(data$x))^2)\nbeta_1_hat_data\n\n[1] 3.058925\n\n\n\nbeta_0_hat_data = mean(data$y) - beta_1_hat_data * mean(data$x)\nbeta_0_hat_data\n\n[1] 1.880772\n\n\n\\[\n\\hat{y_i} = 1.88 + 3.05 x_i\n\\]\nlets confirm this\n\nlm(y~x, data)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n      1.881        3.059  \n\n\nLets calculate the residual sum of squares, residual sum of errors, standard errors, and confidence intervals\n\ndata %&gt;% \n  mutate(\n    y_hat = beta_0_hat_data + beta_1_hat_data * x\n  ) -&gt; data\n\n\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n\n[1] 82.28514\n\n\n\nRSE_data = sqrt((RSS_data/(length(data$y) -2)))\nRSE_data\n\n[1] 0.9163211\n\n\nso \\(\\sigma_{data} = 0.9163\\)\nWe can now compute the standard errors of estiamed coefficients\n\nse_beta_0_data = sqrt(((1/length(data$y)) + (mean(data$x)^2 / sum((data$x - mean(data$x))^2))) * RSE_data^2)\nse_beta_0_data\n\n[1] 0.09179903\n\n\n\nse_beta_1_data = sqrt(RSE_data^2 / (sum((data$x - mean(data$x))^2)))\nse_beta_1_data\n\n[1] 0.07608457\n\n\nLets confirm\n\nsummary(lm(y~x,data))\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.88077    0.09180   20.49   &lt;2e-16 ***\nx            3.05893    0.07608   40.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSo we can say that on average our \\(\\hat{\\beta_0}\\)s are 0.091 differ from \\(\\beta_0\\), and our \\(\\hat{\\beta_1}\\)s differ 0.076 from \\(\\beta_1\\). To make more sense of it we can calculate the confidence intervals\n\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_0_hat_data - 1.96 * se_beta_0_data, \"and\", beta_0_hat_data + 1.96 * se_beta_0_data )\n\nBy 95% confidence we can say that the true beta_0 is between 1.700846 and 2.060698\n\n\n\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_1_hat_data - 1.96 * se_beta_1_data, \"and\", beta_1_hat_data + 1.96 * se_beta_1_data)\n\nBy 95% confidence we can say that the true beta_0 is between 2.909799 and 3.208051\n\n\nLets confirm this\n\nconfint(lm(y~x,data))\n\n               2.5 %   97.5 %\n(Intercept) 1.698600 2.062944\nx           2.907938 3.209912\n\n\nSince standard error tells us the range of the \\(\\beta\\) values via confidence interval, we can infer that if this range does not include 0, than our \\(\\beta\\) values are statistically significant; x is assocaited with y.\nOr we can use standard erros to perform hypothesis tests on the coefficients. We usually don’t care about the intercept, so lets do the hypothesis test on only \\(\\hat{\\beta_1}\\).\n\\[\n\\begin{align}\nH_0 &: \\beta_1 = 0 \\to \\text{there is no relationship between X and Y} \\\\\nH_1 &: \\beta_1 \\neq 0 \\to \\text{there is some relationship between X and Y}\n\\end{align}\n\\] If the null-hypothesis is true =&gt; $ _1 = 0$ =&gt; \\(Y = \\beta_0 + \\epsilon\\) =&gt; \\(X\\) is not associated with \\(Y\\).\nTo test the null-hypothessi, we need to determine whether our estimate \\(\\hat{\\beta_1}\\) is sufficiently far from zero that we can be confident that \\(\\beta_1\\) is non-zero. How far is enough? This depends on the accuracy of \\(\\hat{\\beta_1}\\)–that is it depends on \\(\\text{SE}(\\hat{\\beta_1})\\). If \\(\\text{SE}(\\hat{\\beta_1})\\) is small, then even relatively small values of \\(\\hat{\\beta_1}\\) may provide strong evidence that \\(\\beta_1 \\neq 0\\). If \\(\\text{SE}(\\hat{\\beta_1})\\) is large, then \\(\\hat{\\beta_1}\\) must be large in absolute value in order for us to reject the null hypothessis. In practice we compute a t-statistic given by\n\\[\nt = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}\n\\] (3.14)\nwhich measures the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from zero. From the t-statistic we can compute the p-value; a small p value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in absence of any real association between the predictor and the response. So if p value is small we infer that there is assocaition between the predictor and the response =&gt; we reject the null hypothesis. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1%. When \\(n=30\\) these correspond to tstatsitcs of around 2 and 2.75.\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nHere we see that t statistics are very high, and p values are very low =&gt; reject the null hypothesis for both \\(\\beta\\) values; they are statistically significant.\n\n\n2.1.3 Assessing the Accuracy of the Model\nOnce we concluded the statistically significant variable–rejecting the null hypothesis, we want to quantify the extend to which the model fits the data. We can use either\n\nResidual standard error\n\\(R^2\\)\n\nResidual standard error\nRecall from \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) that associated with each observation is an error term \\(\\epsilon\\). Because of these error terms even if we knew the true regression line, we would not be able to predict \\(Y\\) from \\(X\\). The RSE is an estimate of the standard deviation of \\(\\epsilon\\). It is the average amount that the response will deviate from the true regression line, computed by\n\\[\n\\begin{align}\n\\text{RSE} &= \\sqrt{\\frac{1}{n-2}\\text{RSS}} \\\\\n&= \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2}\n\\end{align}\n\\] (3.15)\nIn the advertising data, RSE was 3.26; actual sales in each market deviate from the true regression line by approximately 3,269 units, on average. This also means that; if the model were correct and the true values of the unknown coefficients \\(\\beta_0\\) and \\(\\beta_1\\) were known exaclty, any predcition of sales on the basis of TV advertising would still be off by about 3,260 units on average. Is this prediction error accaptable? Depends on the data: in the advertising data set the mean value of sales is \\(\\approx 14,000\\) units, and so the percentage error is \\(3,260 / 14,000 = 23%\\).\nThe RSE is considered a measure of the lack of fit of the model \\(Y=\\beta_0 + \\beta_1 + \\epsilon\\) to the data. If the predictions from the model are very close to the true outcome values–\\(\\hat{y_i} \\approx y_i\\) then RSE will be small, and we can concldue that the model fits the data very well. Otherwise, if \\(\\hat{y_i}\\) is very far from \\(y_i\\) then RSE may be quite large, indicating the model doesn’t fit the data well.\n\\(R^2\\)Statistic\nThe RSE provides an absolute measure of lack of fit of the model to the data. \\(R^2\\) provides an alternative measure of fit. It takes the form of a proportion–the proportion of variance explained–and so its always \\(0\\leq R^2 \\leq 1\\) and is independent of the scale of \\(Y\\)–as opposed to RSE.\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\] (3.17)\nwhere \\(\\text{TSS} = \\sum(y_i - \\bar{y})^2\\) is the total sun of squares and \\(\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\\). TSS measures the total variaance in the response \\(Y\\); and can be thought of as the amount of varaiblity ingerent in the response before the regression is performed. RSS measures the amount of varaiblity that is left unexplained after performing the regression. So TSS - RSS measures the amount of variability in the response that is explained by performing the regression, and \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\). As \\(R^2\\) gets closer to 1, a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variablity in the response; this might occur because the linear model is wrong, or the inherit error \\(\\sigma^2 = \\text{RSE}^2\\) is high, or both.\nLets calculate \\(R^2\\) of our estimation on advertising data with the model \\(\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}TV_i\\)\n\nadvertising %&gt;% \n  mutate(sales_hat = beta_0_hat_adv + beta_1_hat_adv * TV) -&gt; advertising\nadvertising\n\n# A tibble: 200 × 5\n      TV radio newspaper sales sales_hat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1     18.0 \n 2  44.5  39.3      45.1  10.4      9.15\n 3  17.2  45.9      69.3   9.3      7.85\n 4 152.   41.3      58.5  18.5     14.2 \n 5 181.   10.8      58.4  12.9     15.6 \n 6   8.7  48.9      75     7.2      7.45\n 7  57.5  32.8      23.5  11.8      9.77\n 8 120.   19.6      11.6  13.2     12.7 \n 9   8.6   2.1       1     4.8      7.44\n10 200.    2.6      21.2  10.6     16.5 \n# ℹ 190 more rows\n\n\n\nRSS = sum((advertising$sales - advertising$sales_hat)^2)\nRSS\n\n[1] 2102.531\n\n\n\nRSE = sqrt(RSS/(length(advertising$sales) -2))\nRSE\n\n[1] 3.258656\n\n\n\nTSS = sum((advertising$sales - mean(advertising$sales))^2)\nTSS\n\n[1] 5417.149\n\n\n\nR2 = (TSS - RSS) / TSS\nR2\n\n[1] 0.6118751\n\n\n61% of the variablity in sales is explained by a linear regression on TV.\nwhat about our data\n\ndata\n\n# A tibble: 100 × 3\n        y      x  y_hat\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.591 -0.667 -0.159\n 2  2.69   0.222  2.56 \n 3 -2.61  -1.03  -1.27 \n 4 -3.54  -1.39  -2.38 \n 5  1.54  -0.545  0.212\n 6  2.22   0.384  3.05 \n 7 -1.34  -1.56  -2.88 \n 8  6.81   1.39   6.14 \n 9  6.26   1.43   6.27 \n10  2.39   0.465  3.30 \n# ℹ 90 more rows\n\n\n\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n\n[1] 82.28514\n\n\n\nRSE_data = sqrt(RSS_data/(length(data$y) -2))\nRSE_data\n\n[1] 0.9163211\n\n\n\nTSS_data = sum((data$y - mean(data$y))^2)\nTSS_data\n\n[1] 1439.473\n\n\n\nR2_data = (TSS_data - RSS_data) / TSS_data\nR2_data\n\n[1] 0.9428366\n\n\n94% of the variablity in y is explained by x; very good fit of the model.\n\\(R^2\\) is better to interpret than RSE."
  },
  {
    "objectID": "Chapter3.html#multiple-linear-regression",
    "href": "Chapter3.html#multiple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\nIn practice we have more than one predictor to explain \\(Y\\).\nHow can we extend our analysis of the advertising order to accomodate the other two (radio and newspaper) additional predictors?\n=&gt; We can run three separate simple linear regressions, each of which uses a different advertising medium as a predictor:\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ radio, advertising))\n\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.31164    0.56290  16.542   &lt;2e-16 ***\nradio        0.20250    0.02041   9.921   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\nWe find that on average, $1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\nWe find that on average, $1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\nWe find that on average, $1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\nHowever this approach is not good. First of all it is unclear to make a sinlge prediction of sales given levesl of the three advertising media budgets, since each has their own regression equation. Second, each of these three regression equations ignores the other two medi in forming estimates for the regression coefficients. Especially if these media budgets are correalted, this can lead to very misleading estimates of the individaul media effects on sales.\nInstead of the seperate linear regressions for each predictor, better approach is to extend the simple linear regression setting \\(Y = \\beta_0 + \\beta_1 X\\) to\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon\n\\]\n(3.19)\nHere we interpret \\(B_j\\) as the average effect of a one unit increase in \\(X_j\\), holding all other predictors fixed.\nFor the advertising example;\n\\[\nsales = \\beta_0 + \\beta_1 TV + \\beta_2 radio + \\beta_3 newspaper + \\epsilon\n\\]\n\n2.2.1 Estimating the Regression Coefficients\nAgain, regression coefficients in (3.19) are unknown, and must be estimated from the data. And with these estimates we can make predictions\n\\[\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\dots + \\hat{\\beta_p}x_p\n\\] (3.21)\nThe parameters are estimated using the same least squares approach with simple linear regression. We choose \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) to minimize the sum of squared residuals\n$$ \\[\\begin{align}\n\n\\text{RSS} &= \\sum_{i = 1}^n(y_i - \\hat{y_i})^2 \\\\\n&= \\sum_{i = 1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_{i1} - \\beta_2x_{i2} - \\dots - \\beta_px_{ip})^2\n\n\\end{align}\\] $$ (3.22)\n\\(\\hat{\\beta_0}, \\hat{\\beta_1},\\dots, \\hat{\\beta_p}\\) values minimize RSS.\nWe are not going to calculate these estimates with our hands, R does that.\nLets see our model results with the three predictors.\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation: for a given amount of Tv and newspaper advertising, spending additional $1,000 on radio(TV)(newspaper) advertising leads to an increase in sales approximately by 189(46)(-1) units.\nIf we compare these effects with one predictor regressions\n\nWe find that on average, $1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\n\n\nWe find that on average, $1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\n\n\nWe find that on average, $1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\n\nFor tv and radio coefficients are similar, but for newspaper: from the simple linear regression coefficient of newspaper was significant, but in multiple linear regression it is not; p value is very high.\n\nconfint(lm(sales ~ TV + radio + newspaper, advertising))\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n\n\nIts confidence interval contains 0.\nThis difference between simple linear regression and multiple linear regression coefficients stems from the fact that in the simple regression, the slope term represents the average effet of a one dollar increase in newspaper advertising, ignoring other preditors such as tv and radio. In contrsat, in the multiple regression setting, the coefficient for newspaper represents the average effect of increeasing newapper spending by one dollar, while holding tv and radio fixed.\nDoes it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite? Yes!\nTake a look at this correlation matrix:\n\ncor(advertising[1:4])\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\nWe can also make it a plot out of this:\n\ncorrplot(cor(advertising[1:4]), method = \"number\")\n\n\n\n\nNotice that correlation between radio and newspaper is 0.35. This reveals a tendency to spend more on newspaper advertising in markets where more is spent on radio advertising. Now suppose the multiple regression is correct and newspaper advertising has no direct impact on sales, but radio advertising does increase sales. Then in markets where we spend more on radio, our sales will tend to be higher, adn as our correaltion matrix shows, we also tend to spend more on newspaper advertising in those same markets. Hence, in a simple linaer regresion which only examines sales vs newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales. So newspaper sales are proxy for radio advertising; newspaper gets credit for the effect of radio on sales.\nThis is a very common issue. Consider running a regression of shark attack versus ice cream sales for data collected at a given beach community. We would see a positive relationship, similar to that seen between sales and newspaper. Of course ice creams doesnt cause shark attacks. In reality higher temperatures cause more people to visit the beach in trun results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature revals that, the former predictr is no longer significant after adjusting for temperature.\n\n\n2.2.2 Some important Questions\nWhen we perform MLR, we usually are interested answering a few important questions\n\nIs at least one of the predictors \\(x_1, x_2, \\dots, x_p\\) useful in predicting the response?\nDo all predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\n\n4 Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\nLets answer these questions:\n\nIs at least one of the predictors \\(x_1, x_2, \\dots, x_p\\) useful in predicting the response?\nIn SLR we simply checked whether \\(\\beta_1 = 0\\) or not. In MLR, we need to ask whether all of the regression coefficients are zero \\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\). So our null hypothesis is\n\\[\n\\begin{align}\nH_0 &: \\beta_1 = \\beta_2 = \\dots = \\beta_o = 0 \\\\\nH_\\alpha &: \\text{at least one} \\space B_j \\space \\text{is non-zero}\n\\end{align}\n\\] This hypothesis test is performed by computing the F-statistic,\n\\[\nF = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\n\\] (3.23)\nSo, if there is no relationship between the resposne and predictors, we expect F-statistic to take on value close to 1. if \\(H_\\alpha\\) is true then \\(F\\) should be greater than 1.\n\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nF statistic is 570 and is far from 1. But it is best to have a look at the p-value of the F statistic which is also very small.\nThis means that at least one of the media is associated with increase sales.\nIn (3.23) we are testing \\(H_0\\) that all the coefficients are zero. Sometimes we want to test that a particular subset of \\(q\\) of the coefficients are zero. This corresponesd to a null hypothesis\n\\[\n  H_0 : \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_p\n  \\]\nIn this case we fit a second model that uses all the variables except those last \\(q\\). suppose that residual sum of squares for that model is \\(RSS_0\\). Then the appropriate F-statistic is\n\\[\n  F = \\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}\n  \\] (3.24)\nOn advertising MLR we saw that newspaper is not significant from its p value. Then why do we need to look at the overall F-statistic? after all, it seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is realted to the respose. This is not true usually, especially when \\(p\\) is large.\nSo after estimating the model first look at the F-statistic, than to the individual t statistic p values.\n\nDo all predictors help to explain \\(Y\\), or is only a subset of the predictors useful? =&gt; Deciding on important variables\nAfter lookig at the F statistic, we can look at the individual p values. But if *p$ is large, we are going to make false discoveries.\nUsually not all predictors are associated with the response. This task of determining which predictors are associated with the response in order to fit a single model involving only those predictors is refered to as variable selection. Check out Chapter 6 for more detail. But here is a breif outline of some of the classical approaches.\nIdeally we want to perform variable selection by trying out a lot of different models, each containing different subset of the predictors. For instance if our \\(p=2\\) then we can consider four models\n\n\na model containing no variables\n\n\na model containing \\(x_1\\) only\n\n\na model containnig \\(x_2\\) only\n\n\na model containing \\(x1\\) and \\(x_2\\).\n\n\nWe can then select the best model out of all the models by looking at some statistics we can use to judge the quality of the model. These are\n\nMallow’s \\(C_p\\)\nAkaike information creterion (AIC)\nBayesian information criterion(BIC)\nadjusted \\(R^2\\)\n\nThese are discussed in more detail in chapter 6.\nWe can also determine which model is the best by plotting various model outputs, such as the residuals, in order to search for patterns.\nBut we cannot consider all models, especially when \\(p\\) is high. There are three classical approaches for this task:\n\nForward selection\n\nbegin with null model a model that contains an intercept but no predictors.\nThen fit p simple linear regressions and add to the null model the variable that results in the lowest RSS.\nThen add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n\nBackward selection\n\nPut all varaibles in the model.\nremove the least statistically significant predictor.\nestimate the new regression with \\(p-1\\) variable, remove the largest p-value predictor. This procedure continues until a stopping rule is reached =&gt; stop after all remaining variables have p value &lt; 0.02\n\nMixed selection\n\nCombination of forward selection and backward selection\nStart with no variables in the model\nadd the varaible that provides the best fit\nadd varaibles one-by-one\nat one point if the p-value for one of the variables in the model rises above a certain treshold, then we remove that variabel from the model.\nContinue untill all variables have sufficiently low p value, and all vairables in the model woudl have a large p-value if added to the model\n\n\nBackwar slecetion cannot be used if \\(p&gt;n\\), forward selection can always be used.\nHow well does the model fit the data? Model Fit\n\nTwo of the most common numerical measures of model fit are RSE and \\(R^2\\).\nIn SLR \\(R^2\\) is equal to \\(cor(Y,X)\\). In MLR \\(R^2 = cor(Y,\\hat{Y})\\).\n\\(R^2\\) will always increase as you add more variable, even though that varaible is not statistically significant. This is because adding another variable must allow us to fit the trainig data(not necessarly test data) more accurately. But this increase in \\(R^2\\) after adding a statistically not-significant varible is very low =&gt; evidence that you can drop the not significant variable. Check out the \\(R^2\\) variables of the following models\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nThey are almost the same.\nBut lets see the \\(R^2\\) of the model containing only Tv\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nit is 0.611.\nIf we add radio\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nIt increaes dramatically. This implies that model that uses TV and radio to predict sales is better than only using Tv. also radio is statistically signifiacnt.\n\nsummary(lm(sales ~ TV + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6231 -1.7346 -0.0948  1.8926  8.4512 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.774948   0.525338  10.993  &lt; 2e-16 ***\nTV          0.046901   0.002581  18.173  &lt; 2e-16 ***\nnewspaper   0.044219   0.010174   4.346 2.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 197 degrees of freedom\nMultiple R-squared:  0.6458,    Adjusted R-squared:  0.6422 \nF-statistic: 179.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nNot with newspaper though.\nOr the opposite\n\nsummary(lm(sales ~ radio, advertising))\n\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.31164    0.56290  16.542   &lt;2e-16 ***\nradio        0.20250    0.02041   9.921   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales~radio+TV, advertising))\n\n\nCall:\nlm(formula = sales ~ radio + TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ radio + newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5289  -2.1449   0.7315   2.7657   7.9751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.188920   0.627672  14.640   &lt;2e-16 ***\nradio       0.199045   0.021870   9.101   &lt;2e-16 ***\nnewspaper   0.006644   0.014909   0.446    0.656    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.284 on 197 degrees of freedom\nMultiple R-squared:  0.3327,    Adjusted R-squared:  0.3259 \nF-statistic: 49.11 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nWhat about RSE:\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nadding newspaper increased the RSE =&gt; no need to add newspaper. Adding newspaper increases RSE because\n\\[\nRSE = \\sqrt{\\frac{1}{n-p-1}RSS}\n\\]\nModels with more variables can have higher RSE if the decrease in RSS is small relative to the increase in \\(p\\).\nSo we can look at both the RSE and \\(R^2\\)."
  }
]