% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{254,254,254}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={ISLR-R21.\_1},
  pdfauthor={MÃ¼cahit Zor},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{ISLR-R21.\_1}
\author{MÃ¼cahit Zor}
\date{2023-08-23}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, enhanced, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, interior hidden, breakable, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

Here are my notes on \emph{Gareth James, Daniela Witten, Trevor Hastie,
Robert Tibshirani. An Introduction to Statistical Learning : with
Applications in R. New York :Springer, 2013}

\bookmarksetup{startatroot}

\hypertarget{what-is-statistical-learning}{%
\chapter{What Is Statistical
Learning?}\label{what-is-statistical-learning}}

Question: How to improve sales of our product?

We have a data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"./data/Advertising.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as\_tibble }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
New names:
Rows: 200 Columns: 5
-- Column specification
-------------------------------------------------------- Delimiter: "," dbl
(5): ...1, TV, radio, newspaper, sales
i Use `spec()` to retrieve the full column specification for this data. i
Specify the column types or set `show_col_types = FALSE` to quiet this message.
* `` -> `...1`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 200 x 4
      TV radio newspaper sales
   <dbl> <dbl>     <dbl> <dbl>
 1 230.   37.8      69.2  22.1
 2  44.5  39.3      45.1  10.4
 3  17.2  45.9      69.3   9.3
 4 152.   41.3      58.5  18.5
 5 181.   10.8      58.4  12.9
 6   8.7  48.9      75     7.2
 7  57.5  32.8      23.5  11.8
 8 120.   19.6      11.6  13.2
 9   8.6   2.1       1     4.8
10 200.    2.6      21.2  10.6
# i 190 more rows
\end{verbatim}

\(n=200\), independent variables (predictors) are \texttt{TV},
\texttt{radio}, and \texttt{newspaper} advertising spendings in
thousands of dollars. We want to explore their relationship with
\texttt{sales}; quantity of product sold for each advertising mixture.
If we determine association between advertising and sales, we can
provide adjustment of advertisement budgeds based on most effective
media to increase sales; we want to develop an accurate model that ca be
used to predict sales on the basis of three media budgets.

We denote all input variables (actual--realized) as
\(X_1, X_2, ..., X_p\) and use \(X\) to refer all of them. In this case
\(X = (X_1, X_2, X_3)\). Sales is denoted with \(Y\).

This means we assume a relationship between \(Y\) and \(X\) in a form of

\[
Y = f(X) + \epsilon
\] (2.1)

\begin{itemize}
\tightlist
\item
  Here \(f\) is some fixed, but unknown function of \(X\).
\item
  \(\epsilon\) is a random \emph{error term} =\textgreater{} independent
  of \(X\) and has a mean zero.
\end{itemize}

So \(f\) represents systematic information that \(X\) provides about
\(Y\).

\(f\) is generally unknown. We will need to estimate \(f\) baed on the
observed points =\textgreater{} \(\hat{f}\).

Statistical learning refers to a set of approaches for estimating \(f\).

\hypertarget{why-estimate-f}{%
\section{\texorpdfstring{Why estimate
\(f\)?}{Why estimate f?}}\label{why-estimate-f}}

Two reasons: * \emph{prediction} * \emph{inference}

\textbf{Prediction}

Most of the time we have \(X\) but we might not have \(Y\). In this
setting, since the error term averages to zero, we can predict \(Y\)
using

\[
\hat{Y} = \hat{f}X
\] (2.2)

Here \(\hat{f}\) is treated as a \emph{black box}. We are not concerned
with the exact form of \(\hat{f}\), we just want to have accurate
predictions of \(Y\).

Imagine we have \(X = (X_1, X_2, \dots, X_p)\); blood sample
characteristics of patients. \(Y\) is a variable showing the patient's
risk for a adverse reaction to a drug. We don't want to give the drug
and see the reaction, so we want to predict reactions.

The accuracy of our predictions \(\hat{Y}\) of \(Y\), depends on two
quantities:

\begin{itemize}
\item
  \emph{reducible error}

  Generally \(\hat{f}\) will not be a perfect estimate for \(f\). This
  inaccuracy will introduce some error, which we call reducible error
  since we can improve our accuracy of \(\hat{f}\) using the most
  appropriate statistical leraning method.
\item
  \emph{irreducible error}

  Even if we estimate \(f\) perfectly, our estimated response would take
  the form \(\hat{Y} = f(X)\); our predictions would still get some
  error. This is because \(Y\) is not just a function of \(X\) but also
  a function of \(\epsilon\), which cannot be predicted by \(X\). So the
  level of \(\epsilon\) would also effect our prediciton accuracy. And
  we cannot remove this error; thus, irreducible.

  \(\epsilon\) is larger than zero; because \(\epsilon\) may contain
  some variables we don't include in our model, but effect \(Y\).
\end{itemize}

\textbf{Inference}

Here we want to understand the way that \(Y\) is affected by \(X\). In
this setting, we wish to estimate \(f\) but we are not concerned with
predicting. We want to understand the relationship between \(X\) and
\(Y\); how \(Y\) changes as \(X\) changes. We \textbf{don't} treate
\(\hat{f}\) as a \emph{black box} now since we need to know its exact
form. In this setting we are interested in answering questions such as

\begin{itemize}
\item
  \emph{Which predictors are associated with the response?}

  Usually not all predictors are associated with \(Y\). We need to
  identify the \emph{important} predictors among a large set of possible
  predictors.
\item
  \emph{What is the relationship between the response and each
  predictor?}

  Some predictors have positive some negative association with \(Y\).
  Depending on the complexity of \(f\), the relationship between \(Y\)
  and \(X_i\) may also depend on the values of other predictors(\(X_j\))
  =\textgreater{} \emph{synergy}
\item
  \emph{Can the relationship between} \(Y\) \emph{and each predictor be
  adequately summarized using a linear equation, or is the relationship
  more complicated?}
\end{itemize}

Sometimes we are interested with prediction: Identifying individuals who
will respond positively to a mailing, based on observations of
demographic variables. Here we are not interested with understanding the
relationship of demographic variables and response, we just want an
accurate model to predict the response using the predictors. This is
prediction.

But often we are interested to answer questions like: \emph{Which media
contribute to sales?}, \emph{Which media generate the biggest boost in
sales?}, or \emph{How much increase in sales is associated with a given
increase in} \texttt{TV} \emph{advertising?}. This is inference.

And sometimes we want a combination of both: \emph{Values of homes based
on crime rate, zoning, distance from a river, air quality, schools, size
of houses etc.} and \emph{How does air quality effect valeus of homes?}.

We use different models for prediction, inference, or combination of the
two.

\hypertarget{how-do-we-estimate-f}{%
\subsection{\texorpdfstring{How Do We Estimate
\(f\)?}{How Do We Estimate f?}}\label{how-do-we-estimate-f}}

There are many linear and non-linaer approaches we will discuss. But
generally these models share certain characteristics. Here are they:

\begin{itemize}
\tightlist
\item
  We will always assume that we have observed a set of \emph{n}
  different data points. These data points, observations, are called
  \emph{training data}; which we will use these observations to train,
  or teach, our model on how to estimate \(f\). Our training data will
  consist of \(\{(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)\}\), where
  \(x_i = (x_{i1}, x_{i2}, \dots, x_{ip})^T\)
\end{itemize}

We want to apply a statistical learning method to the training data to
estimate the unknown function \(f\). We want to find a function
\(\hat{f}\) such that \(Y \approx \hat{f}(X)\) for any obsrvation
\((X,Y)\).

These statistical learning methods can be charactarized as either
\emph{parametric} or \emph{non-parametric}.

\textbf{Parametric Methods}

Parametric methods involve a two step model-based approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Select a model =\textgreater{} Make an assumption about the
  functional form of} \(f\): is it linear, non linear?
\end{enumerate}

For example a linear \(f\) assumption would yield a \emph{linear model}

\[
  f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
  \] (2.4)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \emph{Fit} or \emph{train} the model
\end{enumerate}

After we select a model, we need a procedure that uses training data to
\emph{fit} or \emph{train} the model.

For linear model, we need to estimate the parameters of the model
(\(\beta_0, \beta_1, \dots, \beta_p\)). That is we want to find values
of these parameters such that \[
  Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
  \] The most common approach to fitting the model (2.4) is called
\emph{ordinary least squares}. Chapter3. But there are other approaches
as well.

This model-based approach is called \emph{parametric:} we estimate \(f\)
via estimating a set of parameters.

Disadvantage (potential): model we choose will usually not match the
true unknown form of \(f\) =\textgreater{} our estimates will be poor.
=\textgreater{} solution: choose a \emph{flexible} model that can fit
different possible functional forms for \(f\) =\textgreater{} you will
need to estimate more parametrs =\textgreater{} \emph{overfitting the
data}.

\textbf{Non-parametric Methods}

=\textgreater{} No explicit assumptions about the functional form of
\(f\). The goal is to get an estimate of \(f\) that gets as close to the
data points as possible without being too rough or wiggly
=\textgreater{} advantage over parametric approach: no assumption about
the functional form of \(f\)--potentially accurately fit a wider range
of possible shapes for \(f\).

Disadvantage =\textgreater{} lots of parameters to estimate
=\textgreater{} very large of observations required to obtain an
accurate estimate for \(f\).

\hypertarget{the-trade-off-between-prediction-accuracy-and-model-interpretability}{%
\subsection{The Trade-Off Between Prediction Accuracy and Model
Interpretability}\label{the-trade-off-between-prediction-accuracy-and-model-interpretability}}

Some models are flexible some restrictive; in the sense that they can
produce just a small range of functional forms to estimate \(f\). Linear
regression for instance is a relatively inflexible approach. Other
metgods such as thin plate splines (non-parametric) are more flexible
because they can generate a much wider range of possible functional
forms to estimate \(f\).

\emph{Why would be ever choose to use a more restrictive method instead
of a very flexible approach?} :

\begin{itemize}
\item
  If we are mainly interested in inference, restrictive models are more
  interpretable. They give more information about each predictors effect
  on predicted.
\item
  If we are mainly interested in prediction, flexible models give better
  fit. =\textgreater{} but may yield less accurate fits due to
  \emph{overfitting!}
\end{itemize}

\hypertarget{supervised-vs-unsupervised-learning}{%
\subsection{Supervised vs Unsupervised
Learning}\label{supervised-vs-unsupervised-learning}}

Most statistical learning problems fall into these two categories:
\emph{supervised} or \emph{unsupervised}.

In supervised learning for each observation of the predictor values
\(x_i, i = 1,\dots, n\) there is an associated response value \(y_i\).
We wish to fit a model that relates the response to the predictors with
the aims of either accurately predicting the response for future
observations (prediction) or better understanding the relationship
between the response and the predictors (inference). Linear regression,
GAM, boosting, support vector machines operate in the supervised
learning domain.

Unsupervised leraning describes a situtaion in which for every
observation \(i=1,\dots,n\) we obser a vector of values \(x_i\) but no
associated response \(y_i\). We cannot use a linear regression model
since we dont have \(y_i\) values. Here we can seek to understand the
relationships between the variables or between the observations; like
\emph{cluster analysis}, or clustering: to assert on the basis of
\(x_i,\dots,x_n\) whether the observations fall into relatively distinct
groups.

\hypertarget{regression-vs-classisfication-problems}{%
\section{Regression vs Classisfication
Problems}\label{regression-vs-classisfication-problems}}

Variables can be characterized as either \emph{quantitative} or
\emph{qualitative}(also known as \emph{categorical}). Quantitative
varaibles take on numberical values: a person's age, height, or income,
the value of a house, price of stock. Qualitative varaibles take on
values n one of \(K\) different \emph{classes}, or categories: aperson's
gender(male or female), the brand of a good (A,B, or C), a person's race
etc.

We refer to problems with a quantitative response as \emph{regression}
problems, and probmes with a qualitative response as
\emph{classification} problems. However, the distinction is not
clear-cut.

Least squares regression is used with a quantitative response, whereas
logistic regression is typically used with a qualitative response. Some
statistical methods, such as \emph{K}-nearest negihbors and boosting,
can be used in the case of either quantitative or qualitative.

We usually select statistical learning methods based on whether the
response is quantitative or qualitative: we might use linear regression
wjen quantitative and logistic regression when qualitative. But whether
the \emph{predictors} are qualitative or quantitative is usually not
that important. Most of the statistical learning methods can be applied
regardless of the predictor varible type.

\hypertarget{assessing-model-accuracy}{%
\section{Assessing Model Accuracy}\label{assessing-model-accuracy}}

There is no one method that dominates all others over all possible data
sets. On a particular data set, one metghod may work best, but some
other method may work better on a similar but differet data set. So it
is important to assess the model accuracies of the methods.

Here some ways to asses the model accuracy

\hypertarget{measuring-the-quality-of-fit}{%
\subsection{Measuring the Quality of
Fit}\label{measuring-the-quality-of-fit}}

So, to evaluate the performance of a statistical learning method on a
given data set, we need to measure how well its predictions actually
match the observed data.

In the regression setting, the most commonly-used measure is the
\emph{mean squared error (MSE)}, given by

\[
\text{MSE} = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(x_i))^2
\]

MSE will be small if the predicted responses are veryt close to the true
responses, and large if predicted and true responses differ
substantially \textbf{on average}.

Here since MSE is computed using the training data it is best to refer
it as \textbf{training MSE}. But in general, we do not really care how
well the method works on the training data =\textgreater{} \emph{we are
interested in the accuracy of the predictions that we obtain when we
apply our method to previously unseen test data}.

Imagine: stock price prediction =\textgreater{} we have training and
test data =\textgreater{} we already know the stock prices of the past,
we dont care about the training data accuracy of the model, we want our
model to predict the future prices of stocks best.

Or we have blood characteristics of diabetes patients. We don't want our
model to explain our existing patient's classification of diabetes or
not, we want our model to predict our future patien's situation the
best.

Mathematically:

We fit our statistical learning method on our training observations
\(\{(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)\}\), and we obtain the
estimate \(\hat{f}\). We can then compute
\(\hat{f}(x_1),\dots, \hat{f}(x_n)\). If these are approximately equal
to \(y_1, \dots, y_n\) then our training MSE will be small. Howeer, we
are not interested in whether \(y_i \approx \hat{f}(x_i)\), we want to
know whether \(\hat{f}(x_0)\) is approximately equal to \(y_0\), where
\((x_0,y_0)\) is a \emph{previously unseen test obsrevation not used to
train the statistical learning method}.

That is, we want to choose the method that gives the lowest \emph{test
MSE}!.

So with our \emph{test data} we can compute \emph{test MSE}

\[
\text{MSE}_{test} =\frac{1}{n_{test}} \sum(y_{test_{i}} - \hat{f}(X_{test_i}))
\] (2.6)

We want the test MSE to be small as possible. We can compute test MSE
via (2.6) if we have test data for different models and select the model
with minimum test MSE.

If we don't have a test data, you might think our goal would be to
minimize the training MSE since test and training data are colesly
related. But no; minimal training MSE doesn't guarantee minimal training
MSE

Usually as the level of flexibility increases, the curves fit the
observed data more closely =\textgreater{} lower training MSE. The level
of flexibility is quantified by \emph{degrees of freedom}. More
restricted models have lower degrees of freedom. and usually the
training MSE declines as flexibility increases.

\includegraphics{fig2.9.png}

As the flexibility of the statistical learning method increases, we
observe a monotone decrease in the training MSE and a \(U\)-\emph{shape}
in the test MSE. This is a fundamental property of statistical learning
that holds regardless of the particular data set at hand and regardless
of the statistical method being used. AS model flexibility increases,
training MSE will decrease, but the test MSE may not. When a given
method yields a small training MSE but a large test MSE, we are said to
be \emph{overfitting} the data. This happens because our statistical
learninig procedure is working too hard to find patterns in the training
data, and may be picking up some patterns that are just caused by random
change rather than by true properties of the unknown function \(f\).
When we overfit the trainin data, the test MSE will be very large
because the supposed patterns that the method found in the training data
simply don't exist in the test data.

Note that regardless of whether or not overfitting has occured, we
almost always expect the training MSE to be smaller than the test MSE
because most statistical learning methods either directly or inderectly
seek to mimizie the training MSE. Overfitting refers specifically to the
case in which a less flexible model would have yielded a smaller test
MSE.

In practice, training MSE is computed easily, but estimating test MSE is
hard because usually no test data are available. We will learn
approaches that can be used in practice to estimate the mininmum test
MSE. One important method is \emph{cross-validation}( Chapter 5), which
is a method for estimating test MSE using the training data.

\hypertarget{the-bias-variance-trade-off}{%
\subsection{The Bias-Variance
Trade-Off}\label{the-bias-variance-trade-off}}

The U-shape in the test MSE result of two competing properties of
statistical learnig methods. The expected test MSE, for a given value
\(x_0\) can always be decomposed into sum of \emph{variance} of
\(\hat{f}(x_0)\), the squared \emph{bias} of \(\hat{f}(x_0)\) and the
variance of the error terms \(\epsilon\). That is

This means that to minimize the expected test error, we need to
simultaneously have \emph{low variance} and \emph{low bias}. Since
variance is always bigger than zero; \(\text{Var}(\epsilon)\), and
\(\text{Bias}(\hat{f}(x_0))\) are nonnegative. So, the expected test MSE
can never lie below \(\text{Var}(\epsilon)\), the irreducible error from
(2.3).

What do we mean by the \emph{variance} and \emph{bias} of statistical
learning method?

\emph{Varince} refers to the amount by which \(\hat{f}\) would change if
we estimated it using a different training data set; different training
data sets will result in a different \(\hat{f}\). But ideally,
\(\hat{f}\) should not vary too much between training sets. If a method
has high variance small changes in the training data can result in large
changes in \(\hat{f}\).

Flexible methods have hiher variance, because they fit better to the
data points and changing any of the data points may cause the estiamte
\(\hat{f}\) to chance considerably. But for example, least squares
method is relatively inflexible and has low variance, because mooving
any single observation will cause only a small shift in the position of
the line.(2.9)

\emph{bias} refers to the error that is due to functional form of
\(\hat{f}\). In real life, linear relationships are very rare. So
performing linear regression will result in some bias in the estimate of
\(f\). If your \(f\) is non-linear performing linear regression on
different data sets will not produce an accurate estimate; so linear
regression will result in high bias.

For example in 2.9 true \(f\) is non linear; so linear regression have
high bias, low variance. \includegraphics{fig2.10.png}

In 2.10 true \(f\) is very close to linear, so linaer regression have
low bias, low variance.

Generally more flexible methods result in less bias.

As a general rule, as we use more flexible methods the variance will
increase and bias will decrease. The relative rate of change of these
two quantities determines whether test MSE increases or decreases. As we
increase the flexibility, the bias tends to initially decrease faster
than the variance increases =\textgreater{} test MSE declines. However,
at some point increasing flexibility has little impact on thebias but
starts to significantly increase the variance =\textgreater{} test MSE
increases.

\includegraphics{fig2.12.png}

Figure 2.12 shows bias and variance effect to the test MSE for different
\(f\)s. Horizontal dashed line represents \(\text{Var}(\epsilon)\), the
irreducible error; the red curve test MSE is the sum of squared bias,
variance, and variance of irreducible error. In all cases bias decreases
as flexibility increaes. However, the optimal flexibility is different
for each \(f\). In the left panel the bias initially decreaes rapidly,
decreasing test MSE. In center panel true \(f\) is closer to linear so
there is only a small decrease in bias as flexibility increaes, and the
test MSE only declines slightly before increasing rapidly as the
variance increaes. Right hand panel, as flexibility increaess bias
dramatically decreases because true \(f\) is very non-linear. There is
alsso very little increase in variance as flexibility increases
=\textgreater{} tets MSE decreases before increasing.

This is called bias-variance trade off. Good test set performance of a
method requires low variance as well as low squared bias. This is a
trade off because it is easy to obtain a method with extremely low bias
but high variance(for instance, drawing a curve that passes through
every single training observation, or a method with low variance but
high bias(by fitting a horizontal line to the data). Challange is
finding a method wihch both the variance and squareed bias are low.

In real life, it is not possible to explicitly compute the test MSE,
bias, or variance for methods. But we should keep this in mind.

\hypertarget{the-classification-setting}{%
\subsection{The Classification
Setting}\label{the-classification-setting}}

So far we focused on regression setting. Problems such as bias-variance
trade of also occurs in classification but in a modificated way because
\(y_i\) is no longer numerical.

Suppose that we seek to estiamte \(f\) on the basis of training
observations \(\{(x_1,y_1), \dots, (x_n,y_n)\}\) where now
\(y_1,\dots, y_n\) are qualitative.

We need to quantify the accuracy of our estimate \(\hat{f}\). We can use
the training \emph{error rate}, the proportion of mistakes that are made
if we apply our estimate \(\hat{f}\) to the training observations:

\[
\text{error rate}=\frac{1}{n}\sum^n_{i=1}I(y_i \neq \hat{y_i})
\] (2.8)

\(\hat{y_i}\) is the predicted class label for the \(i\)th observation
using \(\hat{f}\). \(I(y_i \neq \hat{y_i})\) is an \emph{indicator
variable} that equals 1 if \(y_i \neq \hat{y_i}\) and zero if
\({y_i = \hat{y_i}}\). If \(I(y_i \neq \hat{y_i}) = 0\) then \(i\)th
observation was classified correctly, otherwise it was misclassified. So
(2.8) computes the fraction of incorrect classifications.

(2.8) is \emph{training error}. But as the regression setting we are
more interested in \emph{test error} rate. The \emph{test error} rate
associated with a set of test observations of the form

\[
\text{error rate}_{test} = \frac{1}{n_{test}}\sum_{i=1}^{n_{test}}(I(y_{test_i} \neq \hat{y}_{test_i}))
\] (2.9)

A \emph{good} classifier is one for which the test error is smallest.

\textbf{The Bayes Classifier}

We can minimize test error rate by a very simple classifier that
\emph{assigns each observation to the most likely class, given its
predictor values}. In other words, we should simply assign a test
observation with predictor vector \(x_0\) to the class \(j\) for which

\[
Pr(Y = j | X = x_0)
\] (2.10)

is largest. This is \emph{conditional probability:} it is the probabilty
that \(Y=j\) given the observed predictor vector \(x_0\). This
classifier is called \emph{Bayes classifier}.

In a two-class problem where there are only two possible response
values, \emph{class 1} or \emph{class2}, the Bayes classifier
corresponds to predicting class one if \(Pr(Y=1 | X = x_0) > 0.5\), and
class two otherwise.

Imagine having \(X = (X_1, X_2)\). For each value of \(X_1\) and \(X_2\)
there will be a different probability of the response being class 1 or
2. For \(Pr(Y=class1 | X = (X_2,X_2)) > 0.5\) and
\(Pr(Y=class2 | X = (X_1,X_2))\).

The Bayes classifier produces the lowest possible test error rate,
called the \emph{Bayes error rate}. Since the Bayes classifer will
always choose the class for which \(Pr(Y = j | X = x_0)\) is largest,
the error rate at \(X=x_0\) will be \(1-\max_jPr(Y =j | X = x_0)\). In
general, the overall Bayes error rate is given by

\[
\text{Bayes error rate} = 1 - E(\max_jPr(Y =j | X))
\] (2.11)

where the expectation averages the probabilty over all possible values
of X. The Bayes eror rate is analogous to the irreducible eror.

\textbf{K-Nearest Neighbors}

In theory we always want to predict qualitative responses using the
Bayes classifier. But for real data, we do not know the conditional
distribution of \(Y\) given \(X\), and so computing the Bayes classfier
is impossible. So Bayes classifier is like a gold standard to compare
other methods.

Many approaches attemp to estiamte the conditional distribution of \(Y\)
given \(X\), and then classify a given observation to the class with
highest \emph{estimated} probability. One of them is \emph{K-nearest
neighbors}(KNN) classfier.

Given a positive integer \emph{K} and a test observation \(x_0\), the
KNN clasffier first identifies the \emph{K} points in the training data
that are closest to \(x_0\), represented by \(N_0\). It then esitmates
the conditional probability for class \emph{j} as the fraction points in
\(N_0\) whose response values equal to \(j\).

\[
Pr(Y = j | X = x_0) = \frac{1}{K}\sum_{i \in N_0}I(y_i=j)
\] (2.12)

Finally, KNN applies Bayes rule and classifies the test observation
\(x_0\) to the class with the largest probability.

\includegraphics{fig2.14.png}

Figure 2.14 provides an illustrative example of the KNN approach. Left
panel =\textgreater{} Our goal is to make prediction for the black cross
point. When \(K=3\) KNN will identify the 3 oservations that are closest
to the cross. There are two blue and one orange points;
\(Pr(Y=orange | X = x_{cross}) = 1/3\), and
\(Pr(Y=blue | X = x_{cross}) = 2/3\) =\textgreater{} KNN will predict
that the black cross belongs to the blue class.

The choice of K is very important. as K increases flexibility decreases
=\textgreater{} high bias, but low variance.

Just like in regression setting there is not a strong relationship
between the training error rate and the test error rate.

\includegraphics{fig2.17.png}

as in the regression setting, the training error rate consistently
declines as the flexibility(\(1/K\)) increases. However, the test error
rate again have a characteristic U-shape.

In both regression and classification settings, choosing the correct
level of flexibility is critical. The bias-variance tradeoff
=\textgreater{} U-shape in the test error, can make this a difficult
task.

\bookmarksetup{startatroot}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

We will predict quantitative response.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
\FunctionTok{library}\NormalTok{(ISLR)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(dotwhisker)}
\FunctionTok{library}\NormalTok{(hrbrthemes)}
\FunctionTok{library}\NormalTok{(patchwork)}
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{library}\NormalTok{(showtext)}
\NormalTok{\})}
\NormalTok{extrafont}\SpecialCharTok{::}\FunctionTok{loadfonts}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_ipsum\_es}\NormalTok{(}\AttributeTok{axis\_title\_size =} \DecValTok{11}\NormalTok{ , }\AttributeTok{axis\_title\_just =} \StringTok{"c"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{color =}\StringTok{"black"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"./data/Advertising.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as\_tibble }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{advertising}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 200 x 4
      TV radio newspaper sales
   <dbl> <dbl>     <dbl> <dbl>
 1 230.   37.8      69.2  22.1
 2  44.5  39.3      45.1  10.4
 3  17.2  45.9      69.3   9.3
 4 152.   41.3      58.5  18.5
 5 181.   10.8      58.4  12.9
 6   8.7  48.9      75     7.2
 7  57.5  32.8      23.5  11.8
 8 120.   19.6      11.6  13.2
 9   8.6   2.1       1     4.8
10 200.    2.6      21.2  10.6
# i 190 more rows
\end{verbatim}

We are asked to suggest a marketing plan for next year which will yeild
high product sales. We may want to inquire the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Is there a relationship between advertising budget and sales?}

  First we should determine if there is an association between
  adveritisng expenditure and sales. If not, no money shpuld be spent on
  advertising.
\item
  \emph{How strong is the relationship between advertising budget and
  sales?}

  If there is a relationship between advertising and sales, what is the
  strength of this relationship? Given a certain advertising budget, can
  we predict sales with a high level of accuracy? =\textgreater{} strong
  relationsihp.
\item
  \emph{Which media contribute to sales?}

  Do all variables--tv,radio,newspaper-- contribute to sales, or just
  one or the two?
\item
  \emph{How accurately can we estimate the effect of each medium on
  sales?}

  For every ollar spent on advertising in a particular medium, by what
  amount will sales increase? How accuretly can we predict this amount
  of increase?
\item
  \emph{How accurately can we predict future sales?}

  For any given level of media advertisig, what is our prediction for
  sales, and what is the accuracy of this prediciton?
\item
  \emph{Is the relationship linear?}

  If so linear regresion is appropriate tool, if not we may need to
  transform the predictor or the repsonse so that liner regression can
  be used.
\item
  \emph{Is there synergy among the advertising media?}

  Does the effect of a medium on sales depend on other medium levels?
  Does dividing advertisement budget to two or three medium yeild a
  higher sales?
\end{enumerate}

We can answer each of these questions using Linear regression.

\hypertarget{simple-linear-regression}{%
\section{Simple Linear Regression}\label{simple-linear-regression}}

Predicting a quantitative response \(Y\) on the basis of a single
predictor variable \(X\).

Our assumption is that there is approximately a linear relationship
between \(X\) and \(Y\); we can write this linear relationship as

\[
Y = \beta_0 + \beta_1 X_1 + \epsilon
\]

\[
Y \approx \beta_0 + \beta_1X
\] (3.1)

For example lets say \(X\) is \texttt{TV}, and \(Y\) is \texttt{sales}

\[
sales = \beta_0 + \beta_1 \times TV + \epsilon
\] or

\[
sales = \beta_0 + \beta_1 \times TV
\]

On (3.1) \(\beta_0\) and \(\beta_1\) are unknown constants that
represent the \emph{intercept} and \emph{slope} in the linear model.
Together they are known as \emph{coefficients} or \emph{parameters}.

We are going to use or training data to produce estimates for
\(\beta_0\) =\textgreater{} \(\hat{\beta_0}\) and \(\beta_1\)
=\textgreater{} \(\hat{\beta_1}\). Using these predicted coefficients we
can predict sales;

\[
\hat{sales} = \hat{\beta_0} + \hat{\beta_1} \times TV
\]

or as in general form

\[
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
\] (3.2)

\hypertarget{estimating-the-coefficients}{%
\subsection{Estimating the
coefficients}\label{estimating-the-coefficients}}

Since, \(\beta_0\) and \(\beta_1\) are unknown, before we can use (3.1)
to make predictions we must use data to estimate the coefficients. We
have \(n\) observations :

\[
(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)
\]

We want our estimated coefficients to give such predictions that will
fit the avaible data as well =\textgreater{}
\(y_i \approx \hat{\beta_0} + \hat{\beta_1}x_i\) for \(i = 1,\dots, n\).
This coefficients will allow us to draw a regression line and we want
this regression line to be close as possible to the \(n\) data points we
have.

There are different ways to measure \emph{closeness}. The most common
approach is minimizing the \emph{least squares} criterion. Alternative
approaches will be considered in Chapter 6.

Our predictions come from
\(\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i\).

Then for each data we have a \emph{residual}: difference between \(y\)
and \(\hat{y}\):

\[
e_i = y_i - \hat{y_i}
\]

We need to take the squares to get the distances--because of the
negative residuals, and sum them to get the \emph{residual sum of
squares}(RSS)

\[
RSS = e_1^2 + e_2^2 + \dots + e_n^2
\]

this is equal to

\[
RSS = (y_i - \hat{beta_0} - \hat{\beta_1}x_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2) + \dots + (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)
\] (3.3)

The least squares approach chooses \(\hat{\beta_0}\) and
\(\hat{\beta_1}\) to minimize RSS. These minimizers are

\[
\begin{align}
\hat{\beta_1} &= \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
\hat{\beta_0} &= \bar{y_i} - \hat{\beta_1}\bar{x}
\end{align}
\] (3.4)

\(\bar{y} = \frac{1}{n}\sum_{i=1}^ny_i\) and
\(\bar{x} = \frac{1}{n}\sum_{i=1}^nx_i\) are the sample means.

So (3.4) defines the \emph{least squares coefficient estimates} for
simple linear regression.

Lets calculate them with R

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_1\_hat\_adv }\OtherTok{=} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{TV }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV)) }\SpecialCharTok{*}\NormalTok{ ((advertising}\SpecialCharTok{$}\NormalTok{sales }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales)))) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{TV }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{beta\_1\_hat\_adv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.04753664
\end{verbatim}

So; our \$\hat{\beta_1} = 0.0475 \$

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_0\_hat\_adv }\OtherTok{=} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales) }\SpecialCharTok{{-}}\NormalTok{ beta\_1\_hat\_adv }\SpecialCharTok{*} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV)}
\NormalTok{beta\_0\_hat\_adv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.032594
\end{verbatim}

our \(\hat{\beta_0} = 7.032\)

Lets compare them with r function

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, }\AttributeTok{data =}\NormalTok{ advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3860 -1.9545 -0.1913  2.0671  7.2124 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843   15.36   <2e-16 ***
TV          0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

Calculating the predicted values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_hat\_adv }\OtherTok{=}\NormalTok{ beta\_0\_hat\_adv }\SpecialCharTok{+}\NormalTok{ beta\_1\_hat\_adv }\SpecialCharTok{*}\NormalTok{ advertising}\SpecialCharTok{$}\NormalTok{TV}
\NormalTok{y\_hat\_adv[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162  9.765950
 [8] 12.746498  7.441409 16.530414
\end{verbatim}

Our \(y_i\) values are as above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSS }\OtherTok{=} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{sales }\SpecialCharTok{{-}}\NormalTok{ y\_hat\_adv)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{RSS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2102.531
\end{verbatim}

Our \(\text{RSS} = 2102.531\)

So we can draw our regression line

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{TV, }\AttributeTok{y =}\NormalTok{ sales) }\SpecialCharTok{+}  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ beta\_0\_hat\_adv, }\AttributeTok{slope =}\NormalTok{ beta\_1\_hat\_adv, }\AttributeTok{color =} \StringTok{"\#262B70"}\NormalTok{, }\AttributeTok{size =}\FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xend=}\NormalTok{TV, }\AttributeTok{yend=}\NormalTok{y\_hat\_adv), }\AttributeTok{color =} \StringTok{"\#939393"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"\#AA1D2E"}\NormalTok{, }\AttributeTok{size =}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_par}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\caption{For the advertising data, the least squares fit for the
regression of sales onto TV. The fit is found by minimizing the sum of
squared errors. Each grey line segment represents an error, adn the fit
make a comprimise by averaging their squares. In this case a linear fit
captures the essence of the relationship, although it is somewhat
deficient in the left of the plot}

\end{figure}

So we have

\[
\hat{y_i} = 7.032 + 0.0475x_i
\]

According to this approximation an additional \$1,000 spent on TV
increases sales by 47.5 units.

\hypertarget{assessing-the-accuracy-of-the-coefficient-estimates}{%
\subsection{Assessing the Accuracy of the Coefficient
Estimates}\label{assessing-the-accuracy-of-the-coefficient-estimates}}

We assumed that \emph{true} relationship is linear:
\(Y = f(X) + \epsilon\). We don't know \(f\), and \(\epsilon\) is a
mean-zero random error term.

We said \(f\) is approximatly linear, so that
\(f(X) = \beta_0 + \beta_1 X\); which means

\[
Y = \beta_0 + \beta_1 X + \epsilon
\] (3.5)

error term captures: * the true relationship may not be linear * other
variables that affect Y * measurement error

and is independent of \(X\).

(3.5) is the \emph{population regression line}: the best linar
approximation to the true relationship between \(X\) and \(Y\).

\[
\hat{y} = \hat{\beta_0} + \hat{\beta_1}X
\] is the \emph{least squares line}. They are different of course! But
we don't know the population regression line. If we did:

For example, lets create a data;

\begin{itemize}
\tightlist
\item
  First we create random x values from 100 random numbers
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out=}\DecValTok{100}\NormalTok{)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] -2.000000 -1.959596 -1.919192 -1.878788 -1.838384 -1.797980 -1.757576
 [8] -1.717172 -1.676768 -1.636364
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11}\NormalTok{)}
\NormalTok{x }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{x[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] -0.6666667  0.2222222 -1.0303030 -1.3939394 -0.5454545  0.3838384
 [7] -1.5555556  1.3939394  1.4343434  0.4646465
\end{verbatim}

lets define our \(f\)--population parameters \(\beta_0\) and \(\beta_1\)

\[
f(X) = 2 + 3\times X
\] Now lets create our \(Y\) values from this function but we also want
to add random error values as well

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{y[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] -0.5910311  2.6932610 -2.6074622 -3.5444715  1.5421255  2.2173638
 [7] -1.3430610  6.8067360  6.2573073  2.3898188
\end{verbatim}

So we have a data set

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{=} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{y =}\NormalTok{ y, }\AttributeTok{x =}\NormalTok{ x}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now lets plot this data points

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d
\end{verbatim}

\begin{verbatim}
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font width unknown for character 0x2d
\end{verbatim}

\begin{verbatim}
Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d
\end{verbatim}

\begin{verbatim}
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font width unknown for character 0x2d

Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font width unknown for character 0x2d
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Now, even though we already know \(f\) and population parameters
\(\beta_0 = 2\) and \(\beta_1 = 3\), lets estimate them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.01398 -0.65163 -0.06344  0.60455  2.39869 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.88077    0.09180   20.49   <2e-16 ***
x            3.05893    0.07608   40.20   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9163 on 98 degrees of freedom
Multiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 
F-statistic:  1616 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

So our \emph{least squares estimation is}

\[
\hat{y_i} = 1.88 + 3.06 x_i
\] Lets draw this \emph{least squares regression line to our plot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept=}\FloatTok{1.88}\NormalTok{, }\AttributeTok{slope =} \FloatTok{3.06}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_par}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

What about the population regression line

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FloatTok{1.9}\NormalTok{, }\AttributeTok{slope =} \FloatTok{3.06}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{2}\NormalTok{, }\AttributeTok{slope =} \DecValTok{3}\NormalTok{, }\AttributeTok{color =}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_par}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

They are not the same! If we were to have another data from the same
data generation process other estimates of parameters would result with
different \emph{least squares regression lines}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\NormalTok{x\_r1 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r1 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1111}\NormalTok{)}
\NormalTok{x\_r2 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r2 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11111}\NormalTok{)}
\NormalTok{x\_r3 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r3 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r3 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{111111}\NormalTok{)}
\NormalTok{x\_r4 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r4 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r4 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1111111}\NormalTok{)}
\NormalTok{x\_r5 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r5 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r5 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11111111}\NormalTok{)}
\NormalTok{x\_r6 }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{length.out =} \DecValTok{100}\NormalTok{),}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ T)}
\NormalTok{y\_r6 }\OtherTok{=} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x\_r6 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Lets now estimate population parameters for each of these data and plot
them

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_r }\OtherTok{=} \FunctionTok{tibble}\NormalTok{(}
\NormalTok{  y\_r1,x\_r1,y\_r2,x\_r2,y\_r3,x\_r3,y\_r4,x\_r4, y\_r5,x\_r5,y\_r6,x\_r6}
\NormalTok{)}
\NormalTok{data\_r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 12
     y_r1   x_r1   y_r2    x_r2   y_r3   x_r3  y_r4   x_r4   y_r5    x_r5  y_r6
    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl> <dbl>
 1  5.99   1.11   0.807 -0.263   8.82   1.80  -3.05 -1.52   7.64   1.68   6.49 
 2  6.53   1.35   3.74   0.141   6.23   1.92   5.11  1.27   8.71   2      7.75 
 3  6.28   1.31   2.16  -0.0202 -0.272 -0.707  2.39 -0.101  4.59   0.909  1.21 
 4  1.52  -0.141 -0.178 -0.990   5.45   0.788 -4.55 -1.84  -1.41  -1.11   0.478
 5  0.708 -1.03  -0.464 -0.586   0.361 -0.343 -3.31 -1.15   9.59   1.88   2.64 
 6  2.05   0.343  4.12   0.788  -1.09  -1.11  -5.13 -1.96   0.723 -0.586  4.10 
 7  4.54   0.747  5.44   1.60    0.479 -0.707  2.27  0.182  2.29  -0.263  3.68 
 8  1.69  -0.626  8.02   1.80   -2.23  -1.64   1.55  0.222 -1.45  -0.990  5.08 
 9  2.84   0.869  1.02   0.384   4.04   0.949  3.34 -0.747 -2.21  -0.828  8.33 
10 -0.933 -0.990  3.52   0.505  -0.471 -0.505 -1.07 -0.869  1.47   0.0606 2.45 
# i 90 more rows
# i 1 more variable: x_r6 <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(x,y) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{)  }\SpecialCharTok{+} 
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r1)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r1)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#29019F"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r2)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r2)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#0A04BF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r3 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r3)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r3 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r3)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#0930DF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r4 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r4)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r4 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r4)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#0E6DFF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r5 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r5)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r5 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r5)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#2BA8FF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FunctionTok{lm}\NormalTok{(y\_r6 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r6)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =} \FunctionTok{lm}\NormalTok{(y\_r6 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_r6)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =}\StringTok{"\#48D9FF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =}\DecValTok{2}\NormalTok{, }\AttributeTok{slope =}\DecValTok{3}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  
  \FunctionTok{theme\_par}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-19-1.pdf}

}

\end{figure}

So, different data sets generated from the same true model result in
slightly different least squares lines, but the unobserved population
regression line does not change.

This is because we are using a sample, and estimating characteristics of
the population. Usually these characteristics are different, but
generally sample characteristics will provide a good estimate to the
population characteristics.

Computing \(\hat{\beta_0}\) and \(\hat{\beta_1}\) from different sets of
sample data provide different but similar results. And we are trying to
estimate population parameters \(\beta_0\) and \(\beta_1\) with these.
Some of these \(\hat{\beta_0}\) and \(\hat{\beta_1}\) will overestimate,
some will underestimate \(\beta_0\), and \(\beta_1\). But if we could
average all these estimated parameters and take the average, than this
average should be equal to population parameters; if this is the case
this estimator is called \emph{unbiased estimator}. So an unbiased
estimator does not \emph{systematically} over- or under-estimate the
true parameter.

Okay but how close \(\hat{\beta_0}\) and \(\hat{\beta_1}\) are to the
true values \(\beta_0\) and \(\beta_1\). We want to compute the standard
errors associated with \(\hat{\beta_0}\) and \(\hat{\beta_1}\). Standard
error telss us the average amount of estimate differes from the actual
value.

\[
\begin{align}
\text{SE}(\hat{\beta_0})^2 &= \sigma^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x}^2)}\right] \\
\text{SE}(\hat{\beta_1})^2 &= \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{align}
\] (3.8)

Where \(\sigma^2=\text{Var}(\epsilon)\).

Notice that formula of \(\text{SE}(\hat{\beta_1})\) is smaller when
\(x_i\) are more spread out; intutively we have more \emph{leverage} to
estimate a slope when this is the case.

In general \(\sigma^2\) is not known, but can be estimated from the
data. The estimate of \(\sigma\) is known as the \emph{residual standard
error}, and given by the formula

\[
\text{RSE} = \sqrt{\text{RSS}/(n-2)}
\] So, when \(\sigma^2\) is estimated fro mthe data we should write
\(\hat{\text{SE}}(\hat{\beta_1})\) to indicate that an estimate has been
made, but usually we drop this extra hat.

Standard errors can be used to compute \emph{confidence intervals}. A
95\% confidence interval is defines as a range of values such that with
95\% probability, the rage will contain the true unknown value of the
parameter. The range is defined in terms of lower and upper limits
computed from the sample of data. For linear regression, the 95\%
confidence interval for \(\beta_1\) approximately takes the form

\[
\hat{\beta_1} \pm 1.96 \cdot \text{SE}(\hat{\beta_1}) 
\] (3.9)

So there is approximately a 95\% chance that the interval
\[[\hat{\beta_1} - 1.96 \cdot \text{SE}(\hat{\beta_1}), \hat{\beta_1} + 1.96 \cdot \text{SE}(\hat{\beta-1})]\]
(3.10) will contain the true value of \(\beta_1\). Same is true for
\(\beta_0\)

\[
\hat{\beta_0} \pm 1.96 \cdot \text{SE}(\hat{\beta_0})
\] (3.11)

Lets calculate the confidence intervals for \(\beta_0\) and \(\beta_1\)
from our original data and model

\[
\hat{sales_i} = \hat{\beta_0} + \hat{\beta_1}\cdot TV
\] We first need to calculate RSS and RSE:

\[
\text{RSS} = \sum(y_i - \hat{y_i})^2
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSS }\OtherTok{=} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{sales }\SpecialCharTok{{-}}\NormalTok{ y\_hat\_adv)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{RSS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2102.531
\end{verbatim}

\[
\text{RSE} = \sigma = \sqrt{RSS/(n-2)}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSE }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{((RSS }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{length}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales) }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)))}
\NormalTok{RSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.258656
\end{verbatim}

For \(\beta_0\)

\[
\text{SE}(\hat{\beta_0}) = \sigma^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2} \right]
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_beta\_0\_adv }\OtherTok{=}  \FunctionTok{sqrt}\NormalTok{(RSE}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales) }\SpecialCharTok{+}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{TV }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))))}
\NormalTok{se\_beta\_0\_adv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4578429
\end{verbatim}

So we can calculate the confidence interval for \(\beta_0\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"In the absence of any advertising, sales will on average, fall somewhere between"}\NormalTok{,beta\_0\_hat\_adv }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_0\_adv, }\StringTok{"and"}\NormalTok{, beta\_0\_hat\_adv }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_0\_adv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
In the absence of any advertising, sales will on average, fall somewhere between 6.135221 and 7.929966
\end{verbatim}

For \(\hat{\beta_1}\):

\[
\text{SE}(\hat{\beta_1})^2 =\frac{\sigma^2}{\sum(x_i - \bar{x})^2}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_beta\_1\_adv }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(RSE}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{TV }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{TV))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\NormalTok{se\_beta\_1\_adv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.002690607
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"For each $1,000 increase in TV advertising, average increase in sales will be between"}\NormalTok{,(beta\_1\_hat\_adv }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_1\_adv) }\SpecialCharTok{*} \DecValTok{1000}\NormalTok{, }\StringTok{"and"}\NormalTok{, (beta\_1\_hat\_adv }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_1\_adv)}\SpecialCharTok{*}\DecValTok{1000}\NormalTok{, }\StringTok{"by 95\% confidence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
For each $1,000 increase in TV advertising, average increase in sales will be between 42.26305 and 52.81023 by 95% confidence
\end{verbatim}

Lets confirm our results

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3860 -1.9545 -0.1913  2.0671  7.2124 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843   15.36   <2e-16 ***
TV          0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 2.5 %     97.5 %
(Intercept) 6.12971927 7.93546783
TV          0.04223072 0.05284256
\end{verbatim}

So standard errors of our estimated parameters tells us the average
amount of difference from the true population parameters. And using the
confidence intervals we can tell a range of the true population
parameters' interval with a percentage (usually 95\%).

Lets do this for our \texttt{data} as well, which we know has the form

\[
y_i = 2 + 3 x_i + \epsilon_i
\] Lets calculate \(\hat{\beta_0}\) and \(\hat{\beta_1}\) first

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_1\_hat\_data }\OtherTok{=} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x)) }\SpecialCharTok{*}\NormalTok{ (data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y))) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{beta\_1\_hat\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.058925
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_0\_hat\_data }\OtherTok{=} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y) }\SpecialCharTok{{-}}\NormalTok{ beta\_1\_hat\_data }\SpecialCharTok{*} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{beta\_0\_hat\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.880772
\end{verbatim}

\[
\hat{y_i} = 1.88 + 3.05 x_i
\]

lets confirm this

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = data)

Coefficients:
(Intercept)            x  
      1.881        3.059  
\end{verbatim}

Lets calculate the residual sum of squares, residual sum of errors,
standard errors, and confidence intervals

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{y\_hat =}\NormalTok{ beta\_0\_hat\_data }\SpecialCharTok{+}\NormalTok{ beta\_1\_hat\_data }\SpecialCharTok{*}\NormalTok{ x}
\NormalTok{  ) }\OtherTok{{-}\textgreater{}}\NormalTok{ data}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSS\_data }\OtherTok{=} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y\_hat)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{RSS\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 82.28514
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSE\_data }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{((RSS\_data}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{length}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y) }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)))}
\NormalTok{RSE\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9163211
\end{verbatim}

so \(\sigma_{data} = 0.9163\)

We can now compute the standard errors of estiamed coefficients

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_beta\_0\_data }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(((}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y)) }\SpecialCharTok{+}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))) }\SpecialCharTok{*}\NormalTok{ RSE\_data}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{se\_beta\_0\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.09179903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_beta\_1\_data }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(RSE\_data}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\NormalTok{se\_beta\_1\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.07608457
\end{verbatim}

Lets confirm

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.01398 -0.65163 -0.06344  0.60455  2.39869 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.88077    0.09180   20.49   <2e-16 ***
x            3.05893    0.07608   40.20   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9163 on 98 degrees of freedom
Multiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 
F-statistic:  1616 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

So we can say that on average our \(\hat{\beta_0}\)s are 0.091 differ
from \(\beta_0\), and our \(\hat{\beta_1}\)s differ 0.076 from
\(\beta_1\). To make more sense of it we can calculate the confidence
intervals

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"By 95\% confidence we can say that the true beta\_0 is between"}\NormalTok{, beta\_0\_hat\_data }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_0\_data, }\StringTok{"and"}\NormalTok{, beta\_0\_hat\_data }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_0\_data )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
By 95% confidence we can say that the true beta_0 is between 1.700846 and 2.060698
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"By 95\% confidence we can say that the true beta\_0 is between"}\NormalTok{, beta\_1\_hat\_data }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_1\_data, }\StringTok{"and"}\NormalTok{, beta\_1\_hat\_data }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta\_1\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
By 95% confidence we can say that the true beta_0 is between 2.909799 and 3.208051
\end{verbatim}

Lets confirm this

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               2.5 %   97.5 %
(Intercept) 1.698600 2.062944
x           2.907938 3.209912
\end{verbatim}

Lets plot this confidence interval

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{dwplot}\NormalTok{(}\AttributeTok{ci =} \FloatTok{0.95}\NormalTok{,}\AttributeTok{dot\_args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size=}\DecValTok{2}\NormalTok{), }\AttributeTok{vline =} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{, }\AttributeTok{linetype =}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-40-1.pdf}

}

\end{figure}

Since standard error tells us the range of the \(\beta\) values via
confidence interval, we can infer that if this range does not include 0,
than our \(\beta\) values are statistically significant; x is assocaited
with y.

Lets do this for \texttt{advertising} data as well

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{dwplot}\NormalTok{(}\AttributeTok{ci =} \FloatTok{0.95}\NormalTok{,}\AttributeTok{dot\_args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size=}\DecValTok{2}\NormalTok{), }\AttributeTok{vline =} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{, }\AttributeTok{linetype =}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-41-1.pdf}

}

\caption{the 95\% confidence interval does not include 0; TV is
statistically significant}

\end{figure}

Or we can use standard erros to perform \emph{hypothesis tests} on the
coefficients. We usually don't care about the intercept, so lets do the
hypothesis test on only \(\hat{\beta_1}\).

\[
\begin{align}
H_0 &: \beta_1 = 0 \to \text{there is no relationship between X and Y} \\
H_1 &: \beta_1 \neq 0 \to \text{there is some relationship between X and Y}
\end{align}
\] If the \emph{null-hypothesis} is true =\textgreater{} \$ \beta\_1 =
0\$ =\textgreater{} \(Y = \beta_0 + \epsilon\) =\textgreater{} \(X\) is
not associated with \(Y\).

To test the null-hypothessi, we need to determine whether our estimate
\(\hat{\beta_1}\) is sufficiently far from zero that we can be confident
that \(\beta_1\) is non-zero. How far is enough? This depends on the
accuracy of \(\hat{\beta_1}\)--that is it depends on
\(\text{SE}(\hat{\beta_1})\). If \(\text{SE}(\hat{\beta_1})\) is small,
then even relatively small values of \(\hat{\beta_1}\) may provide
strong evidence that \(\beta_1 \neq 0\). If \(\text{SE}(\hat{\beta_1})\)
is large, then \(\hat{\beta_1}\) must be large in absolute value in
order for us to reject the null hypothessis. In practice we compute a
\emph{t-statistic} given by

\[
t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})}
\] (3.14)

which measures the number of standard deviations that \(\hat{\beta_1}\)
is away from zero. From the t-statistic we can compute the
\emph{p-value}; a small p value indicates that it is unlikely to observe
such a substantial association between the predictor and the response
due to chance, in absence of any real association between the predictor
and the response. So if p value is small we infer that there is
assocaition between the predictor and the response =\textgreater{} we
reject the null hypothesis. Typical p-value cutoffs for rejecting the
null hypothesis are 5 or 1\%. When \(n=30\) these correspond to
tstatsitcs of around 2 and 2.75.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3860 -1.9545 -0.1913  2.0671  7.2124 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843   15.36   <2e-16 ***
TV          0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV,advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{confint}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  kableExtra}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{format =} \StringTok{"latex"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
  & 2.5 \% & 97.5 \%\\
\hline
(Intercept) & 6.1297193 & 7.9354678\\
\hline
TV & 0.0422307 & 0.0528426\\
\hline
\end{tabular}

Here we see that t statistics are very high, and p values are very low
=\textgreater{} reject the null hypothesis for both \(\beta\) values;
they are statistically significant.

\hypertarget{assessing-the-accuracy-of-the-model}{%
\subsection{Assessing the Accuracy of the
Model}\label{assessing-the-accuracy-of-the-model}}

Once we concluded the statistically significant variable--rejecting the
null hypothesis, we want to quantify \emph{the extend to which the model
fits the data}. We can use either

\begin{itemize}
\tightlist
\item
  \emph{Residual standard error}
\item
  \(R^2\)
\end{itemize}

\emph{Residual standard error}

Recall from \(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\) that associated
with each observation is an error term \(\epsilon\). Because of these
error terms even if we knew the true regression line, we would not be
able to predict \(Y\) from \(X\). The \emph{RSE} is an estimate of the
standard deviation of \(\epsilon\). It is the average amount that the
response will deviate from the true regression line, computed by

\[
\begin{align}
\text{RSE} &= \sqrt{\frac{1}{n-2}\text{RSS}} \\
&= \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat{y_i})^2}
\end{align}
\] (3.15)

In the advertising data, RSE was 3.26; actual sales in each market
deviate from the true regression line by approximately 3,269 units, on
average. This also means that; if the model were correct and the true
values of the unknown coefficients \(\beta_0\) and \(\beta_1\) were
known exaclty, any predcition of sales on the basis of TV advertising
would still be off by about 3,260 units on average. Is this prediction
error accaptable? Depends on the data: in the \texttt{advertising} data
set the mean value of \texttt{sales} is \(\approx 14,000\) units, and so
the percentage error is \(3,260 / 14,000 = 23%
\).

The RSE is considered a measure of the \emph{lack of fit} of the model
\(Y=\beta_0 + \beta_1 + \epsilon\) to the data. If the predictions from
the model are very close to the true outcome
values--\(\hat{y_i} \approx y_i\) then RSE will be small, and we can
concldue that the model fits the data very well. Otherwise, if
\(\hat{y_i}\) is very far from \(y_i\) then RSE may be quite large,
indicating the model doesn't fit the data well.

\(R^2\)\textbf{Statistic}

The RSE provides an absolute measure of lack of fit of the model to the
data. \(R^2\) provides an alternative measure of fit. It takes the form
of a \emph{proportion}--the proportion of variance explained--and so its
always \(0\leq R^2 \leq 1\) and is independent of the scale of \(Y\)--as
opposed to RSE.

\[
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
\] (3.17)

where \(\text{TSS} = \sum(y_i - \bar{y})^2\) is the \emph{total sun of
squares} and \(\text{RSS} = \sum(y_i - \hat{y_i})^2\). TSS measures the
total variaance in the response \(Y\); and can be thought of as the
amount of varaiblity ingerent in the response before the regression is
performed. RSS measures the amount of varaiblity that is left
unexplained after performing the regression. So TSS - RSS measures the
amount of variability in the response that is explained by performing
the regression, and \(R^2\) measures the \emph{proportion of variability
in} \(Y\) \emph{that can be explained using} \(X\). As \(R^2\) gets
closer to 1, a large proportion of the variability in the response has
been explained by the regression. A number near 0 indicates that the
regression did not explain much of the variablity in the response; this
might occur because the linear model is wrong, or the inherit error
\(\sigma^2 = \text{RSE}^2\) is high, or both.

Lets calculate \(R^2\) of our estimation on \texttt{advertising} data
with the model \(\hat{sales_i} = \hat{\beta_0} + \hat{\beta_1}TV_i\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sales\_hat =}\NormalTok{ beta\_0\_hat\_adv }\SpecialCharTok{+}\NormalTok{ beta\_1\_hat\_adv }\SpecialCharTok{*}\NormalTok{ TV) }\OtherTok{{-}\textgreater{}}\NormalTok{ advertising}
\NormalTok{advertising}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 200 x 5
      TV radio newspaper sales sales_hat
   <dbl> <dbl>     <dbl> <dbl>     <dbl>
 1 230.   37.8      69.2  22.1     18.0 
 2  44.5  39.3      45.1  10.4      9.15
 3  17.2  45.9      69.3   9.3      7.85
 4 152.   41.3      58.5  18.5     14.2 
 5 181.   10.8      58.4  12.9     15.6 
 6   8.7  48.9      75     7.2      7.45
 7  57.5  32.8      23.5  11.8      9.77
 8 120.   19.6      11.6  13.2     12.7 
 9   8.6   2.1       1     4.8      7.44
10 200.    2.6      21.2  10.6     16.5 
# i 190 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSS }\OtherTok{=} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{sales }\SpecialCharTok{{-}}\NormalTok{ advertising}\SpecialCharTok{$}\NormalTok{sales\_hat)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{RSS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2102.531
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSE }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(RSS}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{length}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales) }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{))}
\NormalTok{RSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.258656
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TSS }\OtherTok{=} \FunctionTok{sum}\NormalTok{((advertising}\SpecialCharTok{$}\NormalTok{sales }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(advertising}\SpecialCharTok{$}\NormalTok{sales))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{TSS}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5417.149
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R2 }\OtherTok{=}\NormalTok{ (TSS }\SpecialCharTok{{-}}\NormalTok{ RSS) }\SpecialCharTok{/}\NormalTok{ TSS}
\NormalTok{R2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6118751
\end{verbatim}

61\% of the variablity in \texttt{sales} is explained by a linear
regression on \texttt{TV}.

what about our \texttt{data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
        y      x  y_hat
    <dbl>  <dbl>  <dbl>
 1 -0.591 -0.667 -0.159
 2  2.69   0.222  2.56 
 3 -2.61  -1.03  -1.27 
 4 -3.54  -1.39  -2.38 
 5  1.54  -0.545  0.212
 6  2.22   0.384  3.05 
 7 -1.34  -1.56  -2.88 
 8  6.81   1.39   6.14 
 9  6.26   1.43   6.27 
10  2.39   0.465  3.30 
# i 90 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSS\_data }\OtherTok{=} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y\_hat)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{RSS\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 82.28514
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RSE\_data }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(RSS\_data}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{length}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y) }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{))}
\NormalTok{RSE\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9163211
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TSS\_data }\OtherTok{=} \FunctionTok{sum}\NormalTok{((data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{y))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{TSS\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1439.473
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R2\_data }\OtherTok{=}\NormalTok{ (TSS\_data }\SpecialCharTok{{-}}\NormalTok{ RSS\_data) }\SpecialCharTok{/}\NormalTok{ TSS\_data}
\NormalTok{R2\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9428366
\end{verbatim}

94\% of the variablity in \texttt{y} is explained by \texttt{x}; very
good fit of the model.

\(R^2\) is better to interpret than RSE.

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

In practice we have more than one predictor to explain \(Y\).

How can we extend our analysis of the advertising order to accomodate
the other two (\texttt{radio} and \texttt{newspaper}) additional
predictors?

=\textgreater{} We can run three separate simple linear regressions,
each of which uses a different advertising medium as a predictor:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3860 -1.9545 -0.1913  2.0671  7.2124 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843   15.36   <2e-16 ***
TV          0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ radio, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ radio, data = advertising)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.7305  -2.1324   0.7707   2.7775   8.1810 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  9.31164    0.56290  16.542   <2e-16 ***
radio        0.20250    0.02041   9.921   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.275 on 198 degrees of freedom
Multiple R-squared:  0.332, Adjusted R-squared:  0.3287 
F-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ newspaper, data = advertising)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.2272  -3.3873  -0.8392   3.5059  12.7751 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***
newspaper    0.05469    0.01658    3.30  0.00115 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.092 on 198 degrees of freedom
Multiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 
F-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148
\end{verbatim}

We find that on average, \$1,000 increase in spending on radio
advertising is associated with an increase in sales by around 203 units.

We find that on average, \$1,000 increase in spending on newspaper
advertising is associated with an increase in sales by around 55 units.

We find that on average, \$1,000 increase in spending on TV advertising
is associated with an increase in sales by around 47 units.

\textbf{However} this approach is not good. First of all it is unclear
to make a sinlge prediction of sales given levesl of the three
advertising media budgets, since each has their own regression equation.
Second, each of these three regression equations ignores the other two
medi in forming estimates for the regression coefficients. Especially if
these media budgets are correalted, this can lead to very misleading
estimates of the individaul media effects on sales.

Instead of the seperate linear regressions for each predictor, better
approach is to extend the simple linear regression setting
\(Y = \beta_0 + \beta_1 X\) to

\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
\]

(3.19)

Here we interpret \(B_j\) as the average effect of a one unit increase
in \(X_j\), \emph{holding all other predictors fixed}.

For the advertising example;

\[
sales = \beta_0 + \beta_1 TV + \beta_2 radio + \beta_3 newspaper + \epsilon
\]

\hypertarget{estimating-the-regression-coefficients}{%
\subsection{Estimating the Regression
Coefficients}\label{estimating-the-regression-coefficients}}

Again, regression coefficients in (3.19) are unknown, and must be
estimated from the data. And with these estimates we can make
predictions

\[
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots + \hat{\beta_p}x_p
\] (3.21)

The parameters are estimated using the same least squares approach with
simple linear regression. We choose \(\beta_0, \beta_1, \dots, \beta_p\)
to minimize the sum of squared residuals

\$\$ \begin{align}

\text{RSS} &= \sum_{i = 1}^n(y_i - \hat{y_i})^2 \\
&= \sum_{i = 1}^n(y_i - \hat{\beta_0} - \hat{\beta_1}x_{i1} - \beta_2x_{i2} - \dots - \beta_px_{ip})^2

\end{align} \$\$ (3.22)

\(\hat{\beta_0}, \hat{\beta_1},\dots, \hat{\beta_p}\) values minimize
RSS.

We are not going to calculate these estimates with our hands, R does
that.

Lets see our model results with the three predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

\emph{Interpretation:} for a given amount of Tv and newspaper
advertising, spending additional \$1,000 on radio(TV)(newspaper)
advertising leads to an increase in sales approximately by 189(46)(-1)
units.

If we compare these effects with one predictor regressions

\begin{quote}
We find that on average, \$1,000 increase in spending on radio
advertising is associated with an increase in sales by around 203 units.
\end{quote}

\begin{quote}
We find that on average, \$1,000 increase in spending on newspaper
advertising is associated with an increase in sales by around 55 units.
\end{quote}

\begin{quote}
We find that on average, \$1,000 increase in spending on TV advertising
is associated with an increase in sales by around 47 units.
\end{quote}

For tv and radio coefficients are similar, but for \textbf{newspaper}:
from the simple linear regression coefficient of newspaper was
significant, but in multiple linear regression it is not; p value is
very high.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                  2.5 %     97.5 %
(Intercept)  2.32376228 3.55401646
TV           0.04301371 0.04851558
radio        0.17154745 0.20551259
newspaper   -0.01261595 0.01054097
\end{verbatim}

Its confidence interval contains 0.

This difference between simple linear regression and multiple linear
regression coefficients stems from the fact that in the simple
regression, the slope term represents the average effet of a one dollar
increase in newspaper advertising, ignoring other preditors such as tv
and radio. In contrsat, in the multiple regression setting, the
coefficient for newspaper represents the average effect of increeasing
newapper spending by one dollar, while holding tv and radio fixed.

Does it make sense for the multiple regression to suggest no
relationship between sales and newspaper while the simple linear
regression implies the opposite? Yes!

Take a look at this correlation matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(advertising[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                  TV      radio  newspaper     sales
TV        1.00000000 0.05480866 0.05664787 0.7822244
radio     0.05480866 1.00000000 0.35410375 0.5762226
newspaper 0.05664787 0.35410375 1.00000000 0.2282990
sales     0.78222442 0.57622257 0.22829903 1.0000000
\end{verbatim}

We can also make it a plot out of this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(advertising[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]), }\AttributeTok{method =} \StringTok{"number"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-60-1.pdf}

}

\end{figure}

Notice that correlation between radio and newspaper is 0.35. This
reveals a tendency to spend more on newspaper advertising in markets
where more is spent on radio advertising. Now suppose the multiple
regression is correct and newspaper advertising has no direct impact on
sales, but radio advertising does increase sales. Then in markets where
we spend more on radio, our sales will tend to be higher, adn as our
correaltion matrix shows, we also tend to spend more on newspaper
advertising in those same markets. Hence, in a simple linaer regresion
which only examines sales vs newspaper, we will observe that higher
values of newspaper tend to be associated with higher values of sales,
even though newspaper advertising does not actually affect sales. So
newspaper sales are proxy for radio advertising; newspaper gets credit
for the effect of radio on sales.

This is a very common issue. Consider running a regression of shark
attack versus ice cream sales for data collected at a given beach
community. We would see a positive relationship, similar to that seen
between sales and newspaper. Of course ice creams doesnt cause shark
attacks. In reality higher temperatures cause more people to visit the
beach in trun results in more ice cream sales and more shark attacks. A
multiple regression of attacks versus ice cream sales and temperature
revals that, the former predictr is no longer significant after
adjusting for temperature.

\hypertarget{some-important-questions}{%
\subsection{Some important Questions}\label{some-important-questions}}

When we perform MLR, we usually are interested answering a few important
questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Is at least one of the predictors} \(x_1, x_2, \dots, x_p\)
  \emph{useful in predicting the response?}
\item
  \emph{Do all predictors help to explain} \(Y\), \emph{or is only a
  subset of the predictors useful?}
\item
  \emph{How well does the model fit the data?}
\end{enumerate}

4 \emph{Given a set of predictor values, what response value should we
predict, and how accurate is our prediction?}

Lets answer these questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Is at least one of the predictors} \(x_1, x_2, \dots, x_p\)
  \emph{useful in predicting the response?}

  In SLR we simply checked whether \(\beta_1 = 0\) or not. In MLR, we
  need to ask whether all of the regression coefficients are zero
  \(\beta_1 = \beta_2 = \dots = \beta_p = 0\). So our null hypothesis is

  \[
   \begin{align}
   H_0 &: \beta_1 = \beta_2 = \dots = \beta_o = 0 \\
   H_\alpha &: \text{at least one} \space B_j \space \text{is non-zero}
   \end{align}
   \] This hypothesis test is performed by computing the
  \emph{F-statistic},

  \[
   F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}
   \] (3.23)

  So, if there is no relationship between the resposne and predictors,
  we expect F-statistic to take on value close to 1. if \(H_\alpha\) is
  true then \(F\) should be greater than 1.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

F statistic is 570 and is far from 1. But it is best to have a look at
the p-value of the F statistic which is also very small.

This means that at least one of the media is associated with increase
sales.

In (3.23) we are testing \(H_0\) that all the coefficients are zero.
Sometimes we want to test that a particular subset of \(q\) of the
coefficients are zero. This corresponesd to a null hypothesis

\[
  H_0 : \beta_{p-q+1} = \beta_{p-q+2} = \dots = \beta_p
  \]

In this case we fit a second model that uses all the variables
\emph{except} those last \(q\). suppose that residual sum of squares for
that model is \(RSS_0\). Then the appropriate F-statistic is

\[
  F = \frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}
  \] (3.24)

On advertising MLR we saw that newspaper is not significant from its p
value. Then why do we need to look at the overall F-statistic? after
all, it seems likely that if any one of the p-values for the individual
variables is very small, then \emph{at least one of the predictors is
realted to the respose}. This is not true usually, especially when \(p\)
is large.

So after estimating the model first look at the F-statistic, than to the
individual t statistic p values.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \emph{Do all predictors help to explain} \(Y\), \emph{or is only a
  subset of the predictors useful?} =\textgreater{} \textbf{Deciding on
  important variables}

  After lookig at the F statistic, we can look at the individual p
  values. But if *p\$ is large, we are going to make false discoveries.

  Usually not all predictors are associated with the response. This task
  of determining which predictors are associated with the response in
  order to fit a single model involving only those predictors is refered
  to as \emph{variable selection}. Check out Chapter 6 for more detail.
  But here is a breif outline of some of the classical approaches.

  Ideally we want to perform variable selection by trying out a lot of
  different models, each containing different subset of the predictors.
  For instance if our \(p=2\) then we can consider four models

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumii{(\arabic{enumii})}
    \tightlist
    \item
      a model containing no variables
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\arabic{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      a model containing \(x_1\) only
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\arabic{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      a model containnig \(x_2\) only
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\arabic{enumii})}
    \setcounter{enumii}{3}
    \tightlist
    \item
      a model containing \(x1\) and \(x_2\).
    \end{enumerate}
  \end{itemize}

  We can then select the \emph{best} model out of all the models by
  looking at some statistics we can use to judge the quality of the
  model. These are

  \begin{itemize}
  \tightlist
  \item
    \emph{Mallow}'s \(C_p\)
  \item
    \emph{Akaike information creterion} (AIC)
  \item
    \emph{Bayesian information criterion}(BIC)
  \item
    \emph{adjusted} \(R^2\)
  \end{itemize}

  These are discussed in more detail in chapter 6.

  We can also determine which model is the best by plotting various
  model outputs, such as the residuals, in order to search for patterns.

  But we cannot consider all models, especially when \(p\) is high.
  There are three classical approaches for this task:

  \begin{itemize}
  \tightlist
  \item
    \emph{Forward selection}

    \begin{itemize}
    \item
      begin with \emph{null model} a model that contains an intercept
      but no predictors.
    \item
      Then fit \emph{p} simple linear regressions and add to the null
      model the variable that results in the lowest RSS.
    \item
      Then add to that model the variable that results in the lowest RSS
      for the new two-variable model. This approach is continued until
      some stopping rule is satisfied.
    \end{itemize}
  \item
    \emph{Backward selection}

    \begin{itemize}
    \tightlist
    \item
      Put all varaibles in the model.
    \item
      remove the least statistically significant predictor.
    \item
      estimate the new regression with \(p-1\) variable, remove the
      largest p-value predictor. This procedure continues until a
      stopping rule is reached =\textgreater{} stop after all remaining
      variables have p value \textless{} 0.02
    \end{itemize}
  \item
    \emph{Mixed selection}

    \begin{itemize}
    \tightlist
    \item
      Combination of forward selection and backward selection
    \item
      Start with no variables in the model
    \item
      add the varaible that provides the best fit
    \item
      add varaibles one-by-one
    \item
      at one point if the p-value for one of the variables in the model
      rises above a certain treshold, then we remove that variabel from
      the model.
    \item
      Continue untill all variables have sufficiently low p value, and
      all vairables in the model woudl have a large p-value if added to
      the model
    \end{itemize}
  \end{itemize}

  Backwar slecetion cannot be used if \(p>n\), forward selection can
  always be used.
\item
  \emph{How well does the model fit the data?} \textbf{Model Fit}
\end{enumerate}

Two of the most common numerical measures of model fit are RSE and
\(R^2\).

In SLR \(R^2\) is equal to \(cor(Y,X)\). In MLR
\(R^2 = cor(Y,\hat{Y})\).

\(R^2\) will always increase as you add more variable, even though that
varaible is not statistically significant. This is because adding
another variable must allow us to fit the trainig data(not necessarly
test data) more accurately. But this increase in \(R^2\) after adding a
statistically not-significant varible is very low =\textgreater{}
evidence that you can drop the not significant variable. Check out the
\(R^2\) variables of the following models

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

They are almost the same.

But lets see the \(R^2\) of the model containing only Tv

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3860 -1.9545 -0.1913  2.0671  7.2124 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.032594   0.457843   15.36   <2e-16 ***
TV          0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.259 on 198 degrees of freedom
Multiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

it is 0.611.

If we add radio

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

It increaes dramatically. This implies that model that uses TV and radio
to predict sales is better than only using Tv. also radio is
statistically signifiacnt.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.6231 -1.7346 -0.0948  1.8926  8.4512 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 5.774948   0.525338  10.993  < 2e-16 ***
TV          0.046901   0.002581  18.173  < 2e-16 ***
newspaper   0.044219   0.010174   4.346 2.22e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.121 on 197 degrees of freedom
Multiple R-squared:  0.6458,    Adjusted R-squared:  0.6422 
F-statistic: 179.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

Not with newspaper though.

Or the opposite

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ radio, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ radio, data = advertising)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.7305  -2.1324   0.7707   2.7775   8.1810 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  9.31164    0.56290  16.542   <2e-16 ***
radio        0.20250    0.02041   9.921   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.275 on 198 degrees of freedom
Multiple R-squared:  0.332, Adjusted R-squared:  0.3287 
F-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{radio}\SpecialCharTok{+}\NormalTok{TV, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ radio + TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ radio + newspaper, data = advertising)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5289  -2.1449   0.7315   2.7657   7.9751 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 9.188920   0.627672  14.640   <2e-16 ***
radio       0.199045   0.021870   9.101   <2e-16 ***
newspaper   0.006644   0.014909   0.446    0.656    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.284 on 197 degrees of freedom
Multiple R-squared:  0.3327,    Adjusted R-squared:  0.3259 
F-statistic: 49.11 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

What about RSE:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

adding newspaper increased the RSE =\textgreater{} no need to add
newspaper. Adding newspaper increases RSE because

\[
RSE = \sqrt{\frac{1}{n-p-1}RSS}
\]

Models with more variables can have higher RSE if the decrease in RSS is
small relative to the increase in \(p\).

So we can look at both the RSE and \(R^2\).

\textbf{Four: Predictions}

After fitting the model we can predict \(Y\) =\textgreater{} \(\hat{y}\)
with estimated coefficients. However, there are three sorts of
uncertainty associated with this prediction:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The coefficient esstimates
  \(\hat{\beta_0}, \hat{\beta_1},\dots,\hat{\beta_p}\) are estimates for
  \(\beta_0, \beta_1, \dots, \beta_p\):

  That is, the \emph{least squares plane}
\end{enumerate}

\[
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \dots +  \hat{\beta_p}x_p
\]

which is only an estimate for the \emph{true population regression
plane} \[
f(X) = \beta_0 + \beta_1x_1 + \dots + \beta_px_p
\]

So there is an inaccuracy in the coefficient estimates =\textgreater{}
this is the \emph{reducible error} from Chapter 2. We can compute a
\emph{confidence interval} to determine how close \(\hat{y}\) will be to
\(f(X)\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Assuming a linear model for \(f(X)\) is almost always an aaproximation
  of reality (usually relationships are not linear), so ther is an
  additional source of potentially reducible error =\textgreater{} this
  is the \emph{model bias}.

  When we are using a linear model, we are in fact estimating the best
  linear approximation to the true surface. However, we will ignore this
  discrepancy and operate as if the linear model is correct
\item
  Even if we knew \(f(X)\)--that is even if we knew the true values of
  \(\beta\)--the response value cannot be predicted perfectly because of
  the random error \(\epsilon\) in the model =\textgreater{}
  \emph{irreducable error}. How much will \(Y\) vary from \(\hat{y}\)
  =\textgreater{} we use \emph{prediction intervals} to answer this
  question.

  Predicion intervals are always wider than confidence intervals,
  because they contain both the \emph{reducible error}(error from
  estimating coefficients of \(f(X)\)) and irreducible error.
\end{enumerate}

We use a \emph{confidence interval} to quantify the uncertainty
surrounding the \emph{average} \texttt{sales} over a large number of
cities. For example given that \$100,000 is spent on \texttt{TV}
advertising and \$20,000 is spent on \texttt{radio} advertising in each
city, the 95\% confidence interval is \([10,985, 11,528]\). We interpret
this to mean that 95\% of intervals of this form will contain the true
value of \(f(X)\).

On the other hand, \emph{a prediction interval} can be used to quantify
the uncertainty surrounding \texttt{sales} for a \emph{particular} city.
Given that \$100,000 is spent on \texttt{TV} advertising and \$20,000 is
spent on \texttt{radio} advertising in that city the 95\% prediction
interval is \([7,930, 14,580]\). We interpret this to mean that 95\% of
intervals of this form will contain the true value of \(Y\) for this
city. Note that both intervals are centered at 11,256, but that the
prediction intervaÅ is substantially wider than the confidence interval,
reflecting the increased uncertainty about \texttt{sales} for a given
city in comparison to the average \texttt{sales} over many locations.

\hypertarget{other-considerations-in-the-regression-model}{%
\section{Other Considerations in the Regression
Model}\label{other-considerations-in-the-regression-model}}

\hypertarget{qualitative-predictors}{%
\subsection{Qualitative Predictors}\label{qualitative-predictors}}

In practice not all variables are \emph{quantitative}; some predictors
are \emph{qualitative}.

Check out the \texttt{Credit} data set

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/Credit.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as\_tibble}
\NormalTok{Credit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 400 x 12
      ID Income Limit Rating Cards   Age Education Gender   Student Married
   <int>  <dbl> <int>  <int> <int> <int>     <int> <chr>    <chr>   <chr>  
 1     1   14.9  3606    283     2    34        11 " Male"  No      Yes    
 2     2  106.   6645    483     3    82        15 "Female" Yes     Yes    
 3     3  105.   7075    514     4    71        11 " Male"  No      No     
 4     4  149.   9504    681     3    36        11 "Female" No      No     
 5     5   55.9  4897    357     2    68        16 " Male"  No      Yes    
 6     6   80.2  8047    569     4    77        10 " Male"  No      No     
 7     7   21.0  3388    259     2    37        12 "Female" No      No     
 8     8   71.4  7114    512     2    87         9 " Male"  No      No     
 9     9   15.1  3300    266     5    66        13 "Female" No      No     
10    10   71.1  6819    491     3    41        19 "Female" Yes     Yes    
# i 390 more rows
# i 2 more variables: Ethnicity <chr>, Balance <int>
\end{verbatim}

Lets do a scatterplot of all variables

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pairs}\NormalTok{(.)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-73-1.pdf}

}

\caption{Fig 3.6 The credit data set contains infortmation about
blaance, age, cards, education income, limit and rating for a number of
potential customers}

\end{figure}

\textbf{Predictors with Only Two Levels}

We want to investigate differences in credit card balance between maels
and females, ignoring other variables for the moment. If a qualitative
predictors (also known as \emph{factor}) only has two \emph{levels},
then incorporating it into a regression model is very simple. We create
a \emph{dummy variable} that takes on two possible \emph{numerical}
values. For example based on \texttt{Gender} varaible, we can create a
new varaible that takes the form

\[
x_i = 
\begin{cases}
1 & \text{if}\space i\text{th} \space\text{person is female} \\
0 & \text{if}\space i\text{th} \space\text{person is male}
\end{cases}
\] (3.26)

and use this variable as a predictor in the regression equation. This
results in the model

\[
Y_i = \beta_0 + \beta_1x_i + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1 + \epsilon_i & i\text{th} \space\text{person is female} \\
\beta_0 + \epsilon_i & i\text{th} \space\text{person is male}
\end{cases}
\] (3.27)

Now \(\beta_0\) can be interpreted as the average credit card balance
among males, \(\beta_0 + \beta_1\) as the average credit card among
females, and \(\beta_1\) as the average difference in credit card
balance between females and males.

Here is the regression results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit}\SpecialCharTok{$}\NormalTok{Gender }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Credit}\SpecialCharTok{$}\NormalTok{Gender, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{" Male"}\NormalTok{,}\StringTok{"Female"}\NormalTok{))}
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = Balance ~ Gender, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-529.54 -455.35  -60.17  334.71 1489.20 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    509.80      33.13  15.389   <2e-16 ***
GenderFemale    19.73      46.05   0.429    0.669    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 460.2 on 398 degrees of freedom
Multiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 
F-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685
\end{verbatim}

R converts all \emph{``Female''} values to 1 automatically. If we were
to do this manually

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{as.character}\NormalTok{(Gender)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{ifelse}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"Female"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, .) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Balance ~ Gender, data = .)

Residuals:
    Min      1Q  Median      3Q     Max 
-529.54 -455.35  -60.17  334.71 1489.20 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   509.80      33.13  15.389   <2e-16 ***
Gender         19.73      46.05   0.429    0.669    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 460.2 on 398 degrees of freedom
Multiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 
F-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685
\end{verbatim}

This means that average credit card debt for males is estimated to be
\$509.80, whereas females are estimated to carry \$19.73 in additional
debt for a total of \(\$509.80 + \$19.73 = \$529.53\). However, the
coefficient of the dummy variable is not significant; there is no
statistical evidence of a difference in average credit card balance
between the genders.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Gender, }\AttributeTok{y =}\NormalTok{Balance, }\AttributeTok{fill =}\NormalTok{ Gender) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{(Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Gender) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Balance =} \FunctionTok{mean}\NormalTok{(Balance))),}\AttributeTok{shape =} \DecValTok{4}\NormalTok{, }\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+} \FunctionTok{theme\_clean}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-76-1.pdf}

}

\end{figure}

The desicion to code females as 1 and males as 0 in (3.27) is arbitrary,
and has no effect on the regression fit, but does alter the
interpretation of the coefficients. If we had coded males as 1 and
females as 0:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{factor}\NormalTok{(Gender, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{" Male"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender,.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Balance ~ Gender, data = .)

Residuals:
    Min      1Q  Median      3Q     Max 
-529.54 -455.35  -60.17  334.71 1489.20 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   529.54      31.99  16.554   <2e-16 ***
Gender Male   -19.73      46.05  -0.429    0.669    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 460.2 on 398 degrees of freedom
Multiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 
F-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685
\end{verbatim}

Then we would say that estimated average debt for females is \$529.54,
and for males is \(\$529.52 - \$19.73 = \$509.80\).

Alternatively, instead of 0/1 coding scheme, we could create a dummy
variable

\[
x_i = 
\begin{cases}
1 & \text{if}\space i\text{th}\space \text{person is female} \\
-1 & \text{if}\space i\text{th}\space \text{person is male}
\end{cases}
\] and use this variable in the regression equation. This results in the
model

\[
Y_i = \beta_0 + \beta_1x_i + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1 + \epsilon_i & \text{if}\space i\text{th}\space \text{person is female} \\
\beta_0 - \beta_1 + \epsilon_i & \text{if}\space i\text{th}\space \text{person is male} 
\end{cases}
\] Now \(\beta_0\) can be interpreted as the overall average credit card
balance (ignoring the gender effect), and \(\beta_1\) is the amount that
females are above the average and males are below the average.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{as.character}\NormalTok{(Gender)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{ifelse}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"Female"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender,.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Balance ~ Gender, data = .)

Residuals:
    Min      1Q  Median      3Q     Max 
-529.54 -455.35  -60.17  334.71 1489.20 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  519.670     23.026  22.569   <2e-16 ***
Gender         9.867     23.026   0.429    0.669    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 460.2 on 398 degrees of freedom
Multiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 
F-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685
\end{verbatim}

Now \(\beta_0\) is \$ 519.670 which is the halfway between the male and
female averages of \$509.80 and \$529.53. The estimate for \(\beta_1\)
is \$9.865, which is half of \$19.74, the average difference between
females and males.

\textbf{Qualitative Predictors with More than Two levels}

In this case we need to create an additional dummy. For example have a
look at the \texttt{Ethnicity} variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Ethnicity) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 1
  Ethnicity       
  <chr>           
1 Caucasian       
2 Asian           
3 African American
\end{verbatim}

Has three possible values. Then we need to create two dummies

\[
x_{i1} = 
\begin{cases}
1 & \text{if}\space i\text{th}\space \text{person is Asian} \\
0 & \text{if}\space i\text{th}\space \text{person is not Asian} 
\end{cases}
\]

and

\[
x_{i2} = 
\begin{cases}
1 & \text{if}\space i\text{th}\space \text{person is Caucasian} \\
0 & \text{if}\space i\text{th}\space \text{person is not Caucasian} 
\end{cases}
\]

Then both these varaibles can be used in the regression equation, in
order to obtain the model

\[
Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i = 
\begin{cases}
\beta_0 + \beta_1 + \epsilon_i & \text{if}\space i\text{th}\space \text{person is Asian} \\
\beta_0 + \beta_2 + \epsilon_i & \text{if}\space i\text{th}\space \text{person is Caucasian} \\
\beta_0 + \epsilon_i & \text{if}\space i\text{th}\space \text{person is African American} 
\end{cases}
\] Now \(\beta_0\) can be interpreted as the average credit card balance
for African Americans, \(\beta_1\) can be interpreted as the difference
in the average balance between Asian and African american categories,
and \(\beta_2\) can be interpreted as the difference in average balance
between the Caucasian and African American categories.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Ethnicity, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = Balance ~ Ethnicity, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-531.00 -457.08  -63.25  339.25 1480.50 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)          531.00      46.32  11.464   <2e-16 ***
EthnicityAsian       -18.69      65.02  -0.287    0.774    
EthnicityCaucasian   -12.50      56.68  -0.221    0.826    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 460.9 on 397 degrees of freedom
Multiple R-squared:  0.0002188, Adjusted R-squared:  -0.004818 
F-statistic: 0.04344 on 2 and 397 DF,  p-value: 0.9575
\end{verbatim}

There will always be one fewer dummy variable than the number of levels.
The level with no dummy variable--African American in this example--is
known as the \emph{baseline}.

From the regression results we see that the estimated \texttt{balance}
for the baseline, African American, is \$531.00. It is estimated that
Asian category will have \$18.69 less debt than the African American
category on average, and the Caucasian category will have \$12.50 less
debt that the African American category. However, p-values associated
with the coefficient estimates for the two dummy variables are very
large, suggesting no statistical evidence of a real difference in credit
card balance between the ethnicities. The coefficients will change
bassed on the baseline and coding.

Rather than relying on the individual coefficients, we can use F-test to
test \(H_0 : \beta_1 = \beta_2 = 0\); this does not depend on the
coding. The F-test has a p-value of 0.9575, indicating that we cannot
reject the null hypothesis that there is no relationship between
\texttt{balance} and \texttt{ethnicity}.

Using this dummy variable approach presents no difficulties when using
both quantitative and qualitative predictors. For example, to regress
\texttt{balance} on both a quantitative varaible such as \texttt{income}
and a qualitative variable such as \texttt{student}, we must simply
create a dummy varaible for \texttt{student} and then fit a multiple
linear regression model using \texttt{income} and the dummy variable as
the predictors for credit card balance.

\hypertarget{extensions-of-the-linear-model}{%
\subsection{Extensions of the Linear
Model}\label{extensions-of-the-linear-model}}

Linear regression model is very interpretable and works quite well on
many real-world problems. However, it makes several highly restrictive
assumptions that are ofthen violated in practice. Two of them ost
important assumptions state that the relationship between the predictors
and response are \emph{additive} and \emph{linear}.

The \emph{additive} assumption means that the effect of changes in a
predictor \(x_j\) on the response \(Y\) is independent of the vlaues of
the other predictors.

The \emph{linear} assumption means thatt the change in the response
\(Y\) due to one-unit change in \(x_j\) is constant, regardless of the
vlaue of \(x_j\).

We can relax these assumptions. Here some classical approaches to do
that

\textbf{Removing the Additive Assumption}

From \texttt{Advertising} data, we concluded that both \texttt{TV} and
\texttt{radio} seem to be associated with \texttt{sales}. Our model was
linear; the effect of \texttt{TV} and \texttt{radio} advertising
spending on sales is independent of each other, and their effect is
constant no matter the level of spending.

\[
Y_i = \beta_0 + \beta_1 TV_i + \beta_2 radio_i + \epsilon_i
\] this model means that, the average effect on sales of a one unit
increase in tv is always \(\beta_1\) regardless of the amount spent on
radio.

However, this simple model may be incorrect. Suppose that spending money
on radio advertising actually increases the effectiveness of TV
advertising, so that the slope term for \texttt{TV} should increase as
\texttt{radio} increases. In this situation, given a fixed budget
\$100,000, spending half on \texttt{radio} and half on \texttt{TV} may
increase \texttt{sales} more than allocating the entire amount to either
\texttt{TV} or to \texttt{radio}. In marketing this is known as the
\emph{synergy effect}, in statistics \emph{interaction effect}.

Consider the standard linear regression with two variables,

\[
Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon
\] Here if we increaes \(x_1\) by one unit, \(Y\) will increase by an
average of \(\beta_1\) units. The presence of \(x_2\) does not alter
this statement-regardless of the value of \(x_2\), a one-unit increase
in \(x_1\) will lead to \(\beta_1\) unit increae in \(Y\).

We can extend this model by allowing interaction effects by including a
third predictor, called an \emph{interaction term}, which is constructed
by computing the product of \(x_1\) and \(x_2\):

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon
\] (3.31)

We can write this equation

\[
\begin{align}
Y &= \beta_0 + (\beta_1 + \beta_3 x_2)x_1 + \beta_2x_2 +\epsilon \\
&= \beta_0 + \tilde{\beta_1}x_1 + \beta_2x_2 + \epsilon
\end{align}
\] where \(\tilde{\beta_1} = \beta_1 + \beta_3x_2\). Since
\(\tilde{\beta_1}\) changes with \(x_2\), the effect of \(x_1\) on \(Y\)
is no longer constant: adjusting \(x_2\) will change the impact of
\(x_1\) on \(Y\).

\begin{tcolorbox}[enhanced jigsaw, colback=white, leftrule=.75mm, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, breakable, toptitle=1mm, titlerule=0mm, toprule=.15mm, left=2mm, bottomtitle=1mm, coltitle=black, arc=.35mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Productivy of a factory}, opacityback=0, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm]

Suppose that we are interested in studying the productivity of a
factory. We want to predict the number of \texttt{units} produced on the
basis of the number of production \texttt{lines} and the total number of
\texttt{workers}. Probably the effect of increasing \texttt{lines} on
\texttt{units} produced will depend on the number of \texttt{workers},
since if no workers are available to operate the lines, then increasing
the number of lines will not increase the production. This suggest to
include an interaction term between \texttt{lines} and \texttt{workers}
in a linear model to predict \texttt{units}.

Suppose when we fit the model, we obtain

\[
\begin{align}
units &\approx 1.2 + 3.4 \times lines + 0.22 \times workers + 1.4 \times (lines \times workers) \\
&= 1.2 + (3.4 + 1.4 \times workers) \times lines + 0.22 \times workers
\end{align}
\] Adding an addtional line will increase the number of units produced
by \(3.4 + 1.4 \times workers\). Hence, the more \texttt{workers} we
have, the stronger will be the effect of \texttt{lines}.

\end{tcolorbox}

For the advitising, a linear model that uses \texttt{radio}, \texttt{TV}
and interaction between the two to predict \texttt{sales} takes the form

\[
\begin{align}
sales &= \beta_0 + \beta_1 \times TV + \beta_2 \times radio + \beta_3 \times(radio \times TV) + \epsilon \\
&= \beta_0 + (\beta_1 + \beta_3 \times radio)\times Tv + \beta_2 \times radio + \epsilon
\end{align}
\] (3.33) We can interpret \(\beta_3\) as the increase in the
effectiveness of TV advertising for a one unit increse in radio
advertising (or vice versa). The coefficients that result from fitting
the model (3.33) are

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ TV}\SpecialCharTok{*}\NormalTok{radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = sales ~ TV + radio + TV * radio, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.3366 -0.4028  0.1831  0.5948  1.5246 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.750e+00  2.479e-01  27.233   <2e-16 ***
TV          1.910e-02  1.504e-03  12.699   <2e-16 ***
radio       2.886e-02  8.905e-03   3.241   0.0014 ** 
TV:radio    1.086e-03  5.242e-05  20.727   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9435 on 196 degrees of freedom
Multiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 
F-statistic:  1963 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

Adding the interaction term increased the \(R^2\) significantly from
0.8972 to 0.9678, RSE from 1.681 to 0.9435.

The p-value for the interaction term is very low;
\(H_\alpha : \beta_3 \neq 0\). This means that the relationship is not
additive.

To interpret the coefficients:

\$1,000 increse in \texttt{TV} advertising results with
\(\hat{\beta_1} +\hat{\beta_3}\times radio = \$19.1 + \$1.09 \times radio\)
increase in sales on average.

\$1,000 invrease in \texttt{radio} advertising results with
\(\hat{\beta_2} + \hat{\beta_3} \times TV = \$28.9 + \$1.09 \times TV\)
increase in sales on average.

All p-values are significant =\textgreater{} all three varibles should
be included in the model.

Sometimes interaction term has a very small pvalue but the associated
main effects(in this case \texttt{TV} and \texttt{radio}) do not. The
\emph{hieararchial principle states that if we include an interaction in
a model, we should also include the main effects, even if the p-values
associated with their coefficients are not significant.}

Here both our varaibles were quantitative. However, the consept of
interaction applies to qualitative varaibles, or combination of
qualitative and quantitative varibles. Actually interaction between a
quantitative and qualitative variable has a particularly nice
interpreateion:

Consider the \texttt{Credit} data set. Suppose we want to predict
\texttt{balance} using \texttt{income}(quantitative) and
\texttt{student}(qualitative) varaibles. In the absence of an
interaction term the model takes the form

\[
balance_i  \approx \beta_0 + \beta_1 \times income_i + 
\begin{cases}
\beta_2 & \text{if} \space i\text{th} \space \text{person is student} \\
0 & \text{if} \space i\text{th} \space \text{person is not student}
\end{cases}
\]

\[
balance_i = \beta_1 \times income + 
\begin{cases}
\beta_0 + \beta_2 & \text{if} \space i\text{th} \space \text{person is student} \\
\beta_0 &\text{if} \space i\text{th} \space \text{person is student}
\end{cases}
\] (3.34)

Notice that this amounts to fitting two parallel lines to the data, one
for students and one for non-students. The lines for students and non
students have different intercepts, \(\beta_0 + \beta_2\) versus
\(\beta_0\), but the slope \(\beta_1\) is the same.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Income }\SpecialCharTok{+}\NormalTok{ Student, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = Balance ~ Income + Student, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-762.37 -331.38  -45.04  323.60  818.28 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 211.1430    32.4572   6.505 2.34e-10 ***
Income        5.9843     0.5566  10.751  < 2e-16 ***
StudentYes  382.6705    65.3108   5.859 9.78e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 391.8 on 397 degrees of freedom
Multiple R-squared:  0.2775,    Adjusted R-squared:  0.2738 
F-statistic: 76.22 on 2 and 397 DF,  p-value: < 2.2e-16
\end{verbatim}

Average Non student balance is \$211.1430; students have \$382.6705 more
balance on average compared to non students. \$1,000 increase in income
increases balance by \$5,984 on average. All coefficients are
statistically significant. \(R^2 = 0.2775\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{Balance, }\AttributeTok{x=}\NormalTok{Income) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =}\ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FloatTok{211.1430}\NormalTok{, }\AttributeTok{slope =} \FloatTok{5.9843}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size=}\FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \FloatTok{593.8135}\NormalTok{, }\AttributeTok{slope =} \FloatTok{5.9843}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkred"}\NormalTok{, }\AttributeTok{size=} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{1400}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1400}\NormalTok{,}\DecValTok{400}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{theme\_clean}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{data =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{80}\NormalTok{,}\DecValTok{80}\NormalTok{), }\AttributeTok{y =}\FunctionTok{c}\NormalTok{(}\DecValTok{1130}\NormalTok{,}\DecValTok{640}\NormalTok{), }\AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\StringTok{"Student"}\NormalTok{,}\StringTok{"non{-}student"}\NormalTok{)), }\AttributeTok{inherit.aes =}\NormalTok{ F, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y, }\AttributeTok{label=}\NormalTok{z), }\AttributeTok{angle=}\FloatTok{25.9}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ p1}
\NormalTok{p1}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-83-1.pdf}

}

\end{figure}

Notice the since the slopes are same, two lines are parallel; average
effect of income on balance does not depend on whether or not the
individual is a student. This is a limitation; change in income may have
a very different effect on the credit card balance of a student versus
non student.

This limitation can be addressed by adding an interaction varaible,
created by multiplying \texttt{student} and \texttt{income} with the
dummy for student. Our model now becomes

\[
balance_i \approx \beta_0 + \beta_1 \times income_i + 
\begin{cases}
\beta_2 + \beta_3 \times income_i & \text{if student} \\
0 & \text{if not student}
\end{cases}
\] \[
balance_i = 
\begin{cases}
(\beta_0 + \beta_2) + (\beta_1 + \beta_3) \times income_i & \text{if student} \\
\beta_0 + \beta_1 \times income_i & \text{if not student}
\end{cases}
\] (3.35)

Again, our intercepts differ based on whether individual is student or
not. However, this time slope also differs!

Lets estimate this model

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Income }\SpecialCharTok{+}\NormalTok{ Student }\SpecialCharTok{+}\NormalTok{ Income }\SpecialCharTok{*}\NormalTok{ Student, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = Balance ~ Income + Student + Income * Student, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-773.39 -325.70  -41.13  321.65  814.04 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       200.6232    33.6984   5.953 5.79e-09 ***
Income              6.2182     0.5921  10.502  < 2e-16 ***
StudentYes        476.6758   104.3512   4.568 6.59e-06 ***
Income:StudentYes  -1.9992     1.7313  -1.155    0.249    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 391.6 on 396 degrees of freedom
Multiple R-squared:  0.2799,    Adjusted R-squared:  0.2744 
F-statistic:  51.3 on 3 and 396 DF,  p-value: < 2.2e-16
\end{verbatim}

This results in

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{intercept =} \FunctionTok{c}\NormalTok{(}\FloatTok{200.6232}\NormalTok{, }\FloatTok{200.6232+476.6758}\NormalTok{),}
  \AttributeTok{income =} \FunctionTok{c}\NormalTok{(}\FloatTok{6.2182}\NormalTok{, }\FloatTok{6.2182} \SpecialCharTok{{-}} \FloatTok{1.9992}\NormalTok{),}
  \AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"Non{-}student"}\NormalTok{, }\StringTok{"student"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  intercept income type       
      <dbl>  <dbl> <chr>      
1      201.   6.22 Non-student
2      677.   4.22 student    
\end{verbatim}

and becomes

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Credit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Income, }\AttributeTok{y =}\NormalTok{ Balance) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =}\ConstantTok{NA}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{201}\NormalTok{, }\AttributeTok{slope =} \FloatTok{6.22}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =}\FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{677}\NormalTok{, }\AttributeTok{slope =} \FloatTok{4.22}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkred"}\NormalTok{) }\SpecialCharTok{+}  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{1400}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1400}\NormalTok{,}\DecValTok{400}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{theme\_clean}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{data =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{80}\NormalTok{,}\DecValTok{80}\NormalTok{), }\AttributeTok{y =}\FunctionTok{c}\NormalTok{(}\DecValTok{1130}\NormalTok{,}\DecValTok{640}\NormalTok{), }\AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\StringTok{"Student"}\NormalTok{,}\StringTok{"non{-}student"}\NormalTok{)), }\AttributeTok{inherit.aes =}\NormalTok{ F, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y, }\AttributeTok{label=}\NormalTok{z), }\AttributeTok{angle=}\FloatTok{25.9}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2}
\NormalTok{p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-86-1.pdf}

}

\end{figure}

Lets merge this two plots together

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(p1,p2, }\AttributeTok{ncol =}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-87-1.pdf}

}

\end{figure}

Slope for studens is lower than the slope for non-students
=\textgreater{} increases in income are associated with smaller
increases in credit card balance among students as copared to
non-students.

\textbf{Not-linear Relationships}

Linear model assumes linear relationship between the response and the
predictors. However, this is often not true. We can relax this
assumption using \emph{polynomial regression}.

Consider \texttt{Auto}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{horsepower, }\AttributeTok{y =}\NormalTok{ mpg) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =}\DecValTok{1}\NormalTok{, }\AttributeTok{size =}\FloatTok{1.4}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =}\NormalTok{ F, }\AttributeTok{color =}\StringTok{"\#FEBF63"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x,}\DecValTok{2}\NormalTok{), }\AttributeTok{color =}\StringTok{"\#7FDBDA"}\NormalTok{, }\AttributeTok{se =}\NormalTok{ F) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x,}\DecValTok{5}\NormalTok{), }\AttributeTok{color =}\StringTok{"\#ADE498"}\NormalTok{, }\AttributeTok{se=}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{theme\_blank}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-88-1.pdf}

}

\caption{linear regresion fit is in orange, blue curve is a linear
regression fit for a model that includes horsepower\^{}2, and green
curve includes up to horsepower\^{}5}

\end{figure}

It is obvious that there is relationship between \texttt{mpg} and
\texttt{horsepower}. But it seems like this relationship is not linear:
the data suggest a curved relationship.

A simple approach for incorporating non linear associations in a linear
model is to include transformed versions of the predictors in the model.
For example figure above seem to have a \emph{quadratic} shape,
suggesting that a model of the form

\[
mpg = \beta_0 + \beta_1 \times horsepower + \beta_2 \times horsepower^2 + \epsilon
\] May be a better fit. Equation

You can do it in different ways: 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{hp.sq =}\NormalTok{ horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{horsepower }\SpecialCharTok{+}\NormalTok{ hp.sq,.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ horsepower + hp.sq, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.7135  -2.5943  -0.0859   2.2868  15.8961 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 56.9000997  1.8004268   31.60   <2e-16 ***
horsepower  -0.4661896  0.0311246  -14.98   <2e-16 ***
hp.sq        0.0012305  0.0001221   10.08   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.374 on 389 degrees of freedom
Multiple R-squared:  0.6876,    Adjusted R-squared:  0.686 
F-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), Auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.7135  -2.5943  -0.0859   2.2868  15.8961 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)     56.9000997  1.8004268   31.60   <2e-16 ***
horsepower      -0.4661896  0.0311246  -14.98   <2e-16 ***
I(horsepower^2)  0.0012305  0.0001221   10.08   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.374 on 389 degrees of freedom
Multiple R-squared:  0.6876,    Adjusted R-squared:  0.686 
F-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(horsepower,}\DecValTok{2}\NormalTok{, }\AttributeTok{raw=}\NormalTok{T), Auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ poly(horsepower, 2, raw = T), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.7135  -2.5943  -0.0859   2.2868  15.8961 

Coefficients:
                                Estimate Std. Error t value Pr(>|t|)    
(Intercept)                   56.9000997  1.8004268   31.60   <2e-16 ***
poly(horsepower, 2, raw = T)1 -0.4661896  0.0311246  -14.98   <2e-16 ***
poly(horsepower, 2, raw = T)2  0.0012305  0.0001221   10.08   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.374 on 389 degrees of freedom
Multiple R-squared:  0.6876,    Adjusted R-squared:  0.686 
F-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{nls}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{*}\NormalTok{ horsepower }\SpecialCharTok{+}\NormalTok{ c }\SpecialCharTok{*}\NormalTok{ horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Auto, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\AttributeTok{a=}\DecValTok{0}\NormalTok{,}\AttributeTok{b=}\DecValTok{0}\NormalTok{,}\AttributeTok{c=}\DecValTok{0}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Formula: mpg ~ a + b * horsepower + c * horsepower^2

Parameters:
    Estimate Std. Error t value Pr(>|t|)    
a 56.9000997  1.8004268   31.60   <2e-16 ***
b -0.4661896  0.0311246  -14.98   <2e-16 ***
c  0.0012305  0.0001221   10.08   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.374 on 389 degrees of freedom

Number of iterations to convergence: 1 
Achieved convergence tolerance: 1.293e-09
\end{verbatim}

--

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower, Auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ horsepower, data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5710  -3.2592  -0.3435   2.7630  16.9240 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 39.935861   0.717499   55.66   <2e-16 ***
horsepower  -0.157845   0.006446  -24.49   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.906 on 390 degrees of freedom
Multiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 
F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),Auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.7135  -2.5943  -0.0859   2.2868  15.8961 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)     56.9000997  1.8004268   31.60   <2e-16 ***
horsepower      -0.4661896  0.0311246  -14.98   <2e-16 ***
I(horsepower^2)  0.0012305  0.0001221   10.08   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.374 on 389 degrees of freedom
Multiple R-squared:  0.6876,    Adjusted R-squared:  0.686 
F-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16
\end{verbatim}

The \(R^2\) of the quadratic fit is 0.688, compared to 0.606 for the
linear fit. And the p-value is highly significant for quadratic model.

If adding \(horsepower^2\) increses the \(R^2\) why not include \(^3\)
or \(^4\) or more?

Lets estimate the green curve on the previous plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(horsepower }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(horsepower,}\DecValTok{5}\NormalTok{, }\AttributeTok{raw=}\NormalTok{T),Auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in summary.lm(lm(horsepower ~ poly(horsepower, 5, raw = T), Auto)):
essentially perfect fit: summary may be unreliable
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = horsepower ~ poly(horsepower, 5, raw = T), data = Auto)

Residuals:
       Min         1Q     Median         3Q        Max 
-7.256e-13 -1.620e-15 -1.000e-16  3.540e-15  3.819e-14 

Coefficients:
                                Estimate Std. Error    t value Pr(>|t|)    
(Intercept)                    2.739e-13  2.468e-13  1.110e+00    0.268    
poly(horsepower, 5, raw = T)1  1.000e+00  1.125e-14  8.887e+13   <2e-16 ***
poly(horsepower, 5, raw = T)2  1.496e-16  1.946e-16  7.690e-01    0.442    
poly(horsepower, 5, raw = T)3 -1.196e-18  1.598e-18 -7.480e-01    0.455    
poly(horsepower, 5, raw = T)4  4.368e-21  6.255e-21  6.980e-01    0.485    
poly(horsepower, 5, raw = T)5 -5.894e-24  9.373e-24 -6.290e-01    0.530    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.736e-14 on 386 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
F-statistic: 8.299e+31 on 5 and 386 DF,  p-value: < 2.2e-16
\end{verbatim}

The green fit seems unnecassasrliy wiggly- that is it is unclear
includig the additional terms really has led to a better fit to the
data.

The approach we have just described for extending the linear model to
accomodate non-linear relationships is known as \emph{polynomial
regression}, since we have included polynomial functions of the
predictors in the regression model. Check out Chapter 7.

\hypertarget{potential-problems}{%
\subsection{Potential Problems}\label{potential-problems}}

When we fit a linear regression model to a particular data set, many
problems may occur. Most common among these are the following

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Non-linearity of the response-predictor relationships}.
\item
  \emph{Correalation of error terms}
\item
  \emph{Non-constant variance of error terms}
\item
  \emph{Outliers}
\item
  \emph{High-leverage points}
\item
  \emph{Collinearity}
\end{enumerate}

Lets breifly discuss each

\textbf{1.Non-linearity of the Data}

Linear regression assumes there is a straight line relationship between
the predictors and the respose. If the true relationship is far from
linear, then our estimations are not good. Also our prediction accuracy
is reduced.

\emph{Residual plots} are a useful graphical tool for identifying
non-linearity.

We can plot the residuals \(e_i = y_i - \hat{y_i}\) versus \(x_i\) for
Simple linear regression.

For multiple linear regression since we have lost of \(x_i\) we plot
residuals versus predicted values(\(e_i\) vs \(\hat{y_i}\)).

We want to have \emph{no visible pattern}. If there is a pattern
=\textgreater{} problem!

So what do we expect a plot of ``fitted'' vs ``resid''; if the
relationship is linear as the ``fitted'' values increase ``resid''
should not increase or decrease, should swing with no relationship:
linear.

If the relationship is not linear then we expect ot see a pattern
because as the fitted values increase the resiudls will increase.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower, Auto) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 392 x 9
   .rownames   mpg horsepower .fitted .resid    .hat .sigma   .cooksd .std.resid
   <chr>     <dbl>      <dbl>   <dbl>  <dbl>   <dbl>  <dbl>     <dbl>      <dbl>
 1 1            18        130   19.4  -1.42  0.00368   4.91   1.54e-4    -0.289 
 2 2            15        165   13.9   1.11  0.00888   4.91   2.31e-4     0.227 
 3 3            18        150   16.3   1.74  0.00613   4.91   3.91e-4     0.356 
 4 4            16        150   16.3  -0.259 0.00613   4.91   8.66e-6    -0.0530
 5 5            17        140   17.8  -0.838 0.00473   4.91   6.96e-5    -0.171 
 6 6            15        198    8.68  6.32  0.0177    4.90   1.52e-2     1.30  
 7 7            14        220    5.21  8.79  0.0256    4.89   4.33e-2     1.82  
 8 8            14        215    6.00  8.00  0.0236    4.89   3.30e-2     1.65  
 9 9            14        225    4.42  9.58  0.0276    4.89   5.57e-2     1.98  
10 10           15        190    9.95  5.05  0.0152    4.91   8.31e-3     1.04  
# i 382 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower, Auto) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{horsepower, }\AttributeTok{y =}\NormalTok{ .resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se=}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{theme\_light}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-97-1.pdf}

}

\end{figure}

Here we observe a U-shape trend; indicating non-linear relationship
between \texttt{horsepower} and \texttt{mpg}.

Lets do a plot of fitted vs residual for linear and non linear
regression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower, Auto) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{.fitted, }\AttributeTok{y=}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se=}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_light}\NormalTok{()}
\NormalTok{,}

\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ horsepower }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), Auto) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{.fitted, }\AttributeTok{y=}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se=}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_light}\NormalTok{(), }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-98-1.pdf}

}

\end{figure}

Here is the plots. On the left fitted vs resid for regressing horsepower
onto mpg, on the right regressing horsepower and horsepower\^{}2 onto
mpg.

Blue line is a smooth fit to the residuals to identify trends easily.
The residuals on the left panel exhibit a clear U shape, which is a
strong indication that the true relationship is not linear.

Right hand panel displays a little pattern, suggesting that the
quadratic term improves the fit to the data.

If the residual plot indicates that there are non-linear associations in
the data, then a simple approach is to use non-linear transformations of
the predictors, such as \(\log X\), \(\sqrt{X}\), and \(X^2\), in the
regression model. We will discuss more later on.

\textbf{2.Correlation of Error Terms}

An important assumption of the linear regression is that error terms
\(\epsilon_1, \epsilon_2, \dots, \epsilon_n\) are uncorrelated. This
means that \(\epsilon_{i+1} \neq f(\epsilon_i)\). The standard errors
that are computed for the estimated regression coeffieints or the fitted
values are based on this assumption. If there is correaltion among the
error terms, the estimated standard errors will tend to underestimate
the true standard errors =\textgreater{} confidence and prediction
intervals will be narrower than they should be. P-values will be lower
as well, causing false discoveries perhaps.

Usually correlation among error terms occur in \emph{time series} data.
In many cases, observations that are obtained at adjacent time points
will have positevely correalated errors. To test this we can plot the
residuals from our model as a function of time. If the errors are
uncorrelated, then we expect no pattern. If the error terms are
positevly correlated, then we may see \emph{tracking} in the
residuals--\emph{adjacent residuals may have similar values}.

\includegraphics{fig3.10.png} Check out Figure 3.10. In the top panel,
we see the residuals from a linear regression fit to data generated with
uncorrelated errors. Adjacent years don't take similar values; there is
no evidence of a time-related trend in the residuals. But in the bottom
panel, we see plotted residuals which have correlation 0.9 with time:
now there is a clear pattern in the residuals--adjacent residuals tend
to take on similar values. Finally, the center panel illustrates a more
moderate case in which residuals had a correlation of 0.5. There is
still evidence of tracking, but the pattern is less clear.

Correlation among the error terms can also occur outside of time series
data. For instance, consider a study where we predict people's hights
from their weights. The assumption of uncorrelated errors could be
violated if some of the individauls in the study are members of the same
family, or eat the same diet, or have been exposed to same environmental
factors.

\textbf{3.Non-constant Variance of Error Terms}

Another important assumption of the linear regression model is that
error terms have a constant variance,
\(\text{Var}(\epsilon_i) = \sigma^2\). The standard errors, confidence
intervals, and hypothesis tests rely upon this assumption.

Unfortunately, often variances of the error terms are non-constant
=\textgreater{} \emph{heteroscedasticity}. For instance, variances of
the error terms may increase with the value of the response. We can
identify non-constant variances in the errors from the presence of a
\emph{funnel shape} in the residual plot.

\includegraphics{fig3.11.png} On the left panel magnitude of the
residuals tends to increase with the fitted values. When faced with this
problem, one possible solution is to transform the response \(Y\) using
a concave function such as \(\log Y\) or \(\sqrt{Y}\). Such a
transformation results in a greater amount of shrinkage of the larger
responses, leading to a reduction in heteroscedasticity. On the right
hand panel we see that residuals apper to have constant variance.

\textbf{4.Outliers}

An \emph{outlier} is a point for which \(y_i\) is far from the vlaue
predicted by the model.

\includegraphics{fig3.12.png}

The red point (observation 20) in the left-hand panel of Figure 3.12
illustrates a typical outlier. The red solid line is the least squares
regression fit, while the blue dashed line is the least squares fit
after removel of the outlier. In this case, removing the outlier has
little effect on the least squares line: it leads to almost no change in
the slope, and a minuscule reduction in the intercept. This is normal.
However, it can cause other problems: RSE is 1.09 when outlier is
included, but it is only 0.77 when excluded =\textgreater{} RSE is used
to calculate all confidence intervals, p values =\textgreater{} bias.
\(R^2\) also decreases from 0.829 to 0.805 when included outlier.

Residual plots can be used to identify outliers =\textgreater{} center
panel of Figure 3.12. But it can be difficult to decide how large a
residual needs to before we consider the point to be an outlier. To
address this problem instead of plotting the residuals we can plot the
\emph{studentized residuals} computed by dividing each residuls \(e_i\)
by its estimated standard error.

\[
e_i^*=\frac{e_i}{sd(e_i)} = \frac{e_i}{\sqrt{MSE(1-h_{ii})}}
\] \(h_{ii}\) is leverage point.

Observations whose studentized residuals are greater than 3 in absolute
value are possible outliers. In the right-hand panel of Figure 3.12 the
outlier's studentized residual exceeds 6; while all other observations
have studentized residauls between -2 and 2.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio,advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{.std.resid\_3 =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}=}\DecValTok{3}\NormalTok{,T,F), }\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  print }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{.fitted, }\AttributeTok{y =}\NormalTok{.std.resid, }\AttributeTok{color =}\NormalTok{ .std.resid\_3) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"red"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{ifelse}\NormalTok{(.std.resid\_3 }\SpecialCharTok{==}\NormalTok{ T,id,}\StringTok{""}\NormalTok{)), }\AttributeTok{nudge\_x =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{show.legend =}\NormalTok{ F)}\SpecialCharTok{+} \FunctionTok{theme\_light}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 200 x 11
   sales    TV radio .fitted  .resid    .hat .sigma   .cooksd .std.resid
   <dbl> <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>
 1  22.1 230.   37.8   20.6   1.54   0.0140    1.68 0.00406       0.925 
 2  10.4  44.5  39.3   12.3  -1.95   0.0188    1.68 0.00871      -1.17  
 3   9.3  17.2  45.9   12.3  -3.04   0.0295    1.67 0.0341       -1.83  
 4  18.5 152.   41.3   17.6   0.883  0.0124    1.68 0.00117       0.528 
 5  12.9 181.   10.8   13.2  -0.324  0.00951   1.69 0.000120     -0.194 
 6   7.2   8.7  48.9   12.5  -5.31   0.0347    1.64 0.124        -3.22  
 7  11.8  57.5  32.8   11.7   0.0818 0.0129    1.69 0.0000105     0.0490
 8  13.2 120.   19.6   12.1   1.09   0.00576   1.68 0.000823      0.653 
 9   4.8   8.6   2.1    3.71  1.09   0.0271    1.68 0.00401       0.658 
10  10.6 200.    2.6   12.6  -1.95   0.0171    1.68 0.00797      -1.17  
# i 190 more rows
# i 2 more variables: .std.resid_3 <lgl>, id <int>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-99-1.pdf}

}

\end{figure}

or

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio,advertising), }\AttributeTok{which =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-100-1.pdf}

}

\end{figure}

We have spotted that 6th and 131 are possible outliers.

Here is all the data regression result:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = sales ~ TV + radio, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  car}\SpecialCharTok{::}\FunctionTok{outlierTest}\NormalTok{()}\OtherTok{{-}\textgreater{}}\NormalTok{ outs}
\NormalTok{outs }\CommentTok{\# 131th value is outlier}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     rstudent unadjusted p-value Bonferroni p
131 -5.714235         4.0499e-08   8.0998e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  car}\SpecialCharTok{::}\FunctionTok{outlierTest}\NormalTok{(., }\AttributeTok{cutoff =} \FloatTok{0.30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     rstudent unadjusted p-value Bonferroni p
131 -5.714235         4.0499e-08   8.0998e-06
6   -3.295069         1.1678e-03   2.3356e-01
\end{verbatim}

Here is regression result after removing outliers

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{131}\NormalTok{),]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = sales ~ TV + radio, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.3938 -0.8195  0.2003  1.0785  2.8134 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 3.051908   0.265279   11.51   <2e-16 ***
TV          0.044220   0.001269   34.84   <2e-16 ***
radio       0.195295   0.007315   26.70   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.511 on 195 degrees of freedom
Multiple R-squared:  0.9146,    Adjusted R-squared:  0.9138 
F-statistic:  1045 on 2 and 195 DF,  p-value: < 2.2e-16
\end{verbatim}

\(R^2\) has increased, residual standard errors has decreased, standard
errors of estimates decreased and coefficients has changed.

\textbf{5.High Leverage Points}

Outliers are observations for which the response \(y_i\) is unusual
given the predictor \(x_i\). In contrast, observations with \emph{high
leverage} have an unusual value for \(x_i\).

\includegraphics{fig3.13.png}

For example observation 41 in the left-hand panel of Figure 3.13 has
high leverage =\textgreater{} predictor value for this observation is
large relative to other observations. The red solid line is the least
squares fit to the data, the blue dashed line is the fit when obs 41 is
removed.

Removing the high leverage observation has a much more substantial
impact on the least squares line than removing an outlier. High leverage
obss have a big impact on the estiamted regression line. It is important
to identify them.

For SLR =\textgreater{} look at the range of \(x_i\) and spot the out of
range observations.

MLR =\textgreater{} it is possible to have an observation that is well
within the range of each individual predictor's values, but that is
unusual in terms of the full set of predictors.

Have a look at the center panel in Figure 3.13. Most of the
observations' predictor values fall within the blue dashed ellipse, but
the red observation is well outside of this range. But neither its value
for \(x_1\) nor \(x_2\) is unusual.

We can quantify an observation's leverage using the \emph{leverage
statistic}. A large value indicates a high leverage.

For SLR:

\[
\begin{align}
h_i &= \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i'=1}^n(x_{i'} - \bar{x})^2} \\
&1/n \leq h_i \leq 1 \\
&\bar{h_i} = (p+1)/n
\end{align}
\] as \(x_i\) increases its distance from \(\bar{x}\) \(h_i\) increases.

\(h_i\) is always between \(1/n\) and \(1\), and the average leverage
for all the observations is always equal to \((p+1)/n\). If an
observation has a \(h\) that greatly exceeds \((p+1)/n\) then we may
suspect of high leverage.

Right hand panel of Figure 3.13 shows studentized residuals versus
\(h_i\). Obs 41 stands out as having a very high leverage statistic as
well as a high studentized residual =\textgreater{} It is an outlier as
well as a high leverage observaiton. This is very dangerous. This plot
also shows the reason that obs 20 had relatively little effect on the
least sqaures fit in Figure 3.12: it has low leverage.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 200 x 9
   sales    TV radio .fitted  .resid    .hat .sigma   .cooksd .std.resid
   <dbl> <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>
 1  22.1 230.   37.8   20.6   1.54   0.0140    1.68 0.00406       0.925 
 2  10.4  44.5  39.3   12.3  -1.95   0.0188    1.68 0.00871      -1.17  
 3   9.3  17.2  45.9   12.3  -3.04   0.0295    1.67 0.0341       -1.83  
 4  18.5 152.   41.3   17.6   0.883  0.0124    1.68 0.00117       0.528 
 5  12.9 181.   10.8   13.2  -0.324  0.00951   1.69 0.000120     -0.194 
 6   7.2   8.7  48.9   12.5  -5.31   0.0347    1.64 0.124        -3.22  
 7  11.8  57.5  32.8   11.7   0.0818 0.0129    1.69 0.0000105     0.0490
 8  13.2 120.   19.6   12.1   1.09   0.00576   1.68 0.000823      0.653 
 9   4.8   8.6   2.1    3.71  1.09   0.0271    1.68 0.00401       0.658 
10  10.6 200.    2.6   12.6  -1.95   0.0171    1.68 0.00797      -1.17  
# i 190 more rows
\end{verbatim}

leverages are shown in \texttt{.hat} here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV,advertising), }\AttributeTok{which =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-106-1.pdf}

}

\end{figure}

\textbf{6.Collinearity}

\emph{Collinearity} refers to the situation in which two or more
predictor variables are closely related to one another.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}
\NormalTok{  Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Limit, }\AttributeTok{y =}\NormalTok{ Age) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =}\StringTok{"red"}\NormalTok{, }\AttributeTok{alpha =}\FloatTok{0.4}\NormalTok{),}
\NormalTok{  Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Limit, }\AttributeTok{y=}\NormalTok{ Rating) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =}\StringTok{"red"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{),}
  \AttributeTok{ncol=}\DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-107-1.pdf}

}

\caption{Figure 3.14}

\end{figure}

In the left panel of Figure 3.14 we see no relationship, but on the
right hand panel predicors are highly correlated: they are
\emph{collinear} =\textgreater{} it is difficult to seperate out the
individual effects of colliniar varaibles on the response.

Collinearity reduces the accuracy of the estiamtes of the regression
coefficients, it causes the standard error for \(\hat{\beta_j}\) to
grow. t-statistic for each predictor is calculated by dividing
\(\hat{\beta_j}\) by its standard error =\textgreater{} collinearity
declines the t-statistics =\textgreater{} we may fail to reject
\(H_0:\beta_j = 0\).

Lets do two regressions:

\begin{itemize}
\tightlist
\item
  model1: balance regressed onto age and limit which has no collinearity
\item
  model2: balance regressed onto rating and limit which are collinear.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ Limit, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bind\_rows}\NormalTok{(}
    \FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rating }\SpecialCharTok{+}\NormalTok{ Limit, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =}\DecValTok{2}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  term         estimate std.error statistic   p.value model
  <chr>           <dbl>     <dbl>     <dbl>     <dbl> <dbl>
1 (Intercept) -173.      43.8        -3.96  9.01e-  5     1
2 Age           -2.29     0.672      -3.41  7.23e-  4     1
3 Limit          0.173    0.00503    34.5   1.63e-121     1
4 (Intercept) -378.      45.3        -8.34  1.21e- 15     2
5 Rating         2.20     0.952       2.31  2.13e-  2     2
6 Limit          0.0245   0.0638      0.384 7.01e-  1     2
\end{verbatim}

In the first model both \texttt{age} and \texttt{limit} are
statistically significant. In the second, collinearity between
\texttt{limit} and \texttt{raiting} has caused standard error for the
limit coefficient estimate to increase by a factor of 12 and p-value to
increase to 0.701. The importance of the limit variable has been masked
due to presence of collinearity.

A simple way to detect collinearity is to look at the correlation matrix
of the predictors. Correlation matrix shows the relatinship of two
variables but, it is possible for colÅinearty to exist between three or
more varaibles even if no pair of variables has a particularly high
correlation. This situation is called \emph{multicollinaerity}.

Instead of inspecting the correlatino matrix, better way to assess
multicollinaerity is to compute the \emph{variance inflation factor}
(VIF). The VIF is the raio of the variance of \(\hat{\beta_j}\) when
fitting the model divided by the variance of \(\hat{\beta_j}\) if fit on
its own.

\[
1 \leq VIF
\] Smallest possible value of VIF is 1: complete absence of
collinearity. Usuall there is always some degree of collinearity among
the predictors. A VIF that exceeds 5 or 10 indicates a problematic
amount of collinaerity.

The VIF for each vairable can be computed using the formula

\[
VIF(\hat{\beta_j}) = \frac{1}{1-R^2_{x_j | x_{-j}}}
\] \(R^2_{x_j | x_{-j}}\) is the \(R^2\) from a regression of \(x_j\)
onto all of the other predictors. If \(R^2_{x_j | x_{-j}}\) is close to
one, then collinearity is present, so VIF will be large.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(Balance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ Rating }\SpecialCharTok{+}\NormalTok{ Limit, Credit) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Age     Rating      Limit 
  1.011385 160.668301 160.592880 
\end{verbatim}

A regression of balance on age, rating, and limit indicates that the
predictors have VIF Values of 1.01, 160.67, 160.59. There is
considerable collinearity in the data.

When faced with collinearity, two simple solutions exist: * drop one of
the problematic variables from the regression * combine collinear
variables together into a single predictor. We might take the average of
standardized versions of limit and rating in order to create a new
varaible that measures \emph{credit worthiness}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper,advertising) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

To get a anova table;

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper,advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: sales
           Df Sum Sq Mean Sq   F value Pr(>F)    
TV          1 3314.6  3314.6 1166.7308 <2e-16 ***
radio       1 1545.6  1545.6  544.0501 <2e-16 ***
newspaper   1    0.1     0.1    0.0312 0.8599    
Residuals 196  556.8     2.8                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper,advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 (Intercept)           TV        radio    newspaper 
 2.938889369  0.045764645  0.188530017 -0.001037493 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper,advertising) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{confint}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                  2.5 %     97.5 %
(Intercept)  2.32376228 3.55401646
TV           0.04301371 0.04851558
radio        0.17154745 0.20551259
newspaper   -0.01261595 0.01054097
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vcov}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper,advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              (Intercept)            TV         radio     newspaper
(Intercept)  0.0972867479 -2.657273e-04 -1.115489e-03 -5.910212e-04
TV          -0.0002657273  1.945737e-06 -4.470395e-07 -3.265950e-07
radio       -0.0011154895 -4.470395e-07  7.415335e-05 -1.780062e-05
newspaper   -0.0005910212 -3.265950e-07 -1.780062e-05  3.446875e-05
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{selecting-best-regression-varaibles}{%
\chapter{Selecting best regression
varaibles}\label{selecting-best-regression-varaibles}}

You are creating a new regression mdoel or improving an exiting model.
You have many regression varaibles and you want to select the vest
subset of those varaibles.

The \texttt{step} function can perform stepwise regression, either
forward or backward. Backward stepwise regression starts with many
varaibles and removes the underperformeners:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper, advertising) }\CommentTok{\# full model}
\NormalTok{stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(lm, }\AttributeTok{direction =} \StringTok{"backward"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }\CommentTok{\# best model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=212.79
sales ~ TV + radio + newspaper

            Df Sum of Sq    RSS    AIC
- newspaper  1      0.09  556.9 210.82
<none>                    556.8 212.79
- radio      1   1361.74 1918.6 458.20
- TV         1   3058.01 3614.8 584.90

Step:  AIC=210.82
sales ~ TV + radio

        Df Sum of Sq    RSS    AIC
<none>                556.9 210.82
- radio  1    1545.6 2102.5 474.52
- TV     1    3061.6 3618.5 583.10
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

backward stepwise regression is the easiest approach.

Backward stepwise is easy but sometimes its not feasible to start with
everything because you have too many candidate variables. In that case
use forward stepwise regression; which will start with nothing and
incrementatlly add variables that improve regression. It stops when no
further improvement is possible

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,advertising)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ 1, data = advertising)

Coefficients:
(Intercept)  
      14.02  
\end{verbatim}

we start with minimum model. We must tell step which candidate varaibels
are available for inclusing in the model. We use \texttt{scope} argument
for that.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{,advertising) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(}\AttributeTok{direction =} \StringTok{"forward"}\NormalTok{, }\AttributeTok{scope =}\NormalTok{ (}\SpecialCharTok{\textasciitilde{}}\NormalTok{ TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ newspaper), }\AttributeTok{trace =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

we have the same result with backward, in reality these results may
differ.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(lm, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=212.79
sales ~ TV + radio + newspaper

            Df Sum of Sq    RSS    AIC
- newspaper  1      0.09  556.9 210.82
<none>                    556.8 212.79
- radio      1   1361.74 1918.6 458.20
- TV         1   3058.01 3614.8 584.90

Step:  AIC=210.82
sales ~ TV + radio

            Df Sum of Sq    RSS    AIC
<none>                    556.9 210.82
+ newspaper  1      0.09  556.8 212.79
- radio      1   1545.62 2102.5 474.52
- TV         1   3061.57 3618.5 583.10
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7977 -0.8752  0.2422  1.1708  2.8328 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.92110    0.29449   9.919   <2e-16 ***
TV           0.04575    0.00139  32.909   <2e-16 ***
radio        0.18799    0.00804  23.382   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.681 on 197 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 
F-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{boxcox}\NormalTok{(lm)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-119-1.pdf}

}

\end{figure}

Detecting the outliers :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{outlierTest}\NormalTok{(lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     rstudent unadjusted p-value Bonferroni p
131 -5.757983          3.267e-08    6.534e-06
\end{verbatim}

\includegraphics{rcook1.png}

\includegraphics{rcook2.png}

\hypertarget{identifying-influential-observations}{%
\subsection{Identifying Influential
observations}\label{identifying-influential-observations}}

You want to identify the observatipons that are having the ost influence
on the regression model, this is useful for diagnosing possible porblems
with the data.

\textbf{Solution}

The \texttt{influence.measures} function reports several useful
statistics for identifying inflÄ±uentioal obs. and it flags the
significant ones with an asterisk(*).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ newspaper,advertising)}
\NormalTok{lm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\CommentTok{\# r2 is 0.6422}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + newspaper, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.6231 -1.7346 -0.0948  1.8926  8.4512 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 5.774948   0.525338  10.993  < 2e-16 ***
TV          0.046901   0.002581  18.173  < 2e-16 ***
newspaper   0.044219   0.010174   4.346 2.22e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.121 on 197 degrees of freedom
Multiple R-squared:  0.6458,    Adjusted R-squared:  0.6422 
F-statistic: 179.6 on 2 and 197 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inf }\OtherTok{=} \FunctionTok{influence.measures}\NormalTok{(lm)}
\end{Highlighting}
\end{Shaded}

We need to filter for columns which has at least one true:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(inf}\SpecialCharTok{$}\NormalTok{is.inf) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{obs =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{if\_any}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\SpecialCharTok{\textasciitilde{}}\NormalTok{. }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(}\StringTok{"obs"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ infss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13 x 8
   dfb.1_ dfb.TV dfb.nwsp dffit cov.r cook.d hat     obs
   <lgl>  <lgl>  <lgl>    <lgl> <lgl> <lgl>  <lgl> <int>
 1 FALSE  FALSE  FALSE    FALSE FALSE FALSE  FALSE     1
 2 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE     3
 3 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE     6
 4 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE    13
 5 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  TRUE     17
 6 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE    26
 7 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  TRUE     76
 8 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  TRUE    102
 9 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE   119
10 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE   129
11 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE   132
12 FALSE  FALSE  FALSE    TRUE  TRUE  FALSE  FALSE   166
13 FALSE  FALSE  FALSE    FALSE TRUE  FALSE  FALSE   179
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{infss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]   1   3   6  13  17  26  76 102 119 129 132 166 179
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ newspaper,advertising[}\SpecialCharTok{{-}}\NormalTok{infss,]) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }\CommentTok{\# r\^{}2 improves!}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + newspaper, data = advertising[-infss, 
    ])

Residuals:
    Min      1Q  Median      3Q     Max 
-7.2432 -1.6062 -0.0775  1.9088  6.9490 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 5.348561   0.512438   10.44  < 2e-16 ***
TV          0.047992   0.002574   18.65  < 2e-16 ***
newspaper   0.058656   0.011501    5.10  8.4e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.918 on 184 degrees of freedom
Multiple R-squared:  0.6866,    Adjusted R-squared:  0.6832 
F-statistic: 201.6 on 2 and 184 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm12 }\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ newspaper,advertising[}\SpecialCharTok{{-}}\NormalTok{infss,])}
\end{Highlighting}
\end{Shaded}

\hypertarget{testing-residuals-for-autocorrelation}{%
\section{testing residuals for
autocorrelation}\label{testing-residuals-for-autocorrelation}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmtest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Zorunlu paket yÃ¼kleniyor: zoo
\end{verbatim}

\begin{verbatim}

Attaching package: 'zoo'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    as.Date, as.Date.numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dwtest}\NormalTok{(lm) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Durbin-Watson test

data:  lm
DW = 1.9685, p-value = 0.4097
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

if \$ p \textgreater{} 0.05\$ no evidence of correaltion.

\hypertarget{the-marketing-plan}{%
\section{The Marketing Plan}\label{the-marketing-plan}}

Lets go back to our 7 questions about the \texttt{advertising}data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Is there a relationship between advertising sales and budget?}
\end{enumerate}

\begin{verbatim}
To get an answer to this question, we regress `sales` onto our budget related varaibles `TV`, `radio`, and `newspaper` and test the hypothesis $H_0: \beta_{TV}=\beta_{radio}=\beta_{newspaper} = 0$ with F-statistic.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{advertising }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\StringTok{"sales\_hat"}\NormalTok{)}
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{., advertising) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = sales ~ ., data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8277 -0.8908  0.2418  1.1893  2.8292 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.938889   0.311908   9.422   <2e-16 ***
TV           0.045765   0.001395  32.809   <2e-16 ***
radio        0.188530   0.008611  21.893   <2e-16 ***
newspaper   -0.001037   0.005871  -0.177     0.86    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.686 on 196 degrees of freedom
Multiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 
F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

In this case the p-value corresponding to the F-statistic is very low,
indicating a clear evidence of a relationship between advertising and
sales.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \emph{How strong is the relationship?}

  This is about model accuracy. We can check \(R^2\); the predictors
  explain almost 90\% of the variance in \texttt{sales}.
\item
  \emph{Which media contribute to sales?}

  We check p values of the coefficients. newspaper is not statistically
  significant; only \texttt{TV} and \texttt{radio} are realated to
  sales.
\item
  \emph{How large is the effect of each medium on sales?}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,advertising) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dwplot}\NormalTok{(}\AttributeTok{ci =} \FloatTok{0.95}\NormalTok{,}\AttributeTok{dot\_args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size=}\DecValTok{2}\NormalTok{), }\AttributeTok{vline =} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{, }\AttributeTok{linetype =}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-126-1.pdf}

}

\end{figure}

\begin{verbatim}
We use standard errors to construct confidence intervals for the coefficients. If any of the confidence intervals include 0 that predictor is not statistically significant. Collinearty can result in very wide standard errors; making confint to include zero. Was the collineary the reason that confint of newspaper to be so wide? Lets check VIF scores
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{(}\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,advertising))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       TV     radio newspaper 
 1.004611  1.144952  1.145187 
\end{verbatim}

No all the VIF scores suggest no evidence of collinearity.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \emph{How accurately can we predict future sales?}

  After deciding on our model
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV}\SpecialCharTok{+}\NormalTok{radio }\SpecialCharTok{+}\NormalTok{ TV }\SpecialCharTok{*}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + TV * radio, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.3366 -0.4028  0.1831  0.5948  1.5246 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.750e+00  2.479e-01  27.233   <2e-16 ***
TV          1.910e-02  1.504e-03  12.699   <2e-16 ***
radio       2.886e-02  8.905e-03   3.241   0.0014 ** 
TV:radio    1.086e-03  5.242e-05  20.727   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9435 on 196 degrees of freedom
Multiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 
F-statistic:  1963 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{verbatim}
The accuracy of our estimation depends on whether we wish to predict an individual response, $Y = f(X) + \epsilon$, or the average response, $f(X)$. If the former we use a prediction interval, and if the latter, we use a confidecen interval. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty assocaited with $\epsilon$, the irreducible error.
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \emph{Is the relationship linear?}

  We can use residual plots to identify non-linearity: the plot will
  show a pattern.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio, advertising) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .hat, }\AttributeTok{y =}\NormalTok{ .fitted) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter3_files/figure-pdf/unnamed-chunk-129-1.pdf}

}

\end{figure}

the model seems okay. If we had encountered a non-linearity we would
have to make transformations to the predictors.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  \emph{Is there synergy among the advertising media?}

  LR assumes additive relationship. When we account for interacation
  term our model is improved and the interaction term was statistically
  significant.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{TV }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{+}\NormalTok{ radio }\SpecialCharTok{*}\NormalTok{ TV, advertising) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = sales ~ TV + radio + radio * TV, data = advertising)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.3366 -0.4028  0.1831  0.5948  1.5246 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 6.750e+00  2.479e-01  27.233   <2e-16 ***
TV          1.910e-02  1.504e-03  12.699   <2e-16 ***
radio       2.886e-02  8.905e-03   3.241   0.0014 ** 
TV:radio    1.086e-03  5.242e-05  20.727   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9435 on 196 degrees of freedom
Multiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 
F-statistic:  1963 on 3 and 196 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{comparison-of-linear-regression-with-k-nearest-neighbors}{%
\section{Comparison of Linear Regression with K-Nearest
Neighbors}\label{comparison-of-linear-regression-with-k-nearest-neighbors}}

Linear regression is a parametric approach because it assumes a linear
functinoal form for \(f(x)\). Parametric approaches are easy to fit,
coefficients have simple interpretations and test of statistical
significance can be easily performed. But they make strong assumptions
about the form of \(f(X)\). If the assumed functional for is not
correct, and the prediciton is our goal, then we see a poor performance
of fit.

In contrast, \emph{non-parametric} methods do not explicity assume a
parametric for of \(f(X)\), so they provide a more flexible approach for
performing regression. One of a non-parametric method is \emph{K-nearest
neighbors regressin} (KNN regression) .

The KNN regression method is closely related to the KNN classifier in
Chapter 2. Given a value for \emph{K} and a prediciton point \(x_0\),
KNN regression first identifies the \emph{K} traiinng observations that
are closest to \(x_0\) represented by \(N_0\). It then estimates
\(f(x_0)\) using the average of all the training responses in \(N_0\).
In other words:

\[
\hat{f}(x_0) = \frac{1}{K}\sum_{x \in N_0} y_i
\]

The optimal \(K\) value will depend on the \emph{bias-variance
tradeoff}. A small value for \(K\) provides the most flexile fit, which
will have low bias but high variance. This variance is due to the fact
that the prediciton in a given region is entirely dependent on just one
obsrvation. In contrast, larger values of \(K\) will provide a smoother
and less variable fit; the prediction in a region is an average of
several points, and so changing one observation has a smaller effect.
However, the smoothing may cause bias by masking some of the structure
in \(f(x)\). Chapter 5 will introduce several approaches for estiamting
test error rates which can be used to identify the optimal \(K\) in KNN
regression.

\emph{The parametric approach will outperform the non-parametric
approach if the parametric form that has been selected is close to the
true for of} \(f\).

\includegraphics{fig3.17.png}

in Figure one true relationship is linear, represented by the black
line. Blue curves response to the KNN fits using \(K=1\) and \(K=9\).
\(K=9\) is much closer to \(f(x)\). However, since the true relationship
is linear, KNN approach cannot compete with a linear regression; a
non-parametric approach incurs a cost in variance that is not offset by
reduction in bias.

\includegraphics{fig3.18.png}

The blue dashed line is the OLS regression which outperfroms KNN for
this data. Green sloid line, plotted as a function of \(1/K\) represents
the test set mean squared error(MSE) for KNN. The KNN errors are wlell
above the black dashed line, which is the test MSE for linear
regression. When K is large KNN performs only a little worse that least
squares regression in terms of MSE. It performs far worse when K is
small.

In practice true relationship between \(X\) and \(Y\) is rarely exactly
linear.

\includegraphics{fig3.19.png}

Fig 3.10 examines the relative performances of OLS regression and KNN
under increasing levels of non-linearity in the relationship between
\(X\) and \(Y\). In the top row, the true relationship is nearly linear.
In this case we see that test MSE for linear regression is still
superior to that of KNN for low values of K. However, fro \(4 \leq K\)
KNN outperforms linear regression. The secodn row has more
non-linearity. In this case KNN is better for all valeus of K.

So, KNN performs slightly worse than linear regression when the
relationship is linear, but much better for non-linear situations. Does
this mean we should favour KNN In relaity because the relationships are
rarely linear? No, in relaity, even when the true relationship is highly
non linear, KNN may still provide inferior resutls to linear regression.
Fig 3-18 and 19 had \(p=1\), but for larger \(p\) KNN often performs
worse than linear regression.

\includegraphics{fig3.20.png}

Fig 20 shows a strong non-linear situation. When \(p=1\)or \(p=2\) KNN
is better, but for \(p=3\) results are mixed, for \(4\leqp\) LR is
superior. \emph{This decrease in performance as the dimensions increases
is a common problem for KNN}

As a general rule parametric methods will tend to outperform
non-parametric approaches when there is small number of observations per
predictor.

\bookmarksetup{startatroot}

\hypertarget{classification}{%
\chapter{Classification}\label{classification}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
\FunctionTok{library}\NormalTok{(ISLR)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(dotwhisker)}
\FunctionTok{library}\NormalTok{(hrbrthemes)}
\FunctionTok{library}\NormalTok{(patchwork)}
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{library}\NormalTok{(showtext)}
\NormalTok{\})}
\NormalTok{extrafont}\SpecialCharTok{::}\FunctionTok{loadfonts}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_ipsum\_es}\NormalTok{(}\AttributeTok{axis\_title\_size =} \DecValTok{11}\NormalTok{ , }\AttributeTok{axis\_title\_just =} \StringTok{"c"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.line =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{color =}\StringTok{"black"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

For many cases the response variable is \emph{qualitative}, or
\emph{categorical}.

Clasification is the process for predicting qualitative responses; we
are classifying an observation.

There are three main classifiers,\emph{classification techniques},
mainly:

\begin{itemize}
\tightlist
\item
  \emph{Logistic regression}
\item
  \emph{Linear discriminant analysis}
\item
  \emph{K-nearest neighbors}
\end{itemize}

We discuss more computer intensive methods is later chapters such as
GAM(ch 7), trees, random forests, and boosting(ch 8), and support vector
machines (ch 9).

\hypertarget{an-overview-of-classsification}{%
\section{An overview of
Classsification}\label{an-overview-of-classsification}}

Here some clasffication problems

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A person arrives at the emergency room with a set of symptoms that
  could possible be attributed to one f three medical conditions. Which
  of the three conditions does the individual have?
\item
  An online banking service must be able to determine whether or not a
  transaction being performed on the site is fraudulent, on the basis of
  the user's IP adress, past transaction history and so forth.
\item
  In the bais of DNA sequence data for a number of patients with and
  without a given disease, a biologist would like to figure out which
  DNA mutations are disease causing and which are not.
\end{enumerate}

Just like in LR , in the classification setting we have a set of
training observations \((x_1,y_1), \dots, (x_n,y_n)\) what we can use to
build a classfier. We want our classifier to perform not only on th
training data, but also on test observations that are not used to train
the classier.

\begin{quote}
We are going to use \texttt{Default} data set.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"./data/Default.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 10000 Columns: 4
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (2): default, student
dbl (2): balance, income

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textless{}\textgreater{}\%} 
  \FunctionTok{mutate\_if}\NormalTok{(is.character, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.factor}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10,000 x 4
   default student balance income
   <fct>   <fct>     <dbl>  <dbl>
 1 No      No         730. 44362.
 2 No      Yes        817. 12106.
 3 No      No        1074. 31767.
 4 No      No         529. 35704.
 5 No      No         786. 38463.
 6 No      Yes        920.  7492.
 7 No      No         826. 24905.
 8 No      Yes        809. 17600.
 9 No      No        1161. 37469.
10 No      No           0  29275.
# i 9,990 more rows
\end{verbatim}

We are interested in predicting whether an individual will default on
his or her credit card balance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skimr}\SpecialCharTok{::}\FunctionTok{skim}\NormalTok{(default)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\caption{Data summary}\tabularnewline
\toprule\noalign{}
\endfirsthead
\endhead
\bottomrule\noalign{}
\endlastfoot
Name & default \\
Number of rows & 10000 \\
Number of columns & 4 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Column type frequency: & \\
factor & 2 \\
numeric & 2 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Group variables & None \\
\end{longtable}

\textbf{Variable type: factor}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1867}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1333}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1867}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ordered
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
top\_counts
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
default & 0 & 1 & FALSE & 2 & No: 9667, Yes: 333 \\
student & 0 & 1 & FALSE & 2 & No: 7056, Yes: 2944 \\
\end{longtable}

\textbf{Variable type: numeric}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1333}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0952}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1333}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p25
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p50
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p75
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
hist
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
balance & 0 & 1 & 835.37 & 483.71 & 0.00 & 481.73 & 823.64 & 1166.31 &
2654.32 & âââââ \\
income & 0 & 1 & 33516.98 & 13336.64 & 771.97 & 21340.46 & 34552.64 &
43807.73 & 73554.23 & âââââ \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GGally}\SpecialCharTok{::}\FunctionTok{ggpairs}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ default), }\AttributeTok{data =}\NormalTok{ default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ balance, }\AttributeTok{y =}\NormalTok{ income, }\AttributeTok{color =}\NormalTok{ default, }\AttributeTok{shape =}\NormalTok{ default) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#6CA2C9"}\NormalTok{,}\StringTok{"\#BD5E2A"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)) }\OtherTok{{-}\textgreater{}}\NormalTok{ p1}

\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ default, }\AttributeTok{y =}\NormalTok{ balance, }\AttributeTok{fill =}\NormalTok{ default) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#6CA2C9"}\NormalTok{,}\StringTok{"\#BD5E2A"}\NormalTok{)) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2}

\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ default, }\AttributeTok{y =}\NormalTok{ income, }\AttributeTok{fill =}\NormalTok{ default) }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#6CA2C9"}\NormalTok{,}\StringTok{"\#BD5E2A"}\NormalTok{)) }\OtherTok{{-}\textgreater{}}\NormalTok{ p3}

\CommentTok{\# gridExtra::grid.arrange(p1,p2,p3, nrow=1)}
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(p1,p2,p3, }\AttributeTok{nrow=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Top: The aanual incomes and montly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: boxplots of balances as a function of default status. Bottom: boxplots of income as a functino of default status.}
\end{Highlighting}
\end{Shaded}

people who default tend to have high credit card balances compared to
not defaulted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{port =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  default     n   port
  <fct>   <int>  <dbl>
1 No       9667 0.967 
2 Yes       333 0.0333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# default rate is 3\%}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(student,default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{port =}\NormalTok{ count}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(count))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'student'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 4 x 4
# Groups:   student [2]
  student default count   port
  <fct>   <fct>   <int>  <dbl>
1 No      No       6850 0.971 
2 No      Yes       206 0.0292
3 Yes     No       2817 0.957 
4 Yes     Yes       127 0.0431
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# student are 2 times more likely to default}
\end{Highlighting}
\end{Shaded}

In this chapter we laern how to build a model to predict
\texttt{default}(\(y\)), for any given value of balance (\(x_1\)), and
income (\(x_2\)). Since \(Y\) is not quantitative, SLR is not
appropriate.

\hypertarget{why-not-linear-regression}{%
\section{Why Not Linear Regression?}\label{why-not-linear-regression}}

Why is LR not appropriate here?

Suppose that we are trying to predict the mdeical condition of a patient
in the emergency room on the basis of her symptoms. In this simplified
example, therea rea three possible diagnoses: \texttt{stroke},
\texttt{drug\ overdose}, \texttt{epileptic\ seizure}. We could consider
encoding these values as a quantitative respose variable \(Y\):

\[
Y = 
\begin{cases}
1 & \text{if stroke}; \\
2 & \text{if drug overdose}; \\
3 & \text{if epileptic seizure}.
\end{cases}
\]

We can now predict \(Y\) using \(x_1, \dots, \x_p\). However, this
coding implies an ordering on the outcomes, putting
\texttt{drug\ overdose} in between \texttt{stroke} and
\texttt{epileptic\ seizure} and insisting that the difference between
\texttt{stroke} and \texttt{drug\ overdose} is the same as the
difference between \texttt{drug\ overdose} and
\texttt{epileptic\ seizure}. In practice there is no particular reason
that this needs to be the case. We could have ordered the cases
differently, i.e.~stroke to the 3 etc. Which implies a totally different
relationship among the three conditions. All these combinations would
produce different linear models that would lead to different set of
predictions on test observations.

However, if the response variable's values take on a natural ordering,
such as \emph{mild, moderate, and severe}, and we felt the gap between
mild and moderate was similar to the gap between moderate and sever,
then a 1,2,3 coding would be reasonable. **Unfortunaltely, in general
there is no natural way to convert a qualitative response variable with
more than two levels into a quantittative response that is ready for
linear regression*.

For a \emph{binary(two level)} qualitative response, the situation is
easier. For instance consider only two possiblities for \(Y\):

\[
 Y = 
 \begin{cases}
 0 & \text{if stroke} \\
 1 & \text{if drug overdose}
 \end{cases}
 \]

we can create a dummy varaible and fit a linar regression to this binary
response and predict \texttt{drug\ overdose} if \(\hat{y}>0.5\) and
\texttt{stroke} otherwise. However, if we use a linear regression our
estimates might be outside of \([0,1]\), aking them hard to interpret as
probabilities.

The dummy varaible approach coonot be easliy extended to accommodate
qualitative responses with more than two levels. We prefer
classification methods:

\#\# Logistic Regression

Our \texttt{default} variable can have two values: \texttt{Yes} or
\texttt{No}. Rather than modeling this response \(Y\) directly, logistic
regression models the \emph{probability} that \(Y\) belongs to a
particular category.

For the \texttt{default} data logistic regression models the probability
of default. For example, the probability of default given balance can be
written as

\[
Pr(default = Yes | balance)
\] The values of \(Pr(default = Yes|balance)\), which we abbreviate
\(p(balance)\), will range between 0 and 1. Then for any given value of
\texttt{balance}, a prediction can be made for \texttt{default}. For
example, we can predict \(default = Yes\) or any individaul for whom
\(p(balance)>0.5\). Alternatively, if a company wishes to be
conservative in predicting individuals who are at risk for default, then
they may choose to use a lower threshold, such as \(p(balance)>0.1\).

\hypertarget{the-logistic-model}{%
\subsection{The Logistic Model}\label{the-logistic-model}}

How should we model the relationship between \(p(x) = Pr(y = 1| x)\) and
\(x\)? (when we coded 0/1 for response values)

In section 4.2 we talked of using a linear regression model to represent
these probabilities:

\[
p(x) = \beta_0 + \beta_1 x
\] (4.1)

\includegraphics{fig4.2.png}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{default =} \FunctionTok{ifelse}\NormalTok{(default }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(default}\SpecialCharTok{\textasciitilde{}}\NormalTok{balance,.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10,000 x 4
   default student balance income
     <dbl> <fct>     <dbl>  <dbl>
 1       0 No         730. 44362.
 2       0 Yes        817. 12106.
 3       0 No        1074. 31767.
 4       0 No         529. 35704.
 5       0 No         786. 38463.
 6       0 Yes        920.  7492.
 7       0 No         826. 24905.
 8       0 Yes        809. 17600.
 9       0 No        1161. 37469.
10       0 No           0  29275.
# i 9,990 more rows
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = default ~ balance, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.23533 -0.06939 -0.02628  0.02004  0.99046 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -7.519e-02  3.354e-03  -22.42   <2e-16 ***
balance      1.299e-04  3.475e-06   37.37   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1681 on 9998 degrees of freedom
Multiple R-squared:  0.1226,    Adjusted R-squared:  0.1225 
F-statistic:  1397 on 1 and 9998 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{default =} \FunctionTok{ifelse}\NormalTok{(default }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(default}\SpecialCharTok{\textasciitilde{}}\NormalTok{balance,.) }\OtherTok{{-}\textgreater{}}\NormalTok{ lm\_res}

\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{default =} \FunctionTok{ifelse}\NormalTok{(default }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ balance, }\AttributeTok{y =}\NormalTok{ default) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =} \DecValTok{4}\NormalTok{, }\AttributeTok{color =} \StringTok{"brown4"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ lm\_res}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =}\NormalTok{ lm\_res}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.25}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\AttributeTok{by=}\FloatTok{0.2}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d
\end{verbatim}

\begin{verbatim}
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font width unknown for character 0x2d
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

The plot above is the same as plot on the left pane of 4.2. Here we see
the problem with this approach, for balances close to zero, we predict a
negative probability of default and if we predict for very large
balances we would get values bigger than 1. These predictions are not
sensible. This always happen for any time a straight line is fit to a
binary response that is coded as 0 or 1, in pricpile we can always
predict \(p(x)<0\) for some values of \(x\) and \(p(x)>1\) for others.

To avoid this problem we must model \(p(x)\) using a function that gives
outputs between 0 and 1 for all values of \(x\). Many functions meet
this description, however in logistis regression, we use \emph{logistic
function}

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}
\] (4.2)

To fit the mode (4.2) we use a method called \emph{maximum likelihood},
which we discuss in the next section. The right-hand panel of Figure 4.2
sows then fit of the logistic regression model to the
\texttt{default}data.

Notice that for low balances we now predict the probability of default
as close to , but never below zero. Likewise for high balances we
predict a default probability close to, but never above, one.

The logistic function will always produce an \emph{S-shaped} curve of
this form, and so regardless of the value of \(X\) we will obtain a
sensible prediction.

After a bit of manipulation of (4.2) we find that

\$\$ \begin{align}
p(x) &= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1-p(x) &= 1- \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1- p(x) &= \frac{1}{1 + e^{\beta_0 + \beta_1 x}}  \\

1- p(x) \cdot e^{\beta_0 + \beta_1x} &= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}\\

1-p(x) \cdot e^{\beta_0 + \beta_1 x} &= p(x) \\

\frac{p(x)}{1-p(x)} &= e^{\beta_0 + \beta_1} \space \space \space \space \space \space \space \space \space \space (4.3)

\end{align} \$\$

the quantity \(p(x)/(1 - p(x))\) is called the \emph{odds}, and can take
on any value between 0 and \(\infty\). Values of the odds close to 0 and
\(\infty\) indicate very low and very high probabilities of default,
respectively.

For example, on average 1 in 4 people with an odds of 1/4 will default
since \(p(x) = 0.2\) implies an ods of \(\frac{0.2}{1-0.2} = 1/4\).
Likewise on average 9/10 wpeople with an odds of 9 will default since
\(p(x) = 0.9\) implies an odds of \(\frac{0.9}{1-0.9} = 9\).

By taking the lograithm of both sides of (4.3) we arrive at

\[
\log(\frac{p(x)}{1-p(x)}) = \beta_0 + \beta_1 x
\] (4.4)

the left-hand side is called the \emph{log-odds-} or \emph{logit}. We
see that the logistic regression model (4.2) has a logit that is linear
in \(X\).

Here increasing \(x\) by one unit changes the log odds by \(\beta_1\)
(4.4), or it multiplies the odds by \(e^{\beta_1}\). But because the
relationship between \(p(x)\) and \(x\) in (4.2) is not a straight line,
\(\beta_1\) does not correspond to the change in \(p(x)\) associated
with a one-unit increase in \(x\). The amount that \(p(x)\) changes due
to a one-unit change in \(x\) will depend on the current value of \(x\).
But regardless of the value of \(x\), if \(\beta_1\) is positive, then
increasing \(x\) will be associated with increasing \(p(x)\), vice
versa. We can see that in the right hand panel of Figure 4.2.

\hypertarget{estiamting-the-regression-coefficients}{%
\subsection{Estiamting the Regression
Coefficients}\label{estiamting-the-regression-coefficients}}

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1}}
\] (4.2) The coefficients of \(\beta_0\) and \(\beta_1\) are unknown,
must be estimated based on the available training data. We are going to
use \emph{maximum likelyhood} method. The basic intuition begind using
maximum likelihood to fit a logistic regression model is as follows: we
seek estimates for \(\beta_0\) and \(\beta_1\) such that the predicted
probability \(\hat{p}(x_i)\)\} of default for each individual, using
(4.2), corresponds as closely as possible to the individual's observed
default status. In other words, we try to find \(\hat{\beta_0}\) and
\(\hat{\beta_1}\) such that plugging these estimates into the model for
\(p(x)\),given in (4.2), yields a number close to 1 for all individuals
who defaulted, and a number close to zero for all individuals who did
not. We can formalize this mathematical equation with \emph{likelihood
function}

\[
l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i) \prod_{i':y_{i'} = 0}(1-p(x_{i'}))
\] (4.5)

The estimates \(\hat{\beta_0\) and \(\hat{\beta_1}\) are chosen to
\emph{maximize} this likelihood function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# initialize model}
\FunctionTok{logistic\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ log\_model}
\NormalTok{log\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Logistic Regression Model Specification (classification)

Computational engine: glm 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# setup the recipe }
\NormalTok{default\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(default }\SpecialCharTok{\textasciitilde{}}\NormalTok{ balance, }\AttributeTok{data =}\NormalTok{ default)}
\NormalTok{default\_recipe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
\end{verbatim}

\begin{verbatim}
-- Recipe ----------------------------------------------------------------------
\end{verbatim}

\begin{verbatim}
\end{verbatim}

\begin{verbatim}
-- Inputs 
\end{verbatim}

\begin{verbatim}
Number of variables by role
\end{verbatim}

\begin{verbatim}
outcome:   1
predictor: 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up the workflow}
\FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_model}\NormalTok{(log\_model) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_recipe}\NormalTok{(default\_recipe) }\OtherTok{{-}\textgreater{}}\NormalTok{ default\_workflow}
\NormalTok{default\_workflow}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
== Workflow ====================================================================
Preprocessor: Recipe
Model: logistic_reg()

-- Preprocessor ----------------------------------------------------------------
0 Recipe Steps

-- Model -----------------------------------------------------------------------
Logistic Regression Model Specification (classification)

Computational engine: glm 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# fit model}
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\AttributeTok{conf.int =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 7
  term         estimate std.error statistic   p.value  conf.low conf.high
  <chr>           <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   
2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ balance, }\AttributeTok{y =}\NormalTok{ default, }\AttributeTok{color =}\NormalTok{ .pred\_class) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{scale\_color\_ipsum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

Lets have a look at the Logistic Regression results again:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# fit model}
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\AttributeTok{conf.int =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 7
  term         estimate std.error statistic   p.value  conf.low conf.high
  <chr>           <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   
2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.}
\end{Highlighting}
\end{Shaded}

We can measure the accuracy of the coefficient estimates by computing
their standard errors. The \emph{z-}statistic in the above plays the
same role as \emph{t} statistic in linear regression output. They are
calculated from \(\hat{\beta_i} / \text{SE}(\hat{\beta_i})\), and so a
large(absolute) value of the z-statistic indicates evidence against the
null hypothesis \(H_0: \beta_1 = 0\). This null hypothesis implies that
\(p(x) = \frac{e^{\beta_0}}{1 + e^{\beta_0}}\). In other words, that the
probability of default does not depend on blaance. p value is very low,
we can reject \(H_0\); there is relationship between balance and
probability of default. Intercept is not important here.

\hypertarget{making-predictions}{%
\subsection{Making predictions}\label{making-predictions}}

Once the coefficients have been estimated, we can compute the
probability of default for any given credit card balance.

\[
\hat{p}(x) = \frac{e^{\hat{\beta_0} + \hat{\beta_1}x}}{1 +e^{\hat{\beta_0} + \hat{\beta_1}x}} = \frac{e^{-10.6513 + 0.0055 x}}{1 + e^{-10.6513 + 0.0055 x}}
\]

so for example given income \$1,000 the predicted possiblity of default
is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{10.6513} \SpecialCharTok{+} \FloatTok{0.0055} \SpecialCharTok{*} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{10.6513} \SpecialCharTok{+} \FloatTok{0.0055} \SpecialCharTok{*} \DecValTok{1000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.005758518
\end{verbatim}

which is below 1\%. What about the probability of default for a person
with \$2,000 balance

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{10.6513} \SpecialCharTok{+} \FloatTok{0.0055} \SpecialCharTok{*} \DecValTok{2000}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{10.6513} \SpecialCharTok{+} \FloatTok{0.0055} \SpecialCharTok{*} \DecValTok{2000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5863023
\end{verbatim}

much higher 58\%.

We can get all the predictions for our training data set form

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data=}\NormalTok{default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(.pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10,000 x 7
   default student balance income .pred_class .pred_No .pred_Yes
   <fct>   <fct>     <dbl>  <dbl> <fct>          <dbl>     <dbl>
 1 No      No         730. 44362. No             0.999 0.00131  
 2 No      Yes        817. 12106. No             0.998 0.00211  
 3 No      No        1074. 31767. No             0.991 0.00859  
 4 No      No         529. 35704. No             1.00  0.000434 
 5 No      No         786. 38463. No             0.998 0.00178  
 6 No      Yes        920.  7492. No             0.996 0.00370  
 7 No      No         826. 24905. No             0.998 0.00221  
 8 No      Yes        809. 17600. No             0.998 0.00202  
 9 No      No        1161. 37469. No             0.986 0.0138   
10 No      No           0  29275. No             1.00  0.0000237
# i 9,990 more rows
\end{verbatim}

\begin{verbatim}
# A tibble: 2 x 2
  .pred_class     n
  <fct>       <int>
1 No           9858
2 Yes           142
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# we classified 142 cases to default }

\CommentTok{\# originally }
\NormalTok{default }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  default     n
  <fct>   <int>
1 No       9667
2 Yes       333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 333 cases are actually default}
\end{Highlighting}
\end{Shaded}

We can implement qualitative predictors to the logistic regression using
the dummy variable approach.. Lets predict default by only
\texttt{student} variable

\[
x = 
\begin{cases}
1 & \text{if student} \\
0 & \text{if not student}
\end{cases}
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{logistic\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ log\_model}

\NormalTok{default\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(default }\SpecialCharTok{\textasciitilde{}}\NormalTok{ student, }\AttributeTok{data =}\NormalTok{ default)}
\FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_model}\NormalTok{(log\_model) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_recipe}\NormalTok{(default\_recipe) }\OtherTok{{-}\textgreater{}}\NormalTok{ default\_workflow}
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{(}\AttributeTok{conf.int =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 7
  term        estimate std.error statistic  p.value conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>
1 (Intercept)   -3.50     0.0707    -49.6  0          -3.65     -3.37 
2 studentYes     0.405    0.115       3.52 0.000431    0.177     0.629
\end{verbatim}

The coefficient is positive and the p value is statistically
significant. This indicates that students tend to have higher default
probabilities than non student:

\[
\widehat{Pr}(default = Yes | student = Yes) = \frac{e^{-3.50 + 0.4049 \times 1}}{1 + e^{-3.50 + 0.4049 \times 1}} = 0.0431
\]

\[
\widehat{Pr}(default = Yes | student = No) = \frac{e^{-3.50 + 0.4049 \times 0}}{1 + e^{-3.50 + 0.4049 \times 0}} = 0.0292
\]

\hypertarget{multiple-logistic-regression}{%
\subsection{Multiple Logistic
Regression}\label{multiple-logistic-regression}}

We now consider the problem of predicting a binary response using a
multiple predictors. We can generalize (4.4) as follows:

\[
\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1x_1 + \dots + \beta_p x_p
\] (4.6)

(4.6) can be written as

\[
p(x) = \frac{e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}
\] (4.7)

We again use maximum likelihood method to estimate
\(\beta_0, \beta_1, \dots, \beta_p\).

Lets use all of our variables in our model

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{logistic\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ log\_model}

\NormalTok{recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(default }\SpecialCharTok{\textasciitilde{}}\NormalTok{ balance }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ student, }\AttributeTok{data =}\NormalTok{ default)}

\FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_model}\NormalTok{(log\_model) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_recipe}\NormalTok{(recipe) }\OtherTok{{-}\textgreater{}}\NormalTok{ default\_workflow}

\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  term            estimate  std.error statistic   p.value
  <chr>              <dbl>      <dbl>     <dbl>     <dbl>
1 (Intercept) -10.9        0.492        -22.1   4.91e-108
2 balance       0.00574    0.000232      24.7   4.22e-135
3 income        0.00000303 0.00000820     0.370 7.12e-  1
4 studentYes   -0.647      0.236         -2.74  6.19e-  3
\end{verbatim}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
term & estimate & std.error & statistic & p.value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(Intercept) & -10.8690452 & 0.4922555 & -22.080088 & 0.0000000 \\
balance & 0.0057365 & 0.0002319 & 24.737563 & 0.0000000 \\
income & 0.0000030 & 0.0000082 & 0.369815 & 0.7115203 \\
studentYes & -0.6467758 & 0.2362525 & -2.737646 & 0.0061881 \\
\end{longtable}

Here there is a suprising result. According to the p-values balance and
student variables are associted with the probability of default.
However, coefficient for stundet dummy is negative; indicating that
students are less likely to default than non-students. But this
coefficint was positive in our previous analysis when we regressed
probability of default by student: results showed that probability of
default is twice as likely for students compared to non students.

\includegraphics{fig4.3.png}

Check out the fig 4.3. The orange and blue lines show the average
default rates for students and non students, respectively, as a function
of credit card balance. The nagive coefficient for student in the
multiple logistic regression indicates that \textbf{for a fixed value of
balance and income} a student is less likely to default than a
non-student. We observe from the left hand panel that the student
default ratge is at or below that of the nun student default rate for
every value of balance. But the horizontal broken lines near the base of
the plot, which show the default rates for students and non students
averaged over all values of balance and income, suggest the opposite
effect: the overall student default rate is higher than non student
default rate. Consequently there is a positive coefficient for student
in the single variable logistic regression output.

The right hand panel provies an explanation for this discrepancy: the
varaibles student and blaance are correlated. Students tend to hold
higher levels of debt, which is in turn assocaited with higer
probability of default. In other words, students are more likely to have
large credit card balances, which as we know from the left hand panel of
figure 4.3 tend to be assocaited with high default rates. Thus, even
though an individual student with a given credit card balance will tend
to have a lower probability of default than a non student with the same
credit card balance, the fact that students on the whole tend to have
higher credit card balances means that overall students tend to default
at a higher rate than non students. This is an important distinction. A
student is riskier than a non student if no information about the
student's credit card balance is available. However, that student is
less risky than a non student with the same credit card balance.

This simple example illustrates the dangers and subtleties assocaited
with performing regressions involving only a single predictor when other
predictors may als obe relevant. The results obtained using one
predictor may be quite different from those obtained using multiple
predictors, especially when there is correlation among the predictors.
In general, the phenomenon seen in Figure 4.3 is known as
\emph{confounding}.

Lets put the estimates to our estimated probability function

\$\$ \hat{p}(x) = \frac{
e^{-10.86 + 0.00574 \cdot balance + 3e-6 \cdot income - 0.646 Student}
}

\{ 1 + e\^{}\{-10.86 + 0.00574 \cdot balance + 3e-6 \cdot income - 0.646
Student\} \} \$\$ We can make predictions: a student with a credit card
balnce of \$1,500 and an income of \$40,000 has an estimated probability
of default:

\[
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
} = 0.05780859
\]

A non student with the same balance and income has an estiamted
probability of default of

\[
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
} = 0.1048655
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{default\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ default) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{new\_data =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{income =} \DecValTok{40000}\NormalTok{, }\AttributeTok{balance =} \DecValTok{1500}\NormalTok{, }\AttributeTok{student =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 6
  income balance student .pred_class .pred_No .pred_Yes
   <dbl>   <dbl> <chr>   <fct>          <dbl>     <dbl>
1  40000    1500 Yes     No             0.942    0.0579
2  40000    1500 No      No             0.895    0.105 
\end{verbatim}

\hypertarget{logistic-regression-for-2-response-classes}{%
\subsection{Logistic Regression for \textgreater{} 2 Response
Classes}\label{logistic-regression-for-2-response-classes}}

What if our response variable has more than two classes. Like medical
condition in the emergency room: \texttt{stroke},
\texttt{drug\ overdose}, \texttt{epileptic\ seizure}. In this setting we
wish to model both \(Pr(Y = stroke | x)\) and
\(Pr(Y = drug \space overdose | x)\), with the remaining
\(Pr(Y = epileptic \space seizure |x) = 1 - Pr(Y= stroke | x) - Pr(Y = drug \space overdose | x)\).
We can do it my extending logistic regression but \emph{Linear
Discriminant Analysis} is much suitable for this task

\hypertarget{linear-discriminant-analysis}{%
\section{Linear Discriminant
Analysis}\label{linear-discriminant-analysis}}

Logistic regression involves directly modeling \(Pr(Y = k | X = x)\)
using the logistic function, given by (4.7) for the case of two response
classes. In statistical jargon, we model the conditional distribution of
the response \(Y\), given the predictor(s) \(X\).

We now consider an alternative and less direct approach to estimating
these probabilities. In this alternative approach, we model the
distribution of the predictors \(X\) separately in each of the response
classes (i.e.~given \(Y\)), and then use Bayes' theorem to flip these
around into estimates for \(Pr(Y = k | X = x)\). When these
distributions are assumed to be normal, it turns out that the model is
very similar in form to logistic regression.

Why do we need another method when we have logistic regression?

\begin{itemize}
\item
  When the classes are well separated, the parameter estimates for the
  logistic regression model are surprisingly unstable. Linear
  discrimnant analysis does not suffer from this problem.
\item
  If \emph{n} is small and the distribution of the predictors \(X\) is
  approximately normal in each of the classes the linear discriminant
  model is again more stable than th elogistic regression model
\item
  LDA is popular when we have more than two response classes.
\end{itemize}

\hypertarget{using-bayes-theorem-for-classification}{%
\subsection{Using Bayes' Theorem for
Classification}\label{using-bayes-theorem-for-classification}}

We want to classify an observation into one of \(K\) classes, where
\(2 \leq K\). So \(Y\) can take on \(K\) possible distinct and unordered
values. Let \(\pi_k\) represent the overall or \emph{prior} probability
that a randomly chosen observation comes from the \(k\)th class.; this
is the probability that a given observation is associated with the
\emph{k}th category of the response variable \(Y\). Let
\(f_k(x) \equiv Pr(X = x | Y = k)\) denote the \emph{density} function
of \(X\) for an observation that comes from the \emph{k} th class. In
other words \(f_k(x)\) is relatively large if there is a high
probability that an observation in the \emph{k} th class has
\(X \approx x\), and \(f_k(x)\) is small if it is very unlikely that an
observation in the \emph{k}th class has \(X \approx x\). Then
\emph{Bayes's theorem} states that

\[
Pr(Y = k | X = x) = \frac{
\pi_k\,f_k(x)
}
{
\sum^K_{l = 1}\,\pi_l\,f_l(x)
}
\] (4.10)

We will use the abbreviation \(p_k(X) = Pr(Y = k | X)\). This suggests
that instead of directly computing \(p_k(X)\) as Section 4.3.1, we can
simply plug in estimates of \(\pi_k\) and \(f_k(X)\) into (4.10). In
general, estimating \(\pi_k\) is easy if we have a random sample of
\(Y\)s from the population: we simply compute the fraction of the
training observations that belong to the \emph{k}th class. However,
etimating \(f_k(X)\) tends to be more challenging, unless we assume some
simple forms for these densities. We refer to \(p_k(x)\) as the
\emph{posterior} probability than an observation \(X=x\) belongs to the
\emph{k}th class. That is, it is the probability that the observation
belongs to the \emph{k}th class, \emph{given} the predictor value for
that observation.

We know from Ch.2 that the Bayes classifier, which classifies an
observation to the class for which \(p_k(X)\) is largest, has the lowest
possible error rate out of all classifiers. Therefore, if we can find a
way to estimate \(f_k(X)\) then we can develop a classifier that
approximates the bayes classifier:

\hypertarget{linear-discriminant-analysis-for-p1}{%
\subsection{Linear Discriminant Analysis for
p=1}\label{linear-discriminant-analysis-for-p1}}

Assume we have one predictor. We would like to obtain an estimate for
\(f_k(x)\) that we can plug into (4.10) in order to estimate \(p_k(x)\).
We will then classify an observation to the class for which \(p_k(x)\)
is greatests. In order to estimate \(f_k(x)\) we will first make some
assumptions about this form.

Suppose we assume \(f_k(x)\) is \emph{normal} or \emph{Gaussian}. In the
one dimensional setting, the normal density takes the form

\[
f_k(x) = \frac{
1
}
{
\sqrt{2\pi}\,\sigma_k
}\,exp\,\left(-\frac{1}{2\sigma^2_k}(x - \mu_k)^2\right)
\] (4.11)

where \(\mu_k\) and \(\sigma^2_k\) are the mean and variance parameters
for the \emph{k}th class. For now, let us further assume that
\(\sigma_1^2=\dots=\sigma^2_K\): that is, there is shared variance term
across all \(K\) classes, which for simplicty we can denote by
\(\sigma^2\). Plugging (4.11) into (4.10) we find that

\[
p_k(x) = \frac{
\pi_k\,\frac{1}{\sqrt{2\pi}\,\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_k)^2)
}
{
\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_l)^2)
}
\] (4.12)

The bayes classifier involves assigning an observation \(X=x\) to the
class for which (4.12) is largest. Taking the log of (4.12) and
rearranging the terms, it is not hard to show that this is equivalent to
assigning the observation to the class for which

\[
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} \,- \frac{\mu_k^2}{2\sigma^2}\,+ \log(\pi_k)
\] (4.13)

is largest. For instance if \(K = 2\) and \(\pi_1 = \pi_2\), then the
Bayes clasffier assigns an obsetvation to classs 1 if
\(2x(\mu_1 - \mu_2) > \mu_1^2 - \mu_2^2\), ann to class 2 otherwise. In
this case, the Bayes desicion boundary corresponds to the point where

\$\$ x = \frac{\mu_1^2 - \mu_2^2}{2(\mu_1 - \mu_2)}=
\frac{\mu_1 + \mu_2}{2}

\$\$ (4.14)

An example is shown in the left hand panel of fig 4.4

\includegraphics{fig4.4.png}

The two normal density functions that are displayed, \(f_1(x)\) and
\(f_2(x)\), represent two distinct classes. The mean and the variance
parameters for the two desity functions are
\(\mu_1 = -1.25, \space \mu_2 = 1.25\), and
\(\sigma_1^2 = \sigma_2^2 = 1\). The two densities overlap, and so given
that \(X=x\), there is some uncertainty about the class to which
observation belongs. If we assume that an observation is equally likely
to come from either class--that is \(\pi_1 = \pi_2 = 0.5\)-- then by
inspection of (4.14), we see that the Bayes classfier assigns the
observation to class 1 if \(x<0\) and class 2 otherwise. Note that in
this case, we can compute the Bayes classfier because we know that \(X\)
is drawn from a gaussian distribution within each class, and we know all
of the parameters involved. In a real-life situation we are not able to
calcualte Bayes classifier.

In practice, even if we are quite certain of our assumption that \(X\)
is drawn from a Gaussian distribution within each class, we still have
to estimate the parameters
\(\mu_1, \dots, \mu_K,\space \pi_1,\dots, \pi_K,\) and \(\sigma^2\). The
\emph{linear discriminant analysis} (LDA) method approximates the bayes
classfier vy plugging estimates fro \(\pi_k, \mu_k\) and \(\sigma^2\)
into (4.13). In particular the following estimates are used.:

\$\$ \begin{align}
\hat{\mu}_k &= \frac{1}{n_k}\sum_{i:y_i=k} x_i \\

\hat{\sigma}^2 &= \frac{1}{n-K}\sum^K_{k=1} \sum_{i:y_i =k}(x_i - \hat{\mu}_k)^2

\end{align} \$\$ (4.15) \textbf{Important}

where \emph{n} is the total number of training observations, and
\emph{n\_k} is the number of training observations in the \emph{k}th
class. The estimate for \(\mu_k\) is simply the average of all the
training observations from the \emph{k}th class, while
\(\hat{\sigma}^2\) can be seen as a weighted average of the sample
variances for each of the \(K\) classes. Sometimes we have knowledge of
the class membership probabilities \(\pi_1, \dots, \pi_K\), which can be
used directly. In the absence of any additional information, LDA
estimates \(\pi_K\) using the proportion of the training observations
that belong to the \emph{k}th class. In other words,

\[
\pi_k = n_k /n
\] The LDA classifer plugs the estimates given in (4.15) and (4.16) into
(4.13), and assigns an observation \(X=x\) to the class for which

\[
\hat{\delta}(x) = x \cdot \,\frac{\hat{u}_k}{\hat{\sigma}^2} \,-\,\frac{\hat{u}^2_k}{2\,\hat{\sigma}^2}\,+\,\log(\hat{\pi}_k)
\] (4.17)

is largest. The word \emph{linear} in classfier's name stems from the
fact that \emph{discriminant functions} \(\hat{\delta}_k(x)\) in (4.17)
are linear functions of \(x\).

The right hand panel of Fig 4.4 displays a histogram of a random sample
\(n=20\) observations from each class. To implement LDA, we behan by
estimating \(\pi_k, \mu_k, \sigma^2\) using (4.15) and (4.16). We then
computed the desicion boundy, shown as a black solid line, that results
from assigning an observation to the clas for which (4.17) is largest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\NormalTok{a }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{20}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{1.25}\NormalTok{, }\AttributeTok{sd =}\DecValTok{1}\NormalTok{)}
\NormalTok{b }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{20}\NormalTok{, }\AttributeTok{mean=}\FloatTok{1.25}\NormalTok{, }\AttributeTok{sd =}\DecValTok{1}\NormalTok{)}
\NormalTok{d }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a, }\AttributeTok{class =} \DecValTok{1}\NormalTok{),}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =}\NormalTok{ b, }\AttributeTok{class =}\DecValTok{2}\NormalTok{))}

\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{color =}\NormalTok{ class, }\AttributeTok{group =}\NormalTok{ class) }\SpecialCharTok{+} \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{show.legend =}\NormalTok{ F) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d

Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font
width unknown for character 0x2d
\end{verbatim}

\begin{verbatim}
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :
font width unknown for character 0x2d
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-22-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(class) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_hat =} \FunctionTok{mean}\NormalTok{(x), }\AttributeTok{var\_hat =} \FunctionTok{sd}\NormalTok{(x)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  class mean_hat var_hat
  <dbl>    <dbl>   <dbl>
1     1    -1.58   0.751
2     2     1.35   0.781
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(class) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pi\_hat =}\NormalTok{ count}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(count))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  class count pi_hat
  <dbl> <int>  <dbl>
1     1    20    0.5
2     2    20    0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{d\_k\_c1 =}\NormalTok{ x }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\FloatTok{1.58}\SpecialCharTok{/}\FloatTok{0.751}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\SpecialCharTok{{-}}\FloatTok{1.58}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\FloatTok{0.751}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(}\FloatTok{0.5}\NormalTok{),}
         \AttributeTok{d\_k\_c2 =}\NormalTok{ x }\SpecialCharTok{*}\NormalTok{ (}\FloatTok{1.35} \SpecialCharTok{/} \FloatTok{0.781}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.35}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\FloatTok{0.781}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(}\FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pred.class =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(d\_k\_c1 }\SpecialCharTok{\textgreater{}}\NormalTok{ d\_k\_c2,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)), }\AttributeTok{class =} \FunctionTok{factor}\NormalTok{(class)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ class, }\AttributeTok{estimate =}\NormalTok{ pred.class) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Chapter4_files/figure-pdf/unnamed-chunk-25-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(discrim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'discrim'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:dials':

    smoothness
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{class =} \FunctionTok{factor}\NormalTok{(class)) }\OtherTok{{-}\textgreater{}}\NormalTok{ d}

\FunctionTok{discrim\_linear}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"MASS"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ lda\_spec}

\NormalTok{lda\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(class}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data =}\NormalTok{d)}

\FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_model}\NormalTok{(lda\_spec) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_recipe}\NormalTok{(lda\_recipe) }\OtherTok{{-}\textgreater{}}\NormalTok{ lda\_workflow}

\NormalTok{lda\_workflow }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ d) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{new\_data=}\NormalTok{d) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 40 x 5
        x class .pred_class .pred_1    .pred_2
    <dbl> <fct> <fct>         <dbl>      <dbl>
 1 -2.73  1     1             1.00  0.0000459 
 2  0.327 1     2             0.155 0.845     
 3 -2.21  1     1             1.00  0.000340  
 4 -2.17  1     1             1.00  0.000391  
 5 -3.25  1     1             1.00  0.00000637
 6 -1.52  1     1             0.995 0.00463   
 7 -1.57  1     1             0.996 0.00393   
 8 -1.88  1     1             0.999 0.00119   
 9 -1.36  1     1             0.991 0.00869   
10 -0.822 1     1             0.937 0.0633    
# i 30 more rows
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{exercise}{%
\chapter{Exercise}\label{exercise}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
\FunctionTok{library}\NormalTok{(ISLR)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(sjPlot)}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(dotwhisker)}
\FunctionTok{library}\NormalTok{(hrbrthemes)}
\FunctionTok{library}\NormalTok{(patchwork)}
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{library}\NormalTok{(showtext)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{slr}{%
\section{SLR}\label{slr}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{=}\NormalTok{ MASS}\SpecialCharTok{::}\NormalTok{Boston}
\NormalTok{data }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 506 x 14
      crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio black
     <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl>   <dbl> <dbl>
 1 0.00632  18    2.31     0 0.538  6.58  65.2  4.09     1   296    15.3  397.
 2 0.0273    0    7.07     0 0.469  6.42  78.9  4.97     2   242    17.8  397.
 3 0.0273    0    7.07     0 0.469  7.18  61.1  4.97     2   242    17.8  393.
 4 0.0324    0    2.18     0 0.458  7.00  45.8  6.06     3   222    18.7  395.
 5 0.0690    0    2.18     0 0.458  7.15  54.2  6.06     3   222    18.7  397.
 6 0.0298    0    2.18     0 0.458  6.43  58.7  6.06     3   222    18.7  394.
 7 0.0883   12.5  7.87     0 0.524  6.01  66.6  5.56     5   311    15.2  396.
 8 0.145    12.5  7.87     0 0.524  6.17  96.1  5.95     5   311    15.2  397.
 9 0.211    12.5  7.87     0 0.524  5.63 100    6.08     5   311    15.2  387.
10 0.170    12.5  7.87     0 0.524  6.00  85.9  6.59     5   311    15.2  387.
# i 496 more rows
# i 2 more variables: lstat <dbl>, medv <dbl>
\end{verbatim}

We are going to predict \texttt{medv}(median house value) in Boston,
using 13 predictors such as \texttt{rm}(average number of rooms per
house), \texttt{age}(avarege age of houses), and \texttt{lstat} (percent
of household with low socioeconomics status).

Do a linear regression of \texttt{lstat} on \texttt{medv}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ slr\_res }

\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 34.55384    0.56263   61.41   <2e-16 ***
lstat       -0.95005    0.03873  -24.53   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        
\end{verbatim}

Get the confidence intervals

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{confint}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                2.5 %     97.5 %
(Intercept) 33.448457 35.6592247
lstat       -1.026148 -0.8739505
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dotwhisker}\SpecialCharTok{::}\FunctionTok{dwplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

Lets do some predictions from this model. Lets create predictions for
cases when \texttt{lstat} is equal to 5,10, and 15. We can use
\texttt{predict} function for this. Here is a confidence interval for
our p

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{predict}\NormalTok{(., }\FunctionTok{tibble}\NormalTok{(}\AttributeTok{lstat =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{)), }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       fit      lwr      upr
1 29.80359 29.00741 30.59978
2 25.05335 24.47413 25.63256
3 20.30310 19.73159 20.87461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{predict}\NormalTok{(., }\FunctionTok{tibble}\NormalTok{(}\AttributeTok{lstat =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{)), }\AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       fit       lwr      upr
1 29.80359 17.565675 42.04151
2 25.05335 12.827626 37.27907
3 20.30310  8.077742 32.52846
\end{verbatim}

Lets plot \texttt{medv} and \texttt{lstat} along with the least squares
regression line.

We can either:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{medv, data}\SpecialCharTok{$}\NormalTok{lstat)}
\FunctionTok{abline}\NormalTok{(slr\_res)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ augment }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lstat, }\AttributeTok{y =}\NormalTok{ medv) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ slr\_res}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\AttributeTok{slope =}\NormalTok{ slr\_res}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\AttributeTok{size =}\FloatTok{1.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 506 x 8
    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid
   <dbl> <dbl>   <dbl>  <dbl>   <dbl>  <dbl>       <dbl>      <dbl>
 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 
 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 
 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 
 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 
 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  
 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 
 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250
 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  
 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  
10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954
# i 496 more rows
\end{verbatim}

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

This looks like not a linear relationship actually; lets confirm this
with our residual plot. Since this is a SLR i can plot \(x\) vs \(e_i\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ augment }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{ .resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se=}\NormalTok{F) }\OtherTok{{-}\textgreater{}}\NormalTok{ p1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 506 x 8
    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid
   <dbl> <dbl>   <dbl>  <dbl>   <dbl>  <dbl>       <dbl>      <dbl>
 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 
 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 
 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 
 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 
 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  
 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 
 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250
 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  
 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  
10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954
# i 496 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ augment }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{ .std.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se=}\NormalTok{F) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 506 x 8
    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid
   <dbl> <dbl>   <dbl>  <dbl>   <dbl>  <dbl>       <dbl>      <dbl>
 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 
 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 
 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 
 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 
 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  
 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 
 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250
 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  
 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  
10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954
# i 496 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

Our suspicions are true, it looks like there may not be a linear
relationship, this shows a quadratic relationship.

Our leverage statistics are given in the \texttt{.hat} column, but we
can also get them using \texttt{hatvalues}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{hatvalues}\NormalTok{(slr\_res))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{which.max}\NormalTok{(}\FunctionTok{hatvalues}\NormalTok{(slr\_res))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
375 
375 
\end{verbatim}

Lets check the outliers by plotting standardized residuals vs fitted

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{506}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(id,.std.resid) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(}\StringTok{"id"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ outliers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
      id .std.resid
   <int>      <dbl>
 1   164       3.00
 2   167       3.06
 3   187       3.17
 4   226       3.20
 5   258       3.27
 6   263       3.20
 7   268       3.63
 8   370       3.06
 9   372       3.95
10   373       3.85
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# visualize the outliers}

\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{506}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{.std.resid, }\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{color =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{, F,T)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =}\DecValTok{2}\NormalTok{, }\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+} \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{,id,}\StringTok{""}\NormalTok{)), }\AttributeTok{max.overlaps =} \DecValTok{100}\NormalTok{, }\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+} \FunctionTok{scale\_color\_ipsum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Leverage points: we plot std res vs leverage

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{506}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(.hat }\SpecialCharTok{\textgreater{}} \FloatTok{0.02}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(id, .std.resid,.hat) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{print}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(}\StringTok{"id"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ highlevgs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 3
     id .std.resid   .hat
  <int>      <dbl>  <dbl>
1   142       2.04 0.0204
2   374       2.00 0.0210
3   375       2.50 0.0269
4   413       2.60 0.0203
5   415       1.23 0.0250
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# high leverage points}

\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{506}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{.hat, }\AttributeTok{y =}\NormalTok{ .std.resid, }\AttributeTok{color =} \FunctionTok{ifelse}\NormalTok{(.hat }\SpecialCharTok{\textgreater{}} \FloatTok{0.02}\NormalTok{,F,T)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+} \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} 
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{ifelse}\NormalTok{(.hat }\SpecialCharTok{\textgreater{}} \FloatTok{0.02}\NormalTok{,id,}\StringTok{""}\NormalTok{)), }\AttributeTok{show.legend =}\NormalTok{ F) }\SpecialCharTok{+}\FunctionTok{scale\_color\_ipsum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.168  -3.990  -1.318   2.034  24.500 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 34.55384    0.56263   61.41   <2e-16 ***
lstat       -0.95005    0.03873  -24.53   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.216 on 504 degrees of freedom
Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, data[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(outliers,highlevgs),]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.633  -3.465  -1.012   1.937  19.060 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  34.0494     0.5110   66.63   <2e-16 ***
lstat        -0.9538     0.0358  -26.64   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.376 on 489 degrees of freedom
Multiple R-squared:  0.5921,    Adjusted R-squared:  0.5912 
F-statistic: 709.7 on 1 and 489 DF,  p-value: < 2.2e-16
\end{verbatim}

Removing high leverages and outliers improeves our result.

\hypertarget{multiple-linear-regression-1}{%
\section{Multiple linear
Regression}\label{multiple-linear-regression-1}}

\[
medv = \beta_0 + \beta_1 lstat + \beta_2 age 
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+}\NormalTok{ age, data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat + age, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.981  -3.978  -1.283   1.968  23.158 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***
lstat       -1.03207    0.04819 -21.416  < 2e-16 ***
age          0.03454    0.01223   2.826  0.00491 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.173 on 503 degrees of freedom
Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16
\end{verbatim}

Lets do a regression including all varaibles

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv}\SpecialCharTok{\textasciitilde{}}\NormalTok{., data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ mlr}
\NormalTok{mlr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ ., data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16
\end{verbatim}

Lets calculate the VIF values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mlr }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    crim       zn    indus     chas      nox       rm      age      dis 
1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 
     rad      tax  ptratio    black    lstat 
7.484496 9.008554 1.799084 1.348521 2.941491 
\end{verbatim}

\texttt{age} has the highest p-value, lets remove that from the
regression

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv}\SpecialCharTok{\textasciitilde{}}\NormalTok{. }\SpecialCharTok{{-}}\NormalTok{ age, data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ . - age, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.6054  -2.7313  -0.5188   1.7601  26.2243 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
crim         -0.108006   0.032832  -3.290 0.001075 ** 
zn            0.046334   0.013613   3.404 0.000719 ***
indus         0.020562   0.061433   0.335 0.737989    
chas          2.689026   0.859598   3.128 0.001863 ** 
nox         -17.713540   3.679308  -4.814 1.97e-06 ***
rm            3.814394   0.408480   9.338  < 2e-16 ***
dis          -1.478612   0.190611  -7.757 5.03e-14 ***
rad           0.305786   0.066089   4.627 4.75e-06 ***
tax          -0.012329   0.003755  -3.283 0.001099 ** 
ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
black         0.009321   0.002678   3.481 0.000544 ***
lstat        -0.523852   0.047625 -10.999  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.74 on 493 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 
F-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{interaction-terms}{%
\subsection{Interaction Terms}\label{interaction-terms}}

following model

\[
medv = \beta_0 + \beta_1 lstat + \beta_2 age + \beta_3 lstat \cdot age + \epsilon
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{*}\NormalTok{ age, data) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# short for fit(medv\textasciitilde{} lstat + age + lstat * age)}
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat * age, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.806  -4.045  -1.333   2.085  27.552 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***
lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
age         -0.0007209  0.0198792  -0.036   0.9711    
lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.149 on 502 degrees of freedom
Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
F-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{non-linaer-transformation-of-the-predictors}{%
\subsection{Non-linaer Transformation of the
predictors}\label{non-linaer-transformation-of-the-predictors}}

\[
medv = \beta_0 + \beta_1 lstat + \beta_2 lstat^2 + \epsilon
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(lstat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ lstat + I(lstat^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2834  -3.8313  -0.5295   2.3095  25.4148 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 42.862007   0.872084   49.15   <2e-16 ***
lstat       -2.332821   0.123803  -18.84   <2e-16 ***
I(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.524 on 503 degrees of freedom
Multiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 
F-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16
\end{verbatim}

The coefficient of the quadratic term is statistically significant:
model is improved.

We use \texttt{anova} function to further quantify the extent to which
quadratic fit is superior to linear fit. We It takes models as its
arguments. We want to compare two models, lets create them again and
compare them

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ modelLin}

\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(lstat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ modelQuad}

\FunctionTok{anova}\NormalTok{(modelLin, modelQuad)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: medv ~ lstat
Model 2: medv ~ lstat + I(lstat^2)
  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    
1    504 19472                                 
2    503 15347  1    4125.1 135.2 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The annova function performs a hypothesis test comparing the two models.
The null hypothesis is that the two models fit the data equally well,
and the alternative hypothesis is that the full model is superior. Here
we reject \(H_0\); and the model indcludes quadratic term is superior to
the linaer odel. This is not a suprise to us, because we already
suspected this.

Lets do the plots again

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat, data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Linear model residual plot"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ p1}

\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lstat }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(lstat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Quadratic model residual plot"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\end{figure}

Now on the quadratic model it looks like there is still some pattern in
the residuals. Should we create a cubic fit? Now what we can do is
actually insert like 10 polynomial terms and check whether they are
significant or not

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(lstat,}\DecValTok{10}\NormalTok{),data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ poly(lstat, 10), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.5340  -3.0286  -0.7507   2.0437  26.4738 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)         22.5328     0.2311  97.488  < 2e-16 ***
poly(lstat, 10)1  -152.4595     5.1993 -29.323  < 2e-16 ***
poly(lstat, 10)2    64.2272     5.1993  12.353  < 2e-16 ***
poly(lstat, 10)3   -27.0511     5.1993  -5.203 2.88e-07 ***
poly(lstat, 10)4    25.4517     5.1993   4.895 1.33e-06 ***
poly(lstat, 10)5   -19.2524     5.1993  -3.703 0.000237 ***
poly(lstat, 10)6     6.5088     5.1993   1.252 0.211211    
poly(lstat, 10)7     1.9416     5.1993   0.373 0.708977    
poly(lstat, 10)8    -6.7299     5.1993  -1.294 0.196133    
poly(lstat, 10)9     8.4168     5.1993   1.619 0.106116    
poly(lstat, 10)10   -7.3351     5.1993  -1.411 0.158930    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.199 on 495 degrees of freedom
Multiple R-squared:  0.6867,    Adjusted R-squared:  0.6804 
F-statistic: 108.5 on 10 and 495 DF,  p-value: < 2.2e-16
\end{verbatim}

We see that up to 5 polynomial terms are statistically significant

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(lstat,}\DecValTok{5}\NormalTok{),data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ poly(lstat, 5), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5433  -3.1039  -0.7052   2.0844  27.1153 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***
poly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***
poly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***
poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.215 on 500 degrees of freedom
Multiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 
F-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(lstat,}\DecValTok{5}\NormalTok{),data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"poly 5 model residual plot"}\NormalTok{)  }\OtherTok{{-}\textgreater{}}\NormalTok{ p3}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+}\NormalTok{ p3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-26-1.pdf}

}

\end{figure}

Instead of adding the polynomial transformations, we could try log
transformation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(lstat),data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{y =}\NormalTok{.resid) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =}\NormalTok{F) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"log model residual plot"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{analysis_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\end{figure}

This is actually very nice.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(rm),data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ log(rm), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.487  -2.875  -0.104   2.837  39.816 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -76.488      5.028  -15.21   <2e-16 ***
log(rm)       54.055      2.739   19.73   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.915 on 504 degrees of freedom
Multiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 
F-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fit}\NormalTok{(medv}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pluck}\NormalTok{(}\StringTok{"fit"}\NormalTok{) }\OtherTok{{-}\textgreater{}}\NormalTok{ lmm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{anova}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: medv
           Df  Sum Sq Mean Sq  F value    Pr(>F)    
crim        1  6440.8  6440.8 286.0300 < 2.2e-16 ***
zn          1  3554.3  3554.3 157.8452 < 2.2e-16 ***
indus       1  2551.2  2551.2 113.2984 < 2.2e-16 ***
chas        1  1529.8  1529.8  67.9393 1.543e-15 ***
nox         1    76.2    76.2   3.3861 0.0663505 .  
rm          1 10938.1 10938.1 485.7530 < 2.2e-16 ***
age         1    90.3    90.3   4.0087 0.0458137 *  
dis         1  1779.5  1779.5  79.0262 < 2.2e-16 ***
rad         1    34.1    34.1   1.5159 0.2188325    
tax         1   329.6   329.6  14.6352 0.0001472 ***
ptratio     1  1309.3  1309.3  58.1454 1.266e-13 ***
black       1   593.3   593.3  26.3496 4.109e-07 ***
lstat       1  2410.8  2410.8 107.0634 < 2.2e-16 ***
Residuals 492 11078.8    22.5                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{anova}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: medv
           Df  Sum Sq Mean Sq  F value    Pr(>F)    
crim        1  6440.8  6440.8 286.0300 < 2.2e-16 ***
zn          1  3554.3  3554.3 157.8452 < 2.2e-16 ***
indus       1  2551.2  2551.2 113.2984 < 2.2e-16 ***
chas        1  1529.8  1529.8  67.9393 1.543e-15 ***
nox         1    76.2    76.2   3.3861 0.0663505 .  
rm          1 10938.1 10938.1 485.7530 < 2.2e-16 ***
age         1    90.3    90.3   4.0087 0.0458137 *  
dis         1  1779.5  1779.5  79.0262 < 2.2e-16 ***
rad         1    34.1    34.1   1.5159 0.2188325    
tax         1   329.6   329.6  14.6352 0.0001472 ***
ptratio     1  1309.3  1309.3  58.1454 1.266e-13 ***
black       1   593.3   593.3  26.3496 4.109e-07 ***
lstat       1  2410.8  2410.8 107.0634 < 2.2e-16 ***
Residuals 492 11078.8    22.5                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(}\AttributeTok{direction =} \StringTok{"backward"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=1589.64
medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + 
    tax + ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- age      1      0.06 11079 1587.7
- indus    1      2.52 11081 1587.8
<none>                 11079 1589.6
- chas     1    218.97 11298 1597.5
- tax      1    242.26 11321 1598.6
- crim     1    243.22 11322 1598.6
- zn       1    257.49 11336 1599.3
- black    1    270.63 11349 1599.8
- rad      1    479.15 11558 1609.1
- nox      1    487.16 11566 1609.4
- ptratio  1   1194.23 12273 1639.4
- dis      1   1232.41 12311 1641.0
- rm       1   1871.32 12950 1666.6
- lstat    1   2410.84 13490 1687.3

Step:  AIC=1587.65
medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + 
    ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- indus    1      2.52 11081 1585.8
<none>                 11079 1587.7
- chas     1    219.91 11299 1595.6
- tax      1    242.24 11321 1596.6
- crim     1    243.20 11322 1596.6
- zn       1    260.32 11339 1597.4
- black    1    272.26 11351 1597.9
- rad      1    481.09 11560 1607.2
- nox      1    520.87 11600 1608.9
- ptratio  1   1200.23 12279 1637.7
- dis      1   1352.26 12431 1643.9
- rm       1   1959.55 13038 1668.0
- lstat    1   2718.88 13798 1696.7

Step:  AIC=1585.76
medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + 
    black + lstat

          Df Sum of Sq   RSS    AIC
<none>                 11081 1585.8
- chas     1    227.21 11309 1594.0
- crim     1    245.37 11327 1594.8
- zn       1    257.82 11339 1595.4
- black    1    270.82 11352 1596.0
- tax      1    273.62 11355 1596.1
- rad      1    500.92 11582 1606.1
- nox      1    541.91 11623 1607.9
- ptratio  1   1206.45 12288 1636.0
- dis      1   1448.94 12530 1645.9
- rm       1   1963.66 13045 1666.3
- lstat    1   2723.48 13805 1695.0
\end{verbatim}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ crim + zn + chas + nox + rm + dis + 
    rad + tax + ptratio + black + lstat, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***
crim         -0.108413   0.032779  -3.307 0.001010 ** 
zn            0.045845   0.013523   3.390 0.000754 ***
chas          2.718716   0.854240   3.183 0.001551 ** 
nox         -17.376023   3.535243  -4.915 1.21e-06 ***
rm            3.801579   0.406316   9.356  < 2e-16 ***
dis          -1.492711   0.185731  -8.037 6.84e-15 ***
rad           0.299608   0.063402   4.726 3.00e-06 ***
tax          -0.011778   0.003372  -3.493 0.000521 ***
ptratio      -0.946525   0.129066  -7.334 9.24e-13 ***
black         0.009291   0.002674   3.475 0.000557 ***
lstat        -0.522553   0.047424 -11.019  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(}\AttributeTok{direction =} \StringTok{"forward"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=1589.64
medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + 
    tax + ptratio + black + lstat
\end{verbatim}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ crim + zn + indus + chas + nox + rm + 
    age + dis + rad + tax + ptratio + black + lstat, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.595  -2.730  -0.518   1.777  26.199 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
zn           4.642e-02  1.373e-02   3.382 0.000778 ***
indus        2.056e-02  6.150e-02   0.334 0.738288    
chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***
age          6.922e-04  1.321e-02   0.052 0.958229    
dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
black        9.312e-03  2.686e-03   3.467 0.000573 ***
lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.745 on 492 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 
F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{step}\NormalTok{(}\AttributeTok{direction =} \StringTok{"both"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=1589.64
medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + 
    tax + ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- age      1      0.06 11079 1587.7
- indus    1      2.52 11081 1587.8
<none>                 11079 1589.6
- chas     1    218.97 11298 1597.5
- tax      1    242.26 11321 1598.6
- crim     1    243.22 11322 1598.6
- zn       1    257.49 11336 1599.3
- black    1    270.63 11349 1599.8
- rad      1    479.15 11558 1609.1
- nox      1    487.16 11566 1609.4
- ptratio  1   1194.23 12273 1639.4
- dis      1   1232.41 12311 1641.0
- rm       1   1871.32 12950 1666.6
- lstat    1   2410.84 13490 1687.3

Step:  AIC=1587.65
medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + 
    ptratio + black + lstat

          Df Sum of Sq   RSS    AIC
- indus    1      2.52 11081 1585.8
<none>                 11079 1587.7
+ age      1      0.06 11079 1589.6
- chas     1    219.91 11299 1595.6
- tax      1    242.24 11321 1596.6
- crim     1    243.20 11322 1596.6
- zn       1    260.32 11339 1597.4
- black    1    272.26 11351 1597.9
- rad      1    481.09 11560 1607.2
- nox      1    520.87 11600 1608.9
- ptratio  1   1200.23 12279 1637.7
- dis      1   1352.26 12431 1643.9
- rm       1   1959.55 13038 1668.0
- lstat    1   2718.88 13798 1696.7

Step:  AIC=1585.76
medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + 
    black + lstat

          Df Sum of Sq   RSS    AIC
<none>                 11081 1585.8
+ indus    1      2.52 11079 1587.7
+ age      1      0.06 11081 1587.8
- chas     1    227.21 11309 1594.0
- crim     1    245.37 11327 1594.8
- zn       1    257.82 11339 1595.4
- black    1    270.82 11352 1596.0
- tax      1    273.62 11355 1596.1
- rad      1    500.92 11582 1606.1
- nox      1    541.91 11623 1607.9
- ptratio  1   1206.45 12288 1636.0
- dis      1   1448.94 12530 1645.9
- rm       1   1963.66 13045 1666.3
- lstat    1   2723.48 13805 1695.0
\end{verbatim}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ crim + zn + chas + nox + rm + dis + 
    rad + tax + ptratio + black + lstat, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.5984  -2.7386  -0.5046   1.7273  26.2373 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***
crim         -0.108413   0.032779  -3.307 0.001010 ** 
zn            0.045845   0.013523   3.390 0.000754 ***
chas          2.718716   0.854240   3.183 0.001551 ** 
nox         -17.376023   3.535243  -4.915 1.21e-06 ***
rm            3.801579   0.406316   9.356  < 2e-16 ***
dis          -1.492711   0.185731  -8.037 6.84e-15 ***
rad           0.299608   0.063402   4.726 3.00e-06 ***
tax          -0.011778   0.003372  -3.493 0.000521 ***
ptratio      -0.946525   0.129066  -7.334 9.24e-13 ***
black         0.009291   0.002674   3.475 0.000557 ***
lstat        -0.522553   0.047424 -11.019  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.736 on 494 degrees of freedom
Multiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 
F-statistic: 128.2 on 11 and 494 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
stats::lm(formula = medv ~ ., data = data)

Coefficients:
(Intercept)         crim           zn        indus         chas          nox  
  3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  
         rm          age          dis          rad          tax      ptratio  
  3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  
      black        lstat  
  9.312e-03   -5.248e-01  
\end{verbatim}



\end{document}
