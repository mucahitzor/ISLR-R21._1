<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ISLR-R21._1 - 1&nbsp; What Is Statistical Learning?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Chapter3.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter2.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What Is Statistical Learning?</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ISLR-R21._1</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./ISLR-R21._1.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./ISLR-R21._1.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <a href="https://twitter.com/intent/tweet?url=|url|" rel="" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What Is Statistical Learning?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-estimate-f" id="toc-why-estimate-f" class="nav-link active" data-scroll-target="#why-estimate-f"><span class="header-section-number">1.1</span> Why estimate <span class="math inline">\(f\)</span>?</a>
  <ul class="collapse">
  <li><a href="#how-do-we-estimate-f" id="toc-how-do-we-estimate-f" class="nav-link" data-scroll-target="#how-do-we-estimate-f"><span class="header-section-number">1.1.1</span> How Do We Estimate <span class="math inline">\(f\)</span>?</a></li>
  <li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability" id="toc-the-trade-off-between-prediction-accuracy-and-model-interpretability" class="nav-link" data-scroll-target="#the-trade-off-between-prediction-accuracy-and-model-interpretability"><span class="header-section-number">1.1.2</span> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
  <li><a href="#supervised-vs-unsupervised-learning" id="toc-supervised-vs-unsupervised-learning" class="nav-link" data-scroll-target="#supervised-vs-unsupervised-learning"><span class="header-section-number">1.1.3</span> Supervised vs Unsupervised Learning</a></li>
  </ul></li>
  <li><a href="#regression-vs-classisfication-problems" id="toc-regression-vs-classisfication-problems" class="nav-link" data-scroll-target="#regression-vs-classisfication-problems"><span class="header-section-number">1.2</span> Regression vs Classisfication Problems</a></li>
  <li><a href="#assessing-model-accuracy" id="toc-assessing-model-accuracy" class="nav-link" data-scroll-target="#assessing-model-accuracy"><span class="header-section-number">1.3</span> Assessing Model Accuracy</a>
  <ul class="collapse">
  <li><a href="#measuring-the-quality-of-fit" id="toc-measuring-the-quality-of-fit" class="nav-link" data-scroll-target="#measuring-the-quality-of-fit"><span class="header-section-number">1.3.1</span> Measuring the Quality of Fit</a></li>
  <li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-bias-variance-trade-off"><span class="header-section-number">1.3.2</span> The Bias-Variance Trade-Off</a></li>
  <li><a href="#the-classification-setting" id="toc-the-classification-setting" class="nav-link" data-scroll-target="#the-classification-setting"><span class="header-section-number">1.3.3</span> The Classification Setting</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What Is Statistical Learning?</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Question: How to improve sales of our product?</p>
<p>We have a data set:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>advertising <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"./data/Advertising.csv"</span>) <span class="sc">%&gt;%</span> as_tibble <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
Rows: 200 Columns: 5
── Column specification
──────────────────────────────────────────────────────── Delimiter: "," dbl
(5): ...1, TV, radio, newspaper, sales
ℹ Use `spec()` to retrieve the full column specification for this data. ℹ
Specify the column types or set `show_col_types = FALSE` to quiet this message.
• `` -&gt; `...1`</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>advertising</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 200 × 4
      TV radio newspaper sales
   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
 1 230.   37.8      69.2  22.1
 2  44.5  39.3      45.1  10.4
 3  17.2  45.9      69.3   9.3
 4 152.   41.3      58.5  18.5
 5 181.   10.8      58.4  12.9
 6   8.7  48.9      75     7.2
 7  57.5  32.8      23.5  11.8
 8 120.   19.6      11.6  13.2
 9   8.6   2.1       1     4.8
10 200.    2.6      21.2  10.6
# ℹ 190 more rows</code></pre>
</div>
</div>
<p><span class="math inline">\(n=200\)</span>, independent variables (predictors) are <code>TV</code>, <code>radio</code>, and <code>newspaper</code> advertising spendings in thousands of dollars. We want to explore their relationship with <code>sales</code>; quantity of product sold for each advertising mixture. If we determine association between advertising and sales, we can provide adjustment of advertisement budgeds based on most effective media to increase sales; we want to develop an accurate model that ca be used to predict sales on the basis of three media budgets.</p>
<p>We denote all input variables (actual–realized) as <span class="math inline">\(X_1, X_2, ..., X_p\)</span> and use <span class="math inline">\(X\)</span> to refer all of them. In this case <span class="math inline">\(X = (X_1, X_2, X_3)\)</span>. Sales is denoted with <span class="math inline">\(Y\)</span>.</p>
<p>This means we assume a relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> in a form of</p>
<p><span class="math display">\[
Y = f(X) + \epsilon
\]</span> (2.1)</p>
<ul>
<li>Here <span class="math inline">\(f\)</span> is some fixed, but unknown function of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\epsilon\)</span> is a random <em>error term</em> =&gt; independent of <span class="math inline">\(X\)</span> and has a mean zero.</li>
</ul>
<p>So <span class="math inline">\(f\)</span> represents systematic information that <span class="math inline">\(X\)</span> provides about <span class="math inline">\(Y\)</span>.</p>
<p><span class="math inline">\(f\)</span> is generally unknown. We will need to estimate <span class="math inline">\(f\)</span> baed on the observed points =&gt; <span class="math inline">\(\hat{f}\)</span>.</p>
<p>Statistical learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p>
<section id="why-estimate-f" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="why-estimate-f"><span class="header-section-number">1.1</span> Why estimate <span class="math inline">\(f\)</span>?</h2>
<p>Two reasons: * <em>prediction</em> * <em>inference</em></p>
<p><strong>Prediction</strong></p>
<p>Most of the time we have <span class="math inline">\(X\)</span> but we might not have <span class="math inline">\(Y\)</span>. In this setting, since the error term averages to zero, we can predict <span class="math inline">\(Y\)</span> using</p>
<p><span class="math display">\[
\hat{Y} = \hat{f}X
\]</span> (2.2)</p>
<p>Here <span class="math inline">\(\hat{f}\)</span> is treated as a <em>black box</em>. We are not concerned with the exact form of <span class="math inline">\(\hat{f}\)</span>, we just want to have accurate predictions of <span class="math inline">\(Y\)</span>.</p>
<p>Imagine we have <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span>; blood sample characteristics of patients. <span class="math inline">\(Y\)</span> is a variable showing the patient’s risk for a adverse reaction to a drug. We don’t want to give the drug and see the reaction, so we want to predict reactions.</p>
<p>The accuracy of our predictions <span class="math inline">\(\hat{Y}\)</span> of <span class="math inline">\(Y\)</span>, depends on two quantities:</p>
<ul>
<li><p><em>reducible error</em></p>
<p>Generally <span class="math inline">\(\hat{f}\)</span> will not be a perfect estimate for <span class="math inline">\(f\)</span>. This inaccuracy will introduce some error, which we call reducible error since we can improve our accuracy of <span class="math inline">\(\hat{f}\)</span> using the most appropriate statistical leraning method.</p></li>
<li><p><em>irreducible error</em></p>
<p>Even if we estimate <span class="math inline">\(f\)</span> perfectly, our estimated response would take the form <span class="math inline">\(\hat{Y} = f(X)\)</span>; our predictions would still get some error. This is because <span class="math inline">\(Y\)</span> is not just a function of <span class="math inline">\(X\)</span> but also a function of <span class="math inline">\(\epsilon\)</span>, which cannot be predicted by <span class="math inline">\(X\)</span>. So the level of <span class="math inline">\(\epsilon\)</span> would also effect our prediciton accuracy. And we cannot remove this error; thus, irreducible.</p>
<p><span class="math inline">\(\epsilon\)</span> is larger than zero; because <span class="math inline">\(\epsilon\)</span> may contain some variables we don’t include in our model, but effect <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p><strong>Inference</strong></p>
<p>Here we want to understand the way that <span class="math inline">\(Y\)</span> is affected by <span class="math inline">\(X\)</span>. In this setting, we wish to estimate <span class="math inline">\(f\)</span> but we are not concerned with predicting. We want to understand the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>; how <span class="math inline">\(Y\)</span> changes as <span class="math inline">\(X\)</span> changes. We <strong>don’t</strong> treate <span class="math inline">\(\hat{f}\)</span> as a <em>black box</em> now since we need to know its exact form. In this setting we are interested in answering questions such as</p>
<ul>
<li><p><em>Which predictors are associated with the response?</em></p>
<p>Usually not all predictors are associated with <span class="math inline">\(Y\)</span>. We need to identify the <em>important</em> predictors among a large set of possible predictors.</p></li>
<li><p><em>What is the relationship between the response and each predictor?</em></p>
<p>Some predictors have positive some negative association with <span class="math inline">\(Y\)</span>. Depending on the complexity of <span class="math inline">\(f\)</span>, the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_i\)</span> may also depend on the values of other predictors(<span class="math inline">\(X_j\)</span>) =&gt; <em>synergy</em></p></li>
<li><p><em>Can the relationship between</em> <span class="math inline">\(Y\)</span> <em>and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</em></p></li>
</ul>
<p>Sometimes we are interested with prediction: Identifying individuals who will respond positively to a mailing, based on observations of demographic variables. Here we are not interested with understanding the relationship of demographic variables and response, we just want an accurate model to predict the response using the predictors. This is prediction.</p>
<p>But often we are interested to answer questions like: <em>Which media contribute to sales?</em>, <em>Which media generate the biggest boost in sales?</em>, or <em>How much increase in sales is associated with a given increase in</em> <code>TV</code> <em>advertising?</em>. This is inference.</p>
<p>And sometimes we want a combination of both: <em>Values of homes based on crime rate, zoning, distance from a river, air quality, schools, size of houses etc.</em> and <em>How does air quality effect valeus of homes?</em>.</p>
<p>We use different models for prediction, inference, or combination of the two.</p>
<section id="how-do-we-estimate-f" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="how-do-we-estimate-f"><span class="header-section-number">1.1.1</span> How Do We Estimate <span class="math inline">\(f\)</span>?</h3>
<p>There are many linear and non-linaer approaches we will discuss. But generally these models share certain characteristics. Here are they:</p>
<ul>
<li>We will always assume that we have observed a set of <em>n</em> different data points. These data points, observations, are called <em>training data</em>; which we will use these observations to train, or teach, our model on how to estimate <span class="math inline">\(f\)</span>. Our training data will consist of <span class="math inline">\(\{(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)\}\)</span>, where <span class="math inline">\(x_i = (x_{i1}, x_{i2}, \dots, x_{ip})^T\)</span></li>
</ul>
<p>We want to apply a statistical learning method to the training data to estimate the unknown function <span class="math inline">\(f\)</span>. We want to find a function <span class="math inline">\(\hat{f}\)</span> such that <span class="math inline">\(Y \approx \hat{f}(X)\)</span> for any obsrvation <span class="math inline">\((X,Y)\)</span>.</p>
<p>These statistical learning methods can be charactarized as either <em>parametric</em> or <em>non-parametric</em>.</p>
<p><strong>Parametric Methods</strong></p>
<p>Parametric methods involve a two step model-based approach:</p>
<ol type="1">
<li><em>Select a model =&gt; Make an assumption about the functional form of</em> <span class="math inline">\(f\)</span>: is it linear, non linear?</li>
</ol>
<p>For example a linear <span class="math inline">\(f\)</span> assumption would yield a <em>linear model</em></p>
<p><span class="math display">\[
  f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
  \]</span> (2.4)</p>
<ol start="2" type="1">
<li><em><em>Fit</em> or <em>train</em> the model</em></li>
</ol>
<p>After we select a model, we need a procedure that uses training data to <em>fit</em> or <em>train</em> the model.</p>
<p>For linear model, we need to estimate the parameters of the model (<span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>). That is we want to find values of these parameters such that <span class="math display">\[
  Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
  \]</span> The most common approach to fitting the model (2.4) is called <em>ordinary least squares</em>. Chapter3. But there are other approaches as well.</p>
<p>This model-based approach is called <em>parametric:</em> we estimate <span class="math inline">\(f\)</span> via estimating a set of parameters.</p>
<p>Disadvantage (potential): model we choose will usually not match the true unknown form of <span class="math inline">\(f\)</span> =&gt; our estimates will be poor. =&gt; solution: choose a <em>flexible</em> model that can fit different possible functional forms for <span class="math inline">\(f\)</span> =&gt; you will need to estimate more parametrs =&gt; <em>overfitting the data</em>.</p>
<p><strong>Non-parametric Methods</strong></p>
<p>=&gt; No explicit assumptions about the functional form of <span class="math inline">\(f\)</span>. The goal is to get an estimate of <span class="math inline">\(f\)</span> that gets as close to the data points as possible without being too rough or wiggly =&gt; advantage over parametric approach: no assumption about the functional form of <span class="math inline">\(f\)</span>–potentially accurately fit a wider range of possible shapes for <span class="math inline">\(f\)</span>.</p>
<p>Disadvantage =&gt; lots of parameters to estimate =&gt; very large of observations required to obtain an accurate estimate for <span class="math inline">\(f\)</span>.</p>
</section>
<section id="the-trade-off-between-prediction-accuracy-and-model-interpretability" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="the-trade-off-between-prediction-accuracy-and-model-interpretability"><span class="header-section-number">1.1.2</span> The Trade-Off Between Prediction Accuracy and Model Interpretability</h3>
<p>Some models are flexible some restrictive; in the sense that they can produce just a small range of functional forms to estimate <span class="math inline">\(f\)</span>. Linear regression for instance is a relatively inflexible approach. Other metgods such as thin plate splines (non-parametric) are more flexible because they can generate a much wider range of possible functional forms to estimate <span class="math inline">\(f\)</span>.</p>
<p><em>Why would be ever choose to use a more restrictive method instead of a very flexible approach?</em> :</p>
<ul>
<li><p>If we are mainly interested in inference, restrictive models are more interpretable. They give more information about each predictors effect on predicted.</p></li>
<li><p>If we are mainly interested in prediction, flexible models give better fit. =&gt; but may yield less accurate fits due to <em>overfitting!</em></p></li>
</ul>
</section>
<section id="supervised-vs-unsupervised-learning" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="supervised-vs-unsupervised-learning"><span class="header-section-number">1.1.3</span> Supervised vs Unsupervised Learning</h3>
<p>Most statistical learning problems fall into these two categories: <em>supervised</em> or <em>unsupervised</em>.</p>
<p>In supervised learning for each observation of the predictor values <span class="math inline">\(x_i, i = 1,\dots, n\)</span> there is an associated response value <span class="math inline">\(y_i\)</span>. We wish to fit a model that relates the response to the predictors with the aims of either accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Linear regression, GAM, boosting, support vector machines operate in the supervised learning domain.</p>
<p>Unsupervised leraning describes a situtaion in which for every observation <span class="math inline">\(i=1,\dots,n\)</span> we obser a vector of values <span class="math inline">\(x_i\)</span> but no associated response <span class="math inline">\(y_i\)</span>. We cannot use a linear regression model since we dont have <span class="math inline">\(y_i\)</span> values. Here we can seek to understand the relationships between the variables or between the observations; like <em>cluster analysis</em>, or clustering: to assert on the basis of <span class="math inline">\(x_i,\dots,x_n\)</span> whether the observations fall into relatively distinct groups.</p>
</section>
</section>
<section id="regression-vs-classisfication-problems" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="regression-vs-classisfication-problems"><span class="header-section-number">1.2</span> Regression vs Classisfication Problems</h2>
<p>Variables can be characterized as either <em>quantitative</em> or <em>qualitative</em>(also known as <em>categorical</em>). Quantitative varaibles take on numberical values: a person’s age, height, or income, the value of a house, price of stock. Qualitative varaibles take on values n one of <span class="math inline">\(K\)</span> different <em>classes</em>, or categories: aperson’s gender(male or female), the brand of a good (A,B, or C), a person’s race etc.</p>
<p>We refer to problems with a quantitative response as <em>regression</em> problems, and probmes with a qualitative response as <em>classification</em> problems. However, the distinction is not clear-cut.</p>
<p>Least squares regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative response. Some statistical methods, such as <em>K</em>-nearest negihbors and boosting, can be used in the case of either quantitative or qualitative.</p>
<p>We usually select statistical learning methods based on whether the response is quantitative or qualitative: we might use linear regression wjen quantitative and logistic regression when qualitative. But whether the <em>predictors</em> are qualitative or quantitative is usually not that important. Most of the statistical learning methods can be applied regardless of the predictor varible type.</p>
</section>
<section id="assessing-model-accuracy" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="assessing-model-accuracy"><span class="header-section-number">1.3</span> Assessing Model Accuracy</h2>
<p>There is no one method that dominates all others over all possible data sets. On a particular data set, one metghod may work best, but some other method may work better on a similar but differet data set. So it is important to assess the model accuracies of the methods.</p>
<p>Here some ways to asses the model accuracy</p>
<section id="measuring-the-quality-of-fit" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="measuring-the-quality-of-fit"><span class="header-section-number">1.3.1</span> Measuring the Quality of Fit</h3>
<p>So, to evaluate the performance of a statistical learning method on a given data set, we need to measure how well its predictions actually match the observed data.</p>
<p>In the regression setting, the most commonly-used measure is the <em>mean squared error (MSE)</em>, given by</p>
<p><span class="math display">\[
\text{MSE} = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(x_i))^2
\]</span></p>
<p>MSE will be small if the predicted responses are veryt close to the true responses, and large if predicted and true responses differ substantially <strong>on average</strong>.</p>
<p>Here since MSE is computed using the training data it is best to refer it as <strong>training MSE</strong>. But in general, we do not really care how well the method works on the training data =&gt; <em>we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data</em>.</p>
<p>Imagine: stock price prediction =&gt; we have training and test data =&gt; we already know the stock prices of the past, we dont care about the training data accuracy of the model, we want our model to predict the future prices of stocks best.</p>
<p>Or we have blood characteristics of diabetes patients. We don’t want our model to explain our existing patient’s classification of diabetes or not, we want our model to predict our future patien’s situation the best.</p>
<p>Mathematically:</p>
<p>We fit our statistical learning method on our training observations <span class="math inline">\(\{(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)\}\)</span>, and we obtain the estimate <span class="math inline">\(\hat{f}\)</span>. We can then compute <span class="math inline">\(\hat{f}(x_1),\dots, \hat{f}(x_n)\)</span>. If these are approximately equal to <span class="math inline">\(y_1, \dots, y_n\)</span> then our training MSE will be small. Howeer, we are not interested in whether <span class="math inline">\(y_i \approx \hat{f}(x_i)\)</span>, we want to know whether <span class="math inline">\(\hat{f}(x_0)\)</span> is approximately equal to <span class="math inline">\(y_0\)</span>, where <span class="math inline">\((x_0,y_0)\)</span> is a <em>previously unseen test obsrevation not used to train the statistical learning method</em>.</p>
<p>That is, we want to choose the method that gives the lowest <em>test MSE</em>!.</p>
<p>So with our <em>test data</em> we can compute <em>test MSE</em></p>
<p><span class="math display">\[
\text{MSE}_{test} =\frac{1}{n_{test}} \sum(y_{test_{i}} - \hat{f}(X_{test_i}))
\]</span> (2.6)</p>
<p>We want the test MSE to be small as possible. We can compute test MSE via (2.6) if we have test data for different models and select the model with minimum test MSE.</p>
<p>If we don’t have a test data, you might think our goal would be to minimize the training MSE since test and training data are colesly related. But no; minimal training MSE doesn’t guarantee minimal training MSE</p>
<p>Usually as the level of flexibility increases, the curves fit the observed data more closely =&gt; lower training MSE. The level of flexibility is quantified by <em>degrees of freedom</em>. More restricted models have lower degrees of freedom. and usually the training MSE declines as flexibility increases.</p>
<p><img src="fig2.9.png" class="img-fluid"></p>
<p>As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a <span class="math inline">\(U\)</span>-<em>shape</em> in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. AS model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be <em>overfitting</em> the data. This happens because our statistical learninig procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random change rather than by true properties of the unknown function <span class="math inline">\(f\)</span>. When we overfit the trainin data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.</p>
<p>Note that regardless of whether or not overfitting has occured, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or inderectly seek to mimizie the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.</p>
<p>In practice, training MSE is computed easily, but estimating test MSE is hard because usually no test data are available. We will learn approaches that can be used in practice to estimate the mininmum test MSE. One important method is <em>cross-validation</em>( Chapter 5), which is a method for estimating test MSE using the training data.</p>
</section>
<section id="the-bias-variance-trade-off" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="the-bias-variance-trade-off"><span class="header-section-number">1.3.2</span> The Bias-Variance Trade-Off</h3>
<p>The U-shape in the test MSE result of two competing properties of statistical learnig methods. The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into sum of <em>variance</em> of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared <em>bias</em> of <span class="math inline">\(\hat{f}(x_0)\)</span> and the variance of the error terms <span class="math inline">\(\epsilon\)</span>. That is</p>
<p>This means that to minimize the expected test error, we need to simultaneously have <em>low variance</em> and <em>low bias</em>. Since variance is always bigger than zero; <span class="math inline">\(\text{Var}(\epsilon)\)</span>, and <span class="math inline">\(\text{Bias}(\hat{f}(x_0))\)</span> are nonnegative. So, the expected test MSE can never lie below <span class="math inline">\(\text{Var}(\epsilon)\)</span>, the irreducible error from (2.3).</p>
<p>What do we mean by the <em>variance</em> and <em>bias</em> of statistical learning method?</p>
<p><em>Varince</em> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set; different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally, <span class="math inline">\(\hat{f}\)</span> should not vary too much between training sets. If a method has high variance small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span>.</p>
<p>Flexible methods have hiher variance, because they fit better to the data points and changing any of the data points may cause the estiamte <span class="math inline">\(\hat{f}\)</span> to chance considerably. But for example, least squares method is relatively inflexible and has low variance, because mooving any single observation will cause only a small shift in the position of the line.(2.9)</p>
<p><em>bias</em> refers to the error that is due to functional form of <span class="math inline">\(\hat{f}\)</span>. In real life, linear relationships are very rare. So performing linear regression will result in some bias in the estimate of <span class="math inline">\(f\)</span>. If your <span class="math inline">\(f\)</span> is non-linear performing linear regression on different data sets will not produce an accurate estimate; so linear regression will result in high bias.</p>
<p>For example in 2.9 true <span class="math inline">\(f\)</span> is non linear; so linear regression have high bias, low variance. <img src="fig2.10.png" class="img-fluid"></p>
<p>In 2.10 true <span class="math inline">\(f\)</span> is very close to linear, so linaer regression have low bias, low variance.</p>
<p>Generally more flexible methods result in less bias.</p>
<p>As a general rule, as we use more flexible methods the variance will increase and bias will decrease. The relative rate of change of these two quantities determines whether test MSE increases or decreases. As we increase the flexibility, the bias tends to initially decrease faster than the variance increases =&gt; test MSE declines. However, at some point increasing flexibility has little impact on thebias but starts to significantly increase the variance =&gt; test MSE increases.</p>
<p><img src="fig2.12.png" class="img-fluid"></p>
<p>Figure 2.12 shows bias and variance effect to the test MSE for different <span class="math inline">\(f\)</span>s. Horizontal dashed line represents <span class="math inline">\(\text{Var}(\epsilon)\)</span>, the irreducible error; the red curve test MSE is the sum of squared bias, variance, and variance of irreducible error. In all cases bias decreases as flexibility increaes. However, the optimal flexibility is different for each <span class="math inline">\(f\)</span>. In the left panel the bias initially decreaes rapidly, decreasing test MSE. In center panel true <span class="math inline">\(f\)</span> is closer to linear so there is only a small decrease in bias as flexibility increaes, and the test MSE only declines slightly before increasing rapidly as the variance increaes. Right hand panel, as flexibility increaess bias dramatically decreases because true <span class="math inline">\(f\)</span> is very non-linear. There is alsso very little increase in variance as flexibility increases =&gt; tets MSE decreases before increasing.</p>
<p>This is called bias-variance trade off. Good test set performance of a method requires low variance as well as low squared bias. This is a trade off because it is easy to obtain a method with extremely low bias but high variance(for instance, drawing a curve that passes through every single training observation, or a method with low variance but high bias(by fitting a horizontal line to the data). Challange is finding a method wihch both the variance and squareed bias are low.</p>
<p>In real life, it is not possible to explicitly compute the test MSE, bias, or variance for methods. But we should keep this in mind.</p>
</section>
<section id="the-classification-setting" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="the-classification-setting"><span class="header-section-number">1.3.3</span> The Classification Setting</h3>
<p>So far we focused on regression setting. Problems such as bias-variance trade of also occurs in classification but in a modificated way because <span class="math inline">\(y_i\)</span> is no longer numerical.</p>
<p>Suppose that we seek to estiamte <span class="math inline">\(f\)</span> on the basis of training observations <span class="math inline">\(\{(x_1,y_1), \dots, (x_n,y_n)\}\)</span> where now <span class="math inline">\(y_1,\dots, y_n\)</span> are qualitative.</p>
<p>We need to quantify the accuracy of our estimate <span class="math inline">\(\hat{f}\)</span>. We can use the training <em>error rate</em>, the proportion of mistakes that are made if we apply our estimate <span class="math inline">\(\hat{f}\)</span> to the training observations:</p>
<p><span class="math display">\[
\text{error rate}=\frac{1}{n}\sum^n_{i=1}I(y_i \neq \hat{y_i})
\]</span> (2.8)</p>
<p><span class="math inline">\(\hat{y_i}\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat{f}\)</span>. <span class="math inline">\(I(y_i \neq \hat{y_i})\)</span> is an <em>indicator variable</em> that equals 1 if <span class="math inline">\(y_i \neq \hat{y_i}\)</span> and zero if <span class="math inline">\({y_i = \hat{y_i}}\)</span>. If <span class="math inline">\(I(y_i \neq \hat{y_i}) = 0\)</span> then <span class="math inline">\(i\)</span>th observation was classified correctly, otherwise it was misclassified. So (2.8) computes the fraction of incorrect classifications.</p>
<p>(2.8) is <em>training error</em>. But as the regression setting we are more interested in <em>test error</em> rate. The <em>test error</em> rate associated with a set of test observations of the form</p>
<p><span class="math display">\[
\text{error rate}_{test} = \frac{1}{n_{test}}\sum_{i=1}^{n_{test}}(I(y_{test_i} \neq \hat{y}_{test_i}))
\]</span> (2.9)</p>
<p>A <em>good</em> classifier is one for which the test error is smallest.</p>
<p><strong>The Bayes Classifier</strong></p>
<p>We can minimize test error rate by a very simple classifier that <em>assigns each observation to the most likely class, given its predictor values</em>. In other words, we should simply assign a test observation with predictor vector <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> for which</p>
<p><span class="math display">\[
Pr(Y = j | X = x_0)
\]</span> (2.10)</p>
<p>is largest. This is <em>conditional probability:</em> it is the probabilty that <span class="math inline">\(Y=j\)</span> given the observed predictor vector <span class="math inline">\(x_0\)</span>. This classifier is called <em>Bayes classifier</em>.</p>
<p>In a two-class problem where there are only two possible response values, <em>class 1</em> or <em>class2</em>, the Bayes classifier corresponds to predicting class one if <span class="math inline">\(Pr(Y=1 | X = x_0) &gt; 0.5\)</span>, and class two otherwise.</p>
<p>Imagine having <span class="math inline">\(X = (X_1, X_2)\)</span>. For each value of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> there will be a different probability of the response being class 1 or 2. For <span class="math inline">\(Pr(Y=class1 | X = (X_2,X_2)) &gt; 0.5\)</span> and <span class="math inline">\(Pr(Y=class2 | X = (X_1,X_2))\)</span>.</p>
<p>The Bayes classifier produces the lowest possible test error rate, called the <em>Bayes error rate</em>. Since the Bayes classifer will always choose the class for which <span class="math inline">\(Pr(Y = j | X = x_0)\)</span> is largest, the error rate at <span class="math inline">\(X=x_0\)</span> will be <span class="math inline">\(1-\max_jPr(Y =j | X = x_0)\)</span>. In general, the overall Bayes error rate is given by</p>
<p><span class="math display">\[
\text{Bayes error rate} = 1 - E(\max_jPr(Y =j | X))
\]</span> (2.11)</p>
<p>where the expectation averages the probabilty over all possible values of X. The Bayes eror rate is analogous to the irreducible eror.</p>
<p><strong>K-Nearest Neighbors</strong></p>
<p>In theory we always want to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and so computing the Bayes classfier is impossible. So Bayes classifier is like a gold standard to compare other methods.</p>
<p>Many approaches attemp to estiamte the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and then classify a given observation to the class with highest <em>estimated</em> probability. One of them is <em>K-nearest neighbors</em>(KNN) classfier.</p>
<p>Given a positive integer <em>K</em> and a test observation <span class="math inline">\(x_0\)</span>, the KNN clasffier first identifies the <em>K</em> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>. It then esitmates the conditional probability for class <em>j</em> as the fraction points in <span class="math inline">\(N_0\)</span> whose response values equal to <span class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[
Pr(Y = j | X = x_0) = \frac{1}{K}\sum_{i \in N_0}I(y_i=j)
\]</span> (2.12)</p>
<p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p>
<p><img src="fig2.14.png" class="img-fluid"></p>
<p>Figure 2.14 provides an illustrative example of the KNN approach. Left panel =&gt; Our goal is to make prediction for the black cross point. When <span class="math inline">\(K=3\)</span> KNN will identify the 3 oservations that are closest to the cross. There are two blue and one orange points; <span class="math inline">\(Pr(Y=orange | X = x_{cross}) = 1/3\)</span>, and <span class="math inline">\(Pr(Y=blue | X = x_{cross}) = 2/3\)</span> =&gt; KNN will predict that the black cross belongs to the blue class.</p>
<p>The choice of K is very important. as K increases flexibility decreases =&gt; high bias, but low variance.</p>
<p>Just like in regression setting there is not a strong relationship between the training error rate and the test error rate.</p>
<p><img src="fig2.17.png" class="img-fluid"></p>
<p>as in the regression setting, the training error rate consistently declines as the flexibility(<span class="math inline">\(1/K\)</span>) increases. However, the test error rate again have a characteristic U-shape.</p>
<p>In both regression and classification settings, choosing the correct level of flexibility is critical. The bias-variance tradeoff =&gt; U-shape in the test error, can make this a difficult task.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Chapter3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>