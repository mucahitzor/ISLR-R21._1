[
  {
    "objectID": "Chapter4.html#an-overview-of-classsification",
    "href": "Chapter4.html#an-overview-of-classsification",
    "title": "3  Classification",
    "section": "3.1 An overview of Classsification",
    "text": "3.1 An overview of Classsification\nHere some clasffication problems\n\nA person arrives at the emergency room with a set of symptoms that could possible be attributed to one f three medical conditions. Which of the three conditions does the individual have?\nAn online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP adress, past transaction history and so forth.\nIn the bais of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease causing and which are not.\n\nJust like in LR , in the classification setting we have a set of training observations \\((x_1,y_1), \\dots, (x_n,y_n)\\) what we can use to build a classfier. We want our classifier to perform not only on th training data, but also on test observations that are not used to train the classier.\n\nWe are going to use Default data set.\n\n\ndefault = read_csv(\"./data/Default.csv\")\n\nRows: 10000 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): default, student\ndbl (2): balance, income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefault %&lt;&gt;% \n  mutate_if(is.character, ~as.factor(.)) %&gt;% \n  print()\n\n# A tibble: 10,000 × 4\n   default student balance income\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 No      No         730. 44362.\n 2 No      Yes        817. 12106.\n 3 No      No        1074. 31767.\n 4 No      No         529. 35704.\n 5 No      No         786. 38463.\n 6 No      Yes        920.  7492.\n 7 No      No         826. 24905.\n 8 No      Yes        809. 17600.\n 9 No      No        1161. 37469.\n10 No      No           0  29275.\n# ℹ 9,990 more rows\n\n\nWe are interested in predicting whether an individual will default on his or her credit card balance.\n\nskimr::skim(default)\n\n\nData summary\n\n\nName\ndefault\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndefault\n0\n1\nFALSE\n2\nNo: 9667, Yes: 333\n\n\nstudent\n0\n1\nFALSE\n2\nNo: 7056, Yes: 2944\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbalance\n0\n1\n835.37\n483.71\n0.00\n481.73\n823.64\n1166.31\n2654.32\n▆▇▅▁▁\n\n\nincome\n0\n1\n33516.98\n13336.64\n771.97\n21340.46\n34552.64\n43807.73\n73554.23\n▂▇▇▅▁\n\n\n\n\n\n\nGGally::ggpairs(mapping = aes(color = default), data = default)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\ndefault %&gt;% \n  ggplot() + aes(x = balance, y = income, color = default, shape = default) + geom_point() + scale_color_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) + scale_shape_manual(values = c(1,3)) -&gt; p1\n\ndefault %&gt;% \n  ggplot() + aes(x = default, y = balance, fill = default) + geom_boxplot() + \n  scale_fill_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) -&gt; p2\n\ndefault %&gt;% \n  ggplot() + aes(x = default, y = income, fill = default) + geom_boxplot() + \n  scale_fill_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) -&gt; p3\n\n# gridExtra::grid.arrange(p1,p2,p3, nrow=1)\ngridExtra::grid.arrange(p1,p2,p3, nrow=3)\n\n\n\n# Top: The aanual incomes and montly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: boxplots of balances as a function of default status. Bottom: boxplots of income as a functino of default status.\n\npeople who default tend to have high credit card balances compared to not defaulted.\n\ndefault %&gt;% \n  count(default) %&gt;% \n  mutate(port = n/sum(n))\n\n# A tibble: 2 × 3\n  default     n   port\n  &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 No       9667 0.967 \n2 Yes       333 0.0333\n\n# default rate is 3%\n\n\ndefault %&gt;% \n  group_by(student,default) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(port = count/sum(count))\n\n`summarise()` has grouped output by 'student'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   student [2]\n  student default count   port\n  &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 No      No       6850 0.971 \n2 No      Yes       206 0.0292\n3 Yes     No       2817 0.957 \n4 Yes     Yes       127 0.0431\n\n# student are 2 times more likely to default\n\nIn this chapter we laern how to build a model to predict default(\\(y\\)), for any given value of balance (\\(x_1\\)), and income (\\(x_2\\)). Since \\(Y\\) is not quantitative, SLR is not appropriate."
  },
  {
    "objectID": "Chapter4.html#why-not-linear-regression",
    "href": "Chapter4.html#why-not-linear-regression",
    "title": "3  Classification",
    "section": "3.2 Why Not Linear Regression?",
    "text": "3.2 Why Not Linear Regression?\nWhy is LR not appropriate here?\nSuppose that we are trying to predict the mdeical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, therea rea three possible diagnoses: stroke, drug overdose, epileptic seizure. We could consider encoding these values as a quantitative respose variable \\(Y\\):\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke}; \\\\\n2 & \\text{if drug overdose}; \\\\\n3 & \\text{if epileptic seizure}.\n\\end{cases}\n\\]\nWe can now predict \\(Y\\) using \\(x_1, \\dots, \\x_p\\). However, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure. In practice there is no particular reason that this needs to be the case. We could have ordered the cases differently, i.e. stroke to the 3 etc. Which implies a totally different relationship among the three conditions. All these combinations would produce different linear models that would lead to different set of predictions on test observations.\nHowever, if the response variable’s values take on a natural ordering, such as mild, moderate, and severe, and we felt the gap between mild and moderate was similar to the gap between moderate and sever, then a 1,2,3 coding would be reasonable. **Unfortunaltely, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantittative response that is ready for linear regression*.\nFor a binary(two level) qualitative response, the situation is easier. For instance consider only two possiblities for \\(Y\\):\n\\[\nY =\n\\begin{cases}\n0 & \\text{if stroke} \\\\\n1 & \\text{if drug overdose}\n\\end{cases}\n\\]\nwe can create a dummy varaible and fit a linar regression to this binary response and predict drug overdose if \\(\\hat{y}&gt;0.5\\) and stroke otherwise. However, if we use a linear regression our estimates might be outside of \\([0,1]\\), aking them hard to interpret as probabilities.\nThe dummy varaible approach coonot be easliy extended to accommodate qualitative responses with more than two levels. We prefer classification methods:\n## Logistic Regression\nOur default variable can have two values: Yes or No. Rather than modeling this response \\(Y\\) directly, logistic regression models the probability that \\(Y\\) belongs to a particular category.\nFor the default data logistic regression models the probability of default. For example, the probability of default given balance can be written as\n\\[\nPr(default = Yes | balance)\n\\] The values of \\(Pr(default = Yes|balance)\\), which we abbreviate \\(p(balance)\\), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, we can predict \\(default = Yes\\) or any individaul for whom \\(p(balance)&gt;0.5\\). Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(balance)&gt;0.1\\).\n\n3.2.1 The Logistic Model\nHow should we model the relationship between \\(p(x) = Pr(y = 1| x)\\) and \\(x\\)? (when we coded 0/1 for response values)\nIn section 4.2 we talked of using a linear regression model to represent these probabilities:\n\\[\np(x) = \\beta_0 + \\beta_1 x\n\\] (4.1)\n\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  print() %&gt;% \n  lm(default~balance,.) %&gt;% \n  summary()\n\n# A tibble: 10,000 × 4\n   default student balance income\n     &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1       0 No         730. 44362.\n 2       0 Yes        817. 12106.\n 3       0 No        1074. 31767.\n 4       0 No         529. 35704.\n 5       0 No         786. 38463.\n 6       0 Yes        920.  7492.\n 7       0 No         826. 24905.\n 8       0 Yes        809. 17600.\n 9       0 No        1161. 37469.\n10       0 No           0  29275.\n# ℹ 9,990 more rows\n\n\n\nCall:\nlm(formula = default ~ balance, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23533 -0.06939 -0.02628  0.02004  0.99046 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.519e-02  3.354e-03  -22.42   &lt;2e-16 ***\nbalance      1.299e-04  3.475e-06   37.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1681 on 9998 degrees of freedom\nMultiple R-squared:  0.1226,    Adjusted R-squared:  0.1225 \nF-statistic:  1397 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  lm(default~balance,.) -&gt; lm_res\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  ggplot() + aes(x = balance, y = default) + geom_point(shape = 4, color = \"brown4\") + \n  geom_abline(intercept = lm_res$coefficients[1], slope = lm_res$coefficients[2], color = \"blue\") + scale_y_continuous(expand = c(0,0.25), breaks = seq(-0.2,1,by=0.2) )\n\n\n\n\nThe plot above is the same as plot on the left pane of 4.2. Here we see the problem with this approach, for balances close to zero, we predict a negative probability of default and if we predict for very large balances we would get values bigger than 1. These predictions are not sensible. This always happen for any time a straight line is fit to a binary response that is coded as 0 or 1, in pricpile we can always predict \\(p(x)&lt;0\\) for some values of \\(x\\) and \\(p(x)&gt;1\\) for others.\nTo avoid this problem we must model \\(p(x)\\) using a function that gives outputs between 0 and 1 for all values of \\(x\\). Many functions meet this description, however in logistis regression, we use logistic function\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\n\\] (4.2)\nTo fit the mode (4.2) we use a method called maximum likelihood, which we discuss in the next section. The right-hand panel of Figure 4.2 sows then fit of the logistic regression model to the defaultdata.\nNotice that for low balances we now predict the probability of default as close to , but never below zero. Likewise for high balances we predict a default probability close to, but never above, one.\nThe logistic function will always produce an S-shaped curve of this form, and so regardless of the value of \\(X\\) we will obtain a sensible prediction.\nAfter a bit of manipulation of (4.2) we find that\n$$ \\[\\begin{align}\np(x) &= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\\n\n1-p(x) &= 1- \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\\n\n1- p(x) &= \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x}}  \\\\\n\n1- p(x) \\cdot e^{\\beta_0 + \\beta_1x} &= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\\\\n\n1-p(x) \\cdot e^{\\beta_0 + \\beta_1 x} &= p(x) \\\\\n\n\\frac{p(x)}{1-p(x)} &= e^{\\beta_0 + \\beta_1} \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space (4.3)\n\n\\end{align}\\] $$\nthe quantity \\(p(x)/(1 - p(x))\\) is called the odds, and can take on any value between 0 and \\(\\infty\\). Values of the odds close to 0 and \\(\\infty\\) indicate very low and very high probabilities of default, respectively.\nFor example, on average 1 in 4 people with an odds of 1/4 will default since \\(p(x) = 0.2\\) implies an ods of \\(\\frac{0.2}{1-0.2} = 1/4\\). Likewise on average 9/10 wpeople with an odds of 9 will default since \\(p(x) = 0.9\\) implies an odds of \\(\\frac{0.9}{1-0.9} = 9\\).\nBy taking the lograithm of both sides of (4.3) we arrive at\n\\[\n\\log(\\frac{p(x)}{1-p(x)}) = \\beta_0 + \\beta_1 x\n\\] (4.4)\nthe left-hand side is called the log-odds- or logit. We see that the logistic regression model (4.2) has a logit that is linear in \\(X\\).\nHere increasing \\(x\\) by one unit changes the log odds by \\(\\beta_1\\) (4.4), or it multiplies the odds by \\(e^{\\beta_1}\\). But because the relationship between \\(p(x)\\) and \\(x\\) in (4.2) is not a straight line, \\(\\beta_1\\) does not correspond to the change in \\(p(x)\\) associated with a one-unit increase in \\(x\\). The amount that \\(p(x)\\) changes due to a one-unit change in \\(x\\) will depend on the current value of \\(x\\). But regardless of the value of \\(x\\), if \\(\\beta_1\\) is positive, then increasing \\(x\\) will be associated with increasing \\(p(x)\\), vice versa. We can see that in the right hand panel of Figure 4.2.\n\n\n3.2.2 Estiamting the Regression Coefficients\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1}}\n\\] (4.2) The coefficients of \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, must be estimated based on the available training data. We are going to use maximum likelyhood method. The basic intuition begind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\)} of default for each individual, using (4.2), corresponds as closely as possible to the individual’s observed default status. In other words, we try to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) such that plugging these estimates into the model for \\(p(x)\\),given in (4.2), yields a number close to 1 for all individuals who defaulted, and a number close to zero for all individuals who did not. We can formalize this mathematical equation with likelihood function\n\\[\nl(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1}p(x_i) \\prod_{i':y_{i'} = 0}(1-p(x_{i'}))\n\\] (4.5)\nThe estimates \\(\\hat{\\beta_0\\) and \\(\\hat{\\beta_1}\\) are chosen to maximize this likelihood function.\n\n# initialize model\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\nlog_model\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n# setup the recipe \ndefault_recipe &lt;- recipe(default ~ balance, data = default)\ndefault_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n# set up the workflow\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(default_recipe) -&gt; default_workflow\ndefault_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n## fit model\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;%\n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term         estimate std.error statistic   p.value  conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   \n2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594\n\n# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.\n\n\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  ggplot() + aes(x = balance, y = default, color = .pred_class) + geom_point() + scale_color_ipsum()\n\n\n\n\nLets have a look at the Logistic Regression results again:\n\n## fit model\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;%\n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term         estimate std.error statistic   p.value  conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   \n2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594\n\n# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.\n\nWe can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in the above plays the same role as t statistic in linear regression output. They are calculated from \\(\\hat{\\beta_i} / \\text{SE}(\\hat{\\beta_i})\\), and so a large(absolute) value of the z-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1 = 0\\). This null hypothesis implies that \\(p(x) = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\). In other words, that the probability of default does not depend on blaance. p value is very low, we can reject \\(H_0\\); there is relationship between balance and probability of default. Intercept is not important here.\n\n\n3.2.3 Making predictions\nOnce the coefficients have been estimated, we can compute the probability of default for any given credit card balance.\n\\[\n\\hat{p}(x) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1}x}}{1 +e^{\\hat{\\beta_0} + \\hat{\\beta_1}x}} = \\frac{e^{-10.6513 + 0.0055 x}}{1 + e^{-10.6513 + 0.0055 x}}\n\\]\nso for example given income $1,000 the predicted possiblity of default is\n\nexp(-10.6513 + 0.0055 * 1000) / (1 + exp(-10.6513 + 0.0055 * 1000))\n\n[1] 0.005758518\n\n\nwhich is below 1%. What about the probability of default for a person with $2,000 balance\n\nexp(-10.6513 + 0.0055 * 2000) / (1 + exp(-10.6513 + 0.0055 * 2000))\n\n[1] 0.5863023\n\n\nmuch higher 58%.\nWe can get all the predictions for our training data set form\n\ndefault_workflow %&gt;% \n  fit(data=default) %&gt;% \n  augment(new_data = default) %&gt;% print() %&gt;% \n  count(.pred_class)\n\n# A tibble: 10,000 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 No      No         730. 44362. No             0.999 0.00131  \n 2 No      Yes        817. 12106. No             0.998 0.00211  \n 3 No      No        1074. 31767. No             0.991 0.00859  \n 4 No      No         529. 35704. No             1.00  0.000434 \n 5 No      No         786. 38463. No             0.998 0.00178  \n 6 No      Yes        920.  7492. No             0.996 0.00370  \n 7 No      No         826. 24905. No             0.998 0.00221  \n 8 No      Yes        809. 17600. No             0.998 0.00202  \n 9 No      No        1161. 37469. No             0.986 0.0138   \n10 No      No           0  29275. No             1.00  0.0000237\n# ℹ 9,990 more rows\n\n\n# A tibble: 2 × 2\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 No           9858\n2 Yes           142\n\n# we classified 142 cases to default \n\n# originally \ndefault %&gt;% \n  count(default)\n\n# A tibble: 2 × 2\n  default     n\n  &lt;fct&gt;   &lt;int&gt;\n1 No       9667\n2 Yes       333\n\n# 333 cases are actually default\n\nWe can implement qualitative predictors to the logistic regression using the dummy variable approach.. Lets predict default by only student variable\n\\[\nx =\n\\begin{cases}\n1 & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\n\ndefault_recipe &lt;- recipe(default ~ student, data = default)\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(default_recipe) -&gt; default_workflow\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -3.50     0.0707    -49.6  0          -3.65     -3.37 \n2 studentYes     0.405    0.115       3.52 0.000431    0.177     0.629\n\n\nThe coefficient is positive and the p value is statistically significant. This indicates that students tend to have higher default probabilities than non student:\n\\[\n\\widehat{Pr}(default = Yes | student = Yes) = \\frac{e^{-3.50 + 0.4049 \\times 1}}{1 + e^{-3.50 + 0.4049 \\times 1}} = 0.0431\n\\]\n\\[\n\\widehat{Pr}(default = Yes | student = No) = \\frac{e^{-3.50 + 0.4049 \\times 0}}{1 + e^{-3.50 + 0.4049 \\times 0}} = 0.0292\n\\]\n\n\n3.2.4 Multiple Logistic Regression\nWe now consider the problem of predicting a binary response using a multiple predictors. We can generalize (4.4) as follows:\n\\[\n\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p\n\\] (4.6)\n(4.6) can be written as\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p}}\n\\] (4.7)\nWe again use maximum likelihood method to estimate \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\).\nLets use all of our variables in our model\n\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\n\nrecipe &lt;- recipe(default ~ balance + income + student, data = default)\n\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(recipe) -&gt; default_workflow\n\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  tidy() \n\n# A tibble: 4 × 5\n  term            estimate  std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.9        0.492        -22.1   4.91e-108\n2 balance       0.00574    0.000232      24.7   4.22e-135\n3 income        0.00000303 0.00000820     0.370 7.12e-  1\n4 studentYes   -0.647      0.236         -2.74  6.19e-  3\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\nHere there is a suprising result. According to the p-values balance and student variables are associted with the probability of default. However, coefficient for stundet dummy is negative; indicating that students are less likely to default than non-students. But this coefficint was positive in our previous analysis when we regressed probability of default by student: results showed that probability of default is twice as likely for students compared to non students.\n\nCheck out the fig 4.3. The orange and blue lines show the average default rates for students and non students, respectively, as a function of credit card balance. The nagive coefficient for student in the multiple logistic regression indicates that for a fixed value of balance and income a student is less likely to default than a non-student. We observe from the left hand panel that the student default ratge is at or below that of the nun student default rate for every value of balance. But the horizontal broken lines near the base of the plot, which show the default rates for students and non students averaged over all values of balance and income, suggest the opposite effect: the overall student default rate is higher than non student default rate. Consequently there is a positive coefficient for student in the single variable logistic regression output.\nThe right hand panel provies an explanation for this discrepancy: the varaibles student and blaance are correlated. Students tend to hold higher levels of debt, which is in turn assocaited with higer probability of default. In other words, students are more likely to have large credit card balances, which as we know from the left hand panel of figure 4.3 tend to be assocaited with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall students tend to default at a higher rate than non students. This is an important distinction. A student is riskier than a non student if no information about the student’s credit card balance is available. However, that student is less risky than a non student with the same credit card balance.\nThis simple example illustrates the dangers and subtleties assocaited with performing regressions involving only a single predictor when other predictors may als obe relevant. The results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon seen in Figure 4.3 is known as confounding.\nLets put the estimates to our estimated probability function\n$$ (x) = \n{ 1 + e^{-10.86 + 0.00574 balance + 3e-6 income - 0.646 Student} } $$ We can make predictions: a student with a credit card balnce of $1,500 and an income of $40,000 has an estimated probability of default:\n\\[\n\\hat{p}(x)=\\frac{\ne^{-10.86 + 0.00574 \\cdot 1,500 + 3e-6 \\cdot 40,000 - 0.646 \\cdot 1}\n}\n{\ne^{-10.86 + 0.00574 \\cdot 1,500 + 3e-6 \\cdot 40,000 - 0.646 \\cdot 1}\n} = 0.05780859\n\\]\nA non student with the same balance and income has an estiamted probability of default of\n\\[\n\\hat{p}(x)=\\frac{\ne^{-10.86 + 0.00574 \\cdot 1,500 + 3e-6 \\cdot 40,000 - 0.646 \\cdot 0}\n}\n{\ne^{-10.86 + 0.00574 \\cdot 1,500 + 3e-6 \\cdot 40,000 - 0.646 \\cdot 0}\n} = 0.1048655\n\\]\n\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = tibble(income = 40000, balance = 1500, student = c(\"Yes\",\"No\")))\n\n# A tibble: 2 × 6\n  income balance student .pred_class .pred_No .pred_Yes\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1  40000    1500 Yes     No             0.942    0.0579\n2  40000    1500 No      No             0.895    0.105 \n\n\n\n\n3.2.5 Logistic Regression for &gt; 2 Response Classes\nWhat if our response variable has more than two classes. Like medical condition in the emergency room: stroke, drug overdose, epileptic seizure. In this setting we wish to model both \\(Pr(Y = stroke | x)\\) and \\(Pr(Y = drug \\space overdose | x)\\), with the remaining \\(Pr(Y = epileptic \\space seizure |x) = 1 - Pr(Y= stroke | x) - Pr(Y = drug \\space overdose | x)\\). We can do it my extending logistic regression but Linear Discriminant Analysis is much suitable for this task"
  },
  {
    "objectID": "Chapter4.html#linear-discriminant-analysis",
    "href": "Chapter4.html#linear-discriminant-analysis",
    "title": "3  Classification",
    "section": "3.3 Linear Discriminant Analysis",
    "text": "3.3 Linear Discriminant Analysis\nLogistic regression involves directly modeling \\(Pr(Y = k | X = x)\\) using the logistic function, given by (4.7) for the case of two response classes. In statistical jargon, we model the conditional distribution of the response \\(Y\\), given the predictor(s) \\(X\\).\nWe now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors \\(X\\) separately in each of the response classes (i.e. given \\(Y\\)), and then use Bayes’ theorem to flip these around into estimates for \\(Pr(Y = k | X = x)\\). When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression.\nWhy do we need another method when we have logistic regression?\n\nWhen the classes are well separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discrimnant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes the linear discriminant model is again more stable than th elogistic regression model\nLDA is popular when we have more than two response classes.\n\n\n3.3.1 Using Bayes’ Theorem for Classification\nWe want to classify an observation into one of \\(K\\) classes, where \\(2 \\leq K\\). So \\(Y\\) can take on \\(K\\) possible distinct and unordered values. Let \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k\\)th class.; this is the probability that a given observation is associated with the kth category of the response variable \\(Y\\). Let \\(f_k(x) \\equiv Pr(X = x | Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the k th class. In other words \\(f_k(x)\\) is relatively large if there is a high probability that an observation in the k th class has \\(X \\approx x\\), and \\(f_k(x)\\) is small if it is very unlikely that an observation in the kth class has \\(X \\approx x\\). Then Bayes’s theorem states that\n\\[\nPr(Y = k | X = x) = \\frac{\n\\pi_k\\,f_k(x)\n}\n{\n\\sum^K_{l = 1}\\,\\pi_l\\,f_l(x)\n}\n\\] (4.10)\nWe will use the abbreviation \\(p_k(X) = Pr(Y = k | X)\\). This suggests that instead of directly computing \\(p_k(X)\\) as Section 4.3.1, we can simply plug in estimates of \\(\\pi_k\\) and \\(f_k(X)\\) into (4.10). In general, estimating \\(\\pi_k\\) is easy if we have a random sample of \\(Y\\)s from the population: we simply compute the fraction of the training observations that belong to the kth class. However, etimating \\(f_k(X)\\) tends to be more challenging, unless we assume some simple forms for these densities. We refer to \\(p_k(x)\\) as the posterior probability than an observation \\(X=x\\) belongs to the kth class. That is, it is the probability that the observation belongs to the kth class, given the predictor value for that observation.\nWe know from Ch.2 that the Bayes classifier, which classifies an observation to the class for which \\(p_k(X)\\) is largest, has the lowest possible error rate out of all classifiers. Therefore, if we can find a way to estimate \\(f_k(X)\\) then we can develop a classifier that approximates the bayes classifier:\n\n\n3.3.2 Linear Discriminant Analysis for p=1\nAssume we have one predictor. We would like to obtain an estimate for \\(f_k(x)\\) that we can plug into (4.10) in order to estimate \\(p_k(x)\\). We will then classify an observation to the class for which \\(p_k(x)\\) is greatests. In order to estimate \\(f_k(x)\\) we will first make some assumptions about this form.\nSuppose we assume \\(f_k(x)\\) is normal or Gaussian. In the one dimensional setting, the normal density takes the form\n\\[\nf_k(x) = \\frac{\n1\n}\n{\n\\sqrt{2\\pi}\\,\\sigma_k\n}\\,exp\\,\\left(-\\frac{1}{2\\sigma^2_k}(x - \\mu_k)^2\\right)\n\\] (4.11)\nwhere \\(\\mu_k\\) and \\(\\sigma^2_k\\) are the mean and variance parameters for the kth class. For now, let us further assume that \\(\\sigma_1^2=\\dots=\\sigma^2_K\\): that is, there is shared variance term across all \\(K\\) classes, which for simplicty we can denote by \\(\\sigma^2\\). Plugging (4.11) into (4.10) we find that\n\\[\np_k(x) = \\frac{\n\\pi_k\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)\n}\n{\n\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\,exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)\n}\n\\] (4.12)\nThe bayes classifier involves assigning an observation \\(X=x\\) to the class for which (4.12) is largest. Taking the log of (4.12) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} \\,- \\frac{\\mu_k^2}{2\\sigma^2}\\,+ \\log(\\pi_k)\n\\] (4.13)\nis largest. For instance if \\(K = 2\\) and \\(\\pi_1 = \\pi_2\\), then the Bayes clasffier assigns an obsetvation to classs 1 if \\(2x(\\mu_1 - \\mu_2) &gt; \\mu_1^2 - \\mu_2^2\\), ann to class 2 otherwise. In this case, the Bayes desicion boundary corresponds to the point where\n$$ \\[\\begin{align}\n\\delta_k - delta_j &= 0 \\\\\nx \\cdot \\frac{\\mu_k}{\\sigma} - \\frac{\\mu^2_k}{2\\sigma^2} + \\log(\\pi_k) &- x \\cdot \\frac{\\mu_j}{\\sigma}  \\frac{\\mu_j^2}{2\\sigma^2} + \\log(\\pi_j) = 0 \\\\\nx(\\frac{\\mu_k - \\mu_j}{\\sigma}) &+ \\frac{\\mu_j^2 - \\mu_k^2}{2\\sigma^2} + \\log(\\frac{\\pi_k}{\\pi_j}) = 0 \\\\\nx &= \\frac{\\frac{\\mu^2_k -\\mu^2_j}{2\\sigma^2} - log(\\frac{\\pi_k}{\\pi_j})}{\\frac{\\mu_k - \\mu_j}{\\sigma}}\n\\end{align}\\]\n\\[\n(4.14)\nfor this spesific case:\n\\] x = = $$\nAn example is shown in the left hand panel of fig 4.4\n\nThe two normal density functions that are displayed, \\(f_1(x)\\) and \\(f_2(x)\\), represent two distinct classes. The mean and the variance parameters for the two desity functions are \\(\\mu_1 = -1.25, \\space \\mu_2 = 1.25\\), and \\(\\sigma_1^2 = \\sigma_2^2 = 1\\). The two densities overlap, and so given that \\(X=x\\), there is some uncertainty about the class to which observation belongs. If we assume that an observation is equally likely to come from either class–that is \\(\\pi_1 = \\pi_2 = 0.5\\)– then by inspection of (4.14), we see that the Bayes classfier assigns the observation to class 1 if \\(x&lt;0\\) and class 2 otherwise. Note that in this case, we can compute the Bayes classfier because we know that \\(X\\) is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation we are not able to calcualte Bayes classifier.\nIn practice, even if we are quite certain of our assumption that \\(X\\) is drawn from a Gaussian distribution within each class, we still have to estimate the parameters \\(\\mu_1, \\dots, \\mu_K,\\space \\pi_1,\\dots, \\pi_K,\\) and \\(\\sigma^2\\). The linear discriminant analysis (LDA) method approximates the bayes classfier vy plugging estimates fro \\(\\pi_k, \\mu_k\\) and \\(\\sigma^2\\) into (4.13). In particular the following estimates are used.:\n$$ \\[\\begin{align}\n\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i=k} x_i \\\\\n\n\\hat{\\sigma}^2 &= \\frac{1}{n-K}\\sum^K_{k=1} \\sum_{i:y_i =k}(x_i - \\hat{\\mu}_k)^2\n\n\\end{align}\\] $$ (4.15) Important\nwhere n is the total number of training observations, and n_k is the number of training observations in the kth class. The estimate for \\(\\mu_k\\) is simply the average of all the training observations from the kth class, while \\(\\hat{\\sigma}^2\\) can be seen as a weighted average of the sample variances for each of the \\(K\\) classes. Sometimes we have knowledge of the class membership probabilities \\(\\pi_1, \\dots, \\pi_K\\), which can be used directly. In the absence of any additional information, LDA estimates \\(\\pi_K\\) using the proportion of the training observations that belong to the kth class. In other words,\n\\[\n\\pi_k = n_k /n\n\\] The LDA classifer plugs the estimates given in (4.15) and (4.16) into (4.13), and assigns an observation \\(X=x\\) to the class for which\n\\[\n\\hat{\\delta}(x) = x \\cdot \\,\\frac{\\hat{u}_k}{\\hat{\\sigma}^2} \\,-\\,\\frac{\\hat{u}^2_k}{2\\,\\hat{\\sigma}^2}\\,+\\,\\log(\\hat{\\pi}_k)\n\\] (4.17)\nis largest. The word linear in classfier’s name stems from the fact that discriminant functions \\(\\hat{\\delta}_k(x)\\) in (4.17) are linear functions of \\(x\\).\nThe right hand panel of Fig 4.4 displays a histogram of a random sample \\(n=20\\) observations from each class. To implement LDA, we behan by estimating \\(\\pi_k, \\mu_k, \\sigma^2\\) using (4.15) and (4.16). We then computed the desicion boundy, shown as a black solid line, that results from assigning an observation to the clas for which (4.17) is largest.\n\nset.seed(12)\na = rnorm(n=20, mean = -1.25, sd =1)\nb = rnorm(n=20, mean=1.25, sd =1)\nd = rbind(tibble(x = a, class = 1),tibble(x = b, class =2))\n\nd %&gt;% ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F) \n\n\n\nd %&gt;% \n  mutate(d_x_1 = x * -1.25/1 - (-1.25^2/2) + log(0.5),\n         d_x_2 = x * 1.25/1 - (1.25^2/2) + log(0.5),\n         .pred.class = ifelse(abs(d_x_2) &gt; abs(d_x_1) ,1,2)) %&gt;% \n  ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F)  \n\n\n\n\n\nBDB = ((((-1.25)^2 - 1.25^2)/2) - log(0.5/0.5)) / ((-1.25 - 1.25)/1) # = 0\n\n\nd %&gt;% \n  mutate(d_x_1 = x * -1.25/1 - (-1.25^2/2) + log(0.5),\n         d_x_2 = x * 1.25/1 - (1.25^2/2) + log(0.5),\n         .pred.class = ifelse(abs(d_x_2) &gt; abs(d_x_1) ,1,2)) %&gt;% \n  ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F)  + geom_vline(xintercept = 0, linetype = \"dashed\")\n\n\n\n\nHere in this case bayess classfier assigns the observation to class 1 if For any \\(x &lt;0\\) and 2 if \\(x&gt;0\\). we can compute the Bayess classfier because we know that \\(X\\) is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In real-life we cannot calculate bayes classfier\n\nd %&gt;% \n  group_by(class) %&gt;% \n  summarise(mean_hat = mean(x), var_hat = sd(x)^2) \n\n# A tibble: 2 × 3\n  class mean_hat var_hat\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    -1.58   0.751\n2     2     1.35   0.781\n\n\n\nd %&gt;% \n  group_by(class) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(pi_hat = count/sum(count))\n\n# A tibble: 2 × 3\n  class count pi_hat\n  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     1    20    0.5\n2     2    20    0.5\n\n\n\nd %&gt;% \n  mutate(d_k_c1 = x * (-1.58/0.751) - (-1.58^2/(2*0.751)) + log(0.5),\n         d_k_c2 = x * (1.35 / 0.781) - (1.35^2/(2*0.781)) + log(0.5)) %&gt;% \n  mutate(pred.class = factor(ifelse(d_k_c1 &gt; d_k_c2,1,2)), class = factor(class)) %&gt;% \n  conf_mat(truth = class, estimate = pred.class) %&gt;% \n  autoplot(type = \"heatmap\")\n\n\n\n\n\nlibrary(discrim)\n\n\nAttaching package: 'discrim'\n\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nd %&gt;% mutate(class = factor(class)) -&gt; d\n\ndiscrim_linear() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"MASS\") -&gt; lda_spec\n\nlda_recipe &lt;- recipe(class~x,data =d)\n\nworkflow() %&gt;% \n  add_model(lda_spec) %&gt;% \n  add_recipe(lda_recipe) -&gt; lda_workflow\n\nlda_workflow %&gt;% \n  fit(data = d) %&gt;% \n  augment(new_data=d) \n\n# A tibble: 40 × 5\n        x class .pred_class .pred_1    .pred_2\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 -2.73  1     1             1.00  0.0000459 \n 2  0.327 1     2             0.155 0.845     \n 3 -2.21  1     1             1.00  0.000340  \n 4 -2.17  1     1             1.00  0.000391  \n 5 -3.25  1     1             1.00  0.00000637\n 6 -1.52  1     1             0.995 0.00463   \n 7 -1.57  1     1             0.996 0.00393   \n 8 -1.88  1     1             0.999 0.00119   \n 9 -1.36  1     1             0.991 0.00869   \n10 -0.822 1     1             0.937 0.0633    \n# ℹ 30 more rows\n\n\nTo sum up; the LDA classfier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance \\(\\sigma^\\), and plugging estimates for these parameters into the Bayes classfier. In section 4.4.4, we will consider a less stringent set of assumptions, by allowing the observations in the kth class to have a class specific variance, \\(\\sigma^2_k\\)\n\n\n3.3.3 Linear Discriminant Analysis for p &gt; 1\nWe are going to extend the LDA classfier to the case of multiple predictors. To do this, we will assume that \\(X = (x_1, x_2, \\dots, x_p)\\) is drawn from a multip variate Gaussian (or multivariate normal) distribution, with a class-specific meean vector and a common covariance matrix.\nThe multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional distribution, as in (4.11), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with \\(p=2\\) are shown in Figure 4.5.\n\nThe height of the surface at any particular point represnts the probability that bot \\(X_1\\) and \\(X_2\\) fall in a small region on that point. In either panel, if the surface is cut along the \\(X_1\\) axis or along the \\(X_2\\) axis, the resulting cross section will have the shape of a one dimensional normal distribution. The left hand panel illustrates an example in which \\(var(x_1) = var(x_2)\\) and \\(cor(x_1,x_2) = 0\\); this surface has a charactersitic bell shpae. However, the bell shape will be distorted if the preditors are correlated or have unequal variances, as in illusterated in the right hand panel. In this case, the base of the bell will have an elliptical, rather than circular shape.\nTo indicate that a p-dimnensional random varaible \\(X\\) has a multipvariate Gaussian distribution we write \\(X \\sim N(\\mu,\\sum)\\). Here \\(E(X) = \\mu\\) is the mean of \\(X\\) ( a vector with p components), and \\(Cov(X) = \\sum\\) is the $ p$ covariance matrix of \\(X\\). Formally, the multivariate Gaussian density is defined as\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} \\exp\\left(-\\frac{1}{2}(x -\\mu)^T \\textstyle\\sum^{-1}(x -\\mu) \\right)\n\\] (4.18)\nIn this case of \\(p&gt;1\\) predictors, the LDA classfier assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution \\(N(\\mu_k,\\textstyle\\sum)\\), where \\(\\mu_k\\) is class-specific mean vector, adn \\(\\textstyle\\sum\\) is a covariance matrix that is common to all \\(K\\) classes. Plugging the density function for the kth class, \\(f_k(X=x)\\) into (4.10) and performing a little bit of algebra reveals that the Bayes classfier assigns an observation \\(X=x\\) to the class for which\n\\[\n\\delta_k(x) = x^T \\textstyle\\sum^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\textstyle\\sum^{-1}\\mu_k + \\log \\pi_k\n\\] (4.19)\nis largest. This is the vector/matrix version of 4.13.\nAn example is shown in the left-hand panel of Figure 4.6\n\nThree equally sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represetn regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes desicion boundaries. In other words, they represent the set of values \\(x\\) for which \\(\\delta_k(x) = \\delta_l(x)\\); i.e.\n\\[\nx^T\\textstyle\\sum^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\textstyle\\sum^{-1} = x^T\\textstyle\\sum^{-1}\\mu_l - \\frac{1}{2}\\mu_l^T\\textstyle\\sum^{-1}\\mu_l\n\\] (4.20)\nfor \\(k\\neq1\\). The \\(\\log \\pi_k\\) term from 4.19 has dissapeared because each of the three classes has the same number ob observations; i.e. \\(\\pi_k\\) is the same for each class. No that there are three lines representing the Bayes desciion boundaries because there are three pairs of clases among the three classes. That is, on desicion boundary separets class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classfier will classify an observation according to the region in which it is located.\nOnce again, we need to estimate the unknown parametrs \\(\\mu_1, \\dots, \\mu_K, \\pi_1, \\dots, \\pi_K\\) and \\(\\sum\\); the formulas are similar to those used in the one dimensiona lcase, given in 4.15. To assign a new observation \\(X=x\\), LDA plugs these estiamtes into 4.19 and classfiers to the class for which \\(\\hat{\\delta}(x)\\) is largest. Note that in 3.19 \\(\\delta_k(x)\\) is a linear function of \\(x\\); that is the LDA desicion rule depend on \\(x\\) only through a linear combination of its elements. Once again, this is the reason for the word linear in LDA.\nIn the right hand panel of Fig 4.6, 20 observations drawn from each of the three classes are displayed, and the resulting LDA desicion boundaries are shown as solid black lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test erro rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively. This indicates that LDA is performing well on this data.\nWe can perfrom LDA on the default data in order to predict whether or not an individaul will default on the bassi of credit card balance and student status. The LDA model fit to the 10,000 training samples results in a training error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.\n\nFirst of all training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classfier to perform worse if we use it to predict whether or not. anew set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parametsr p to number of samples n, the more we expect this overfitting to play a role. For these data, we don’t expect tihs to be a problem, since \\(p=2\\) and \\(n = 10,000\\).\nSecond, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individaul will not default, regardless of his or her credit car balance and student statsu, will result in an error rate of 3.33%. In other words, the trivial null classifier will achive an error rate thatis only a bit higher than the LDA training set error rate.\n\nIn pracitice, a binay classifiar such as this one can make two types of errors: it can incorretly assign an individual who default to the no default category, or it can incorretly assign an individual who does not defaul to default category. It is often of interst to determine which of these two types of errors are being made. A confusion matrix show for the default data in Table 4.4 is a convinient way to dispaly this information.\n\ndiscrim_linear() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"MASS\") -&gt; lda_spec\n\nlda_recipe &lt;- recipe(default ~ balance + student, data = default)\n\nworkflow() %&gt;% \n  add_model(lda_spec) %&gt;% \n  add_recipe(lda_recipe) -&gt; lda_workflow\n\nlda_workflow %&gt;% \n  fit(data = default)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nlda(..y ~ ., data = data)\n\nPrior probabilities of groups:\n    No    Yes \n0.9667 0.0333 \n\nGroup means:\n      balance studentYes\nNo   803.9438  0.2914037\nYes 1747.8217  0.3813814\n\nCoefficients of linear discriminants:\n                    LD1\nbalance     0.002244397\nstudentYes -0.249059498\n\n\n\none thing we can look in LDA outpit is the group means. We see that defaulted people have 2 times balance than not defaulted. % of students are higher for defaulted people.\n\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data =default)\n\n# A tibble: 10,000 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 No      No         730. 44362. No             0.997  0.00313 \n 2 No      Yes        817. 12106. No             0.997  0.00281 \n 3 No      No        1074. 31767. No             0.984  0.0156  \n 4 No      No         529. 35704. No             0.999  0.00122 \n 5 No      No         786. 38463. No             0.996  0.00407 \n 6 No      Yes        920.  7492. No             0.995  0.00454 \n 7 No      No         826. 24905. No             0.995  0.00491 \n 8 No      Yes        809. 17600. No             0.997  0.00270 \n 9 No      No        1161. 37469. No             0.977  0.0234  \n10 No      No           0  29275. No             1.00   0.000102\n# ℹ 9,990 more rows\n\n\nLets have a confusion matrix\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  conf_mat(truth = default, estimate = .pred_class) %&gt;% print()\n\n          Truth\nPrediction   No  Yes\n       No  9644  252\n       Yes   23   81\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;%\n  sens(truth = default, estimate = .pred_class, estimator = \"binary\") %&gt;% print() # overall accuracy; specificity; the goal of LDA is to max this\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.998\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;%\n  sens(truth = default, estimate = .pred_class,event_level = \"second\") # accuracy of Yes predictions; 1-0.243 = 0.757 -&gt; sensitivity; the percentage o true defaulters that are identified\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.243\n\n\nThis confusion matrix table shows 104 people would default. Of those people, 81 is actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorretly labeled. This looks like a pretty low error rate! However, of the 333 individuals who default, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who default is very high. From the perspective of a credit card company that is tryibng to identify high-risk individuals, an error rate of 75.7% among individauls who default may be well unaccapteble.\nClass-specific performance is also important in medicine and biology where the terms sensivity and specificty characterize the performance of a classifer or screening test. In this case the sensivity is the percentage of true defaulters that are identified, a low 34.3% in this case. The specifity is the percentage of nond-defaulters that are correctly identified, here (\\(1-23/9,667 = 99.8%\\))\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  accuracy(truth = default, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\nWhy does LDA do such a poor job of classfiying the customers who default? In other words, why does it have such a low sensivity? AS we have seen, LDA is trying to approximate the Bayes classfier, which has the lowest total error rate out of all classfiers (if the Gaussian model is correct). That is the Bayes classfier will yield the smallest possible total number of misclassifed observaitons, irrespective of which class the errors come from. That is, some misclassfications will result from incorretly assigninga customer who does not default tot he default class, and others will result from incorreclty assigning a customer who defaults tot he non default class. In contrast a credt card company might particualrly wihch to aviod incorrectly classifying an individual who wilkl defualt, where as incorrectly classfiyng an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classfier that better meets the credit card company’s needs.\nThe Bayes classfier works by assignig an observation to the class for which the posterior probability \\(p_k(X)\\) is greatest. In the two-class case, this amounts to assigning an observation to the default class if\n\\[\nPr(default = Yes | X = x) &gt; 0.5\n\\]\n\nlda_workflow %&gt;% \n  fit(data =default) %&gt;% \n  augment(new_data = default) %&gt;% \n  filter(.pred_Yes &gt; 0.45, .pred_Yes &lt; 0.55)\n\n# A tibble: 39 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 Yes     No        1964. 39055. Yes            0.488     0.512\n 2 Yes     No        1992. 42133. Yes            0.456     0.544\n 3 Yes     No        1981. 28128. Yes            0.468     0.532\n 4 Yes     No        1964. 50554. Yes            0.489     0.511\n 5 Yes     No        1972. 34363. Yes            0.479     0.521\n 6 No      Yes       2027. 20470. No             0.545     0.455\n 7 No      Yes       2052. 13131. No             0.516     0.484\n 8 Yes     Yes       2036. 14436. No             0.535     0.465\n 9 Yes     No        1923. 56203. No             0.537     0.463\n10 No      No        1956. 45508. Yes            0.499     0.501\n# ℹ 29 more rows\n\n\nThus the bayes classfier, andby extension LDA, uses a threshold of 50% for the posterior probabiblity of default in order to assign an observation to the default class. However, if we are concerned about incorretly predicting the default status for individauls who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the default class. In other words, instead of assigning an observation to the default class if 4.21 holds, we could instead assign an observation to this class if\n\\[\nPr(default = Yes | X = x) &gt; 0.2\n\\]\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.2, \"Yes\", \"No\"), .pred_class = factor(.pred_class)) %&gt;% \n  conf_mat(truth = default, estimate = .pred_class) %&gt;% print()\n\n          Truth\nPrediction   No  Yes\n       No  9432  138\n       Yes  235  195\n\n\nNow LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4%. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this imporvement comes at a cost: now 235 individauls who do not default are incorrectly classified. As a result, the overall error rate has increased in the total error rate to be a small price to pay for more accurate identification of individauls who do indeed default.\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  mutate(.pred_class = ifelse(.pred_Yes &gt; 0.2, \"Yes\", \"No\"), .pred_class = factor(.pred_class)) %&gt;% \n  sens(truth = default, estimate = .pred_class, event_level = \"second\") \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.586\n\n\n\nFigure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. Using a threshold of 0.5 as in 4.21 minimizes th eoverall error rate, shown as a black solid line. This is to be expected since the Bayes classifier uses a threshold of 0.5 and is known to have the lowest overall error rate. But when a threshold of 0.5 is used the error rate among the individuals who default is quite high (blue dashed line). As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individiuals who do not increasess. How can we decide which threshold value is the best? Such a decision must be based on domain knowledge such as detailed information about the costs associated with default.\nThe ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible threshold. The name “ROC” means receiver operating characteristics.\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  roc_curve(truth = default,.pred_No) %&gt;% \n  autoplot() + labs(x = \"False positive rate : 1 - specifity\", y = \"True positive rate: sensitivity\", title = \"ROC curve\")\n\n\n\n#| fig-cap: A ROC curve for the LDA classifer on the default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctşy identified, using a given threshold value. The flase posiitve rate is 1- speficity: the fraction of non defaulters that we classify incorretly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner i dicating a high true positive rate and a low false positive rate. The dotted line represts the no information classifier: this is what we would expect if student statust and credit card balance are not associated with probability of default.\n\nThe overall performance of a classifier, summarized over all possible thresholds, is given by the area under the ROC curve (AUC) An ideal ROC curve will hug the top left corner so the larger the AUC the better the classifier. For this data the AUC is 0.95.\n\nlda_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.950\n\n\nwhich is close to maximum of 1 so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5. ROC curves are useful for comparing different classfiers since they take into account of all possible thresholds. It turns out that the ROC curve for the logistic regression model of section 4.3.4 fit to these data is virtually indistinguaslibe from this one for the LDA model.\nAs we have seen above varying the classfier threshold changes its true positive and false positive rate. These are also called the sensivity and one minus the specificity of our classifer. Since there is an almost bewildering array of terms used in this context, we now give a summary.\n\nTable 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemology literature we think of “+” as “disease” that we are trying to detect, and “-” as the “non disease” state. To make the connection to the classical hypothessi testing literatureü, we think of “-” as the null hypothessi and “+”. as the alternative (non-null) hypothessi. In the context of the default data, “+” indicates an individual who defaults, and “-” indicates one who does not."
  }
]