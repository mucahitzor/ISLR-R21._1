[
  {
    "objectID": "ExerciseLinearModel.html#simple-linear-regression",
    "href": "ExerciseLinearModel.html#simple-linear-regression",
    "title": "3  Summary: Linear Regression",
    "section": "3.1 Simple Linear Regression",
    "text": "3.1 Simple Linear Regression\nWe have an idea in mind: we want to explain one response variable \\(y\\) varies with changes in one single predictor variable \\(x\\); that is we are trying to explain \\(y\\) in terms of\\(x\\).\nWe assume a linear relationship between \\(X\\) and \\(Y\\) and we need to write a model for it.\nNow, we know that \\(y\\) is not effected only by \\(x\\)–it would not make any sense– there are some variables also effecting \\(y\\). That is, even if we knew the true relationship between \\(x\\) and \\(y\\)–\\(f(x)\\) we would not perfectly explain the variations in \\(y\\) with the changes in \\(x\\) solely. So we would have the following equation\n\\[\n\\begin{align}\ny_i & \\approx f(x_i) \\\\\ny &\\approx f(x)\n\\end{align}\n\\]\nFor each observation we would have \\(x\\) and \\(y\\) values but the function would not constitute a perfect fit to data. Changes in \\(x\\) would not explain changes in \\(y\\) perfectly; so \\(y\\) would never be equal to the function of \\(x\\). So even if we knew the true function \\(f\\) we would not be able to predict \\(y_i\\) values using \\(x_i\\); because \\(x\\) is not the only variable affecting \\(y\\). So using our true function \\(f\\) we could put each observation \\(x\\) to the function and get some values (\\(f(x)\\)) which would not be equal to the true \\(y\\) values observed. The difference than would be the effect of other variables to \\(y\\); the effect which is not being explained by \\(x\\). For each \\(x\\) value we would have different values for this difference \\(y - f(x)\\). This difference is called error and marked with \\(\\epsilon\\) or \\(u\\) symbols. So now we can get the eqaulity and write\n\\[\n\\begin{align}\ny_i &= f(x_i) + u_i \\\\\ny &= f(x) + u\n\\end{align}\n\\]\nGenerally our goal is to estimate \\(f\\) with different approaches: statistical learning.\nFor SLR case we make several assumptions. The most important one is the form of this function: \\(f\\) is linear. So we can write a simple linear regression model of:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n\\] or\n\\[\ny = \\beta_0 + \\beta_1 x + u\n\\] (a.1)\n\\(u\\) here captures all the effects except the \\(x\\)’s, so we refer it as unobserved. If other factors in \\(u\\) is zero, \\(\\Delta u = 0\\), then \\(x\\) has a linear effect on y:\n\\[\n\\Delta y = \\beta_1 \\Delta x, \\space \\text{if} \\space \\Delta u = 0\n\\] So, the change in \\(y\\) is equal to change in \\(x\\) multiplied by \\(\\beta_1\\). Now notice that this \\(\\beta_1\\) is a constant and the changes in \\(x\\) are effecting \\(y\\) at the same rate of \\(\\beta_1\\) no matter the rate of change or the level of \\(x\\); this is called the linearity–one unit change in \\(x\\) has the same effect on \\(y\\) regardless of the initial value of \\(x\\).\n\n3.1.1 Assumptions about \\(u\\)\nThere is a key assumption about \\(u\\) that we can always make. As long as the intercept \\(\\beta_0\\) is included in the equation, we can assume the average value of \\(u\\) in the population is zero: \\[\nE(u) = 0\n\\]\nzero conditional mean assumption\nThe other assumption is regarding how \\(u\\) and \\(x\\) are related. Now we are going to assume there is no relationship between \\(x\\) and \\(u\\). We can check this relation with correlation coefficient but correlation only checks at one level of relationship: it is possible for \\(u\\) to be uncorrelated with \\(x\\) while being correlated with functions of \\(x\\), such as \\(x^2\\). A better assumption involves the expected value of u given x:\nBecause \\(u\\) and \\(x\\) are random variables, and they are independent from each other, for any \\(x\\) we can obtain the expected(or average) value of \\(u\\) given any value of \\(x\\) \\[\nE(u|x) = E(u) = 0\n\\] So zero conditional mean assumption states that for any given value of \\(x\\) the average of the unobservables is the same and therefore must equal to average value of \\(u\\) in the entire population.\n\n\n\n\n\n\nExamples of Simple Linear Regression\n\n\n\n\\[\nsales = \\beta_0 + \\beta_1 \\times x + u\n\\] \\[\nwage = \\beta_0 + \\beta_1 educ + u\n\\]\nFor example consider the wage regression. To simplify lets assume that wage is effected only by educ and natural ability.Then the assumption \\(E(u|x) = E(u) = 0\\) requires \\(E(abil|8) = E(ability)=0 = E(abil|16)...\\); the average level of ability is the same regardless of years of education. The average ability must be same for all education levels. If we think that average ability level increases with years of education, then this assumption is false (this would happen if on average, people with more ability choose to become more educated). Now we cannot observe natural ability, we cannot measure it, we cannot take its average. But this is an issue that we must adress before applying SLR analysis.\n\n\n\n\n3.1.2 Estimating the coefficients: Deriving the Ordinary least squares estimates\nSo our simple regression model which we think is true for the population is\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n\\]\nSince \\(\\beta_0\\) and \\(\\beta_1\\) is unknown before we can use the above equation to make predictions we must use sample data from the population to estimate coefficients. Let \\(\\{(x_i,y_i), i = 1,\\dots,n\\}\\) denote a random sample of size \\(n\\) from the population.\nAnd we will get an estimated linear model of\n\\[\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\n\\]\nThe formulas for these coefficients are\n\\[\n\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\end{align}\n\\] These coefficients are colled OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\). These estimates come from the approach of minimizing theleast squares criterion:\nso we have \\[\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} x_i\n\\] \\(y_i\\) is the prediction from each observation \\(x_i\\) with OLS coefficients. Now since our OLS estimates are going to be different from the real coefficients, for each observation \\(x_i\\) \\(y_i\\) and \\(\\hat{y_i}\\) will be different. This difference is called residuals(\\(\\hat{u_i}\\))\n\\[\n\\hat{u_i} = y_i - \\hat{y_i} = y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i\n\\]\nThese residuals are the distance of each prediction from the actual value of the response. There are \\(n\\) residuals (\\(\\hat{u_i}\\)) (note that these are not the same as \\(u_i\\)). Before going further have a look at the figure below\n \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) formulas come from minimizing the sum of squared residauls or residual sum of squares(RSS)\n\\[\nRSS = \\sum_{i=1}^n\\hat{u_i}^2 = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 = \\sum_{i=1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2\n\\]\nSo the OLS comes from the fact that these estiamtes minimize the sum of squared residuals.\nOnce we determine the OLS intercept and slope estimates we form the OLS regression line, or sometimes refreed to as sample regression function (SRF)(since this function is estimated version of population regression function \\(f(x)\\))\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n\\] (a.2)\nLets use advertising data to estimate the coefficients, find the residuals, and draw the OLS regression line;\nOur population regression is\n\\[\nsales = \\beta_0 + \\beta_1 TV + u_i\n\\] The data:\n\nadvertising = read_csv(\"./data/Advertising.csv\", col_select = -1)\n\nNew names:\nRows: 200 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(4): TV, radio, newspaper, sales\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nadvertising\n\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n\n\nThe OLS coefficients\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV, advertising) %&gt;% \n  pluck(\"fit\") -&gt; lm_result\nlm_result %&gt;% summary()\n\n\nCall:\nstats::lm(formula = sales ~ TV, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nSo our OLS regression is\n\\[\n\\hat{y_i} = 7.032 + 0.0475 \\times TV\n\\] This means that additional $1,000 spending on TV will increase sales by 47.5 units on average.\nHere is our \\(\\hat{y}\\) values in .fitted column, residuals in .resid column,\n\nlm_result %&gt;% augment()\n\n# A tibble: 200 × 8\n   sales    TV .fitted .resid    .hat .sigma   .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1  22.1 230.    18.0   4.13  0.00970   3.25 0.00794       1.27  \n 2  10.4  44.5    9.15  1.25  0.0122    3.27 0.000920      0.387 \n 3   9.3  17.2    7.85  1.45  0.0165    3.27 0.00169       0.449 \n 4  18.5 152.    14.2   4.27  0.00501   3.25 0.00434       1.31  \n 5  12.9 181.    15.6  -2.73  0.00578   3.26 0.00205      -0.839 \n 6   7.2   8.7    7.45 -0.246 0.0180    3.27 0.0000534    -0.0762\n 7  11.8  57.5    9.77  2.03  0.0105    3.26 0.00208       0.627 \n 8  13.2 120.    12.7   0.454 0.00549   3.27 0.0000538     0.140 \n 9   4.8   8.6    7.44 -2.64  0.0181    3.26 0.00616      -0.818 \n10  10.6 200.    16.5  -5.93  0.00690   3.24 0.0116       -1.83  \n# ℹ 190 more rows\n\n\nSo we can calculate \\(RSS = \\sum\\hat{u_i}^2 = \\sum(y_i - \\hat{y_i})^2\\)\n\nlm_result %&gt;% \n  augment() %&gt;% \n  mutate(.resid_squared = .resid^2) %&gt;% \n  pull(.resid_squared) %&gt;% sum()\n\n[1] 2102.531\n\n\nLets draw our OLS regression line\n\nlm_result %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x=TV, y = sales) + geom_point() + geom_abline(intercept = lm_result$coefficients[1], slope = lm_result$coefficients[2], color =\"#0b53c1\")+ \n  geom_segment(aes(xend = TV, yend = .fitted), color = \"#ff0055\")\n\n\n\n\nFor the advertising data, the least squares fit for the regression of sales onto TV. The fit is found by minimizing the sum of squared errors. Each red line segment represents a residual, and the fit make a comprimise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  }
]