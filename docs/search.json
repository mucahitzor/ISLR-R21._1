[
  {
    "objectID": "Chapter3.html#simple-linear-regression",
    "href": "Chapter3.html#simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.1 Simple Linear Regression",
    "text": "2.1 Simple Linear Regression\nPredicting a quantitative response \\(Y\\) on the basis of a single predictor variable \\(X\\).\nOur assumption is that there is approximately a linear relationship between \\(X\\) and \\(Y\\); we can write this linear relationship as\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\epsilon\n\\]\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\] (3.1)\nFor example lets say \\(X\\) is TV, and \\(Y\\) is sales\n\\[\nsales = \\beta_0 + \\beta_1 \\times TV + \\epsilon\n\\] or\n\\[\nsales = \\beta_0 + \\beta_1 \\times TV\n\\]\nOn (3.1) \\(\\beta_0\\) and \\(\\beta_1\\) are unknown constants that represent the intercept and slope in the linear model. Together they are known as coefficients or parameters.\nWe are going to use or training data to produce estimates for \\(\\beta_0\\) =&gt; \\(\\hat{\\beta_0}\\) and \\(\\beta_1\\) =&gt; \\(\\hat{\\beta_1}\\). Using these predicted coefficients we can predict sales;\n\\[\n\\hat{sales} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times TV\n\\]\nor as in general form\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\n\\] (3.2)\n\n2.1.1 Estimating the coefficients\nSince, \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, before we can use (3.1) to make predictions we must use data to estimate the coefficients. We have \\(n\\) observations :\n\\[\n(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\n\\]\nWe want our estimated coefficients to give such predictions that will fit the avaible data as well =&gt; \\(y_i \\approx \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\) for \\(i = 1,\\dots, n\\). This coefficients will allow us to draw a regression line and we want this regression line to be close as possible to the \\(n\\) data points we have.\nThere are different ways to measure closeness. The most common approach is minimizing the least squares criterion. Alternative approaches will be considered in Chapter 6.\nOur predictions come from \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\).\nThen for each data we have a residual: difference between \\(y\\) and \\(\\hat{y}\\):\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\nWe need to take the squares to get the distances–because of the negative residuals, and sum them to get the residual sum of squares(RSS)\n\\[\nRSS = e_1^2 + e_2^2 + \\dots + e_n^2\n\\]\nthis is equal to\n\\[\nRSS = (y_i - \\hat{beta_0} - \\hat{\\beta_1}x_1)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2) + \\dots + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1}x_n)\n\\] (3.3)\nThe least squares approach chooses \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) to minimize RSS. These minimizers are\n\\[\n\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y_i} - \\hat{\\beta_1}\\bar{x}\n\\end{align}\n\\] (3.4)\n\\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i\\) and \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i\\) are the sample means.\nSo (3.4) defines the least squares coefficient estimates for simple linear regression.\nLets calculate them with R\n\nbeta_1_hat_adv = sum((advertising$TV - mean(advertising$TV)) * ((advertising$sales - mean(advertising$sales)))) / sum((advertising$TV - mean(advertising$TV))^2)\nbeta_1_hat_adv\n\n[1] 0.04753664\n\n\nSo; our $ = 0.0475 $\n\nbeta_0_hat_adv = mean(advertising$sales) - beta_1_hat_adv * mean(advertising$TV)\nbeta_0_hat_adv\n\n[1] 7.032594\n\n\nour \\(\\hat{\\beta_0} = 7.032\\)\nLets compare them with r function\n\nsummary(lm(sales ~ TV, data = advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nCalculating the predicted values\n\ny_hat_adv = beta_0_hat_adv + beta_1_hat_adv * advertising$TV\ny_hat_adv[1:10]\n\n [1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162  9.765950\n [8] 12.746498  7.441409 16.530414\n\n\nOur \\(y_i\\) values are as above.\n\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n\n[1] 2102.531\n\n\nOur \\(\\text{RSS} = 2102.531\\)\nSo we can draw our regression line\n\nadvertising %&gt;%\n  ggplot() + aes(x=TV, y = sales) +  geom_abline(intercept = beta_0_hat_adv, slope = beta_1_hat_adv, color = \"#262B70\", size =1.2) +\n  geom_segment(aes(xend=TV, yend=y_hat_adv), color = \"#939393\") +\n  geom_point(color = \"#AA1D2E\", size =2) + theme_par()\n\n\n\n\nFor the advertising data, the least squares fit for the regression of sales onto TV. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, adn the fit make a comprimise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot\n\n\n\n\nSo we have\n\\[\n\\hat{y_i} = 7.032 + 0.0475x_i\n\\]\nAccording to this approximation an additional $1,000 spent on TV increases sales by 47.5 units.\n\n\n2.1.2 Assessing the Accuracy of the Coefficient Estimates\nWe assumed that true relationship is linear: \\(Y = f(X) + \\epsilon\\). We don’t know \\(f\\), and \\(\\epsilon\\) is a mean-zero random error term.\nWe said \\(f\\) is approximatly linear, so that \\(f(X) = \\beta_0 + \\beta_1 X\\); which means\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\] (3.5)\nerror term captures: * the true relationship may not be linear * other variables that affect Y * measurement error\nand is independent of \\(X\\).\n(3.5) is the population regression line: the best linar approximation to the true relationship between \\(X\\) and \\(Y\\).\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}X\n\\] is the least squares line. They are different of course! But we don’t know the population regression line. If we did:\nFor example, lets create a data;\n\nFirst we create random x values from 100 random numbers\n\n\nseq(-2,2,length.out=100)[1:10]\n\n [1] -2.000000 -1.959596 -1.919192 -1.878788 -1.838384 -1.797980 -1.757576\n [8] -1.717172 -1.676768 -1.636364\n\n\n\nset.seed(11)\nx = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\nx[1:10]\n\n [1] -0.6666667  0.2222222 -1.0303030 -1.3939394 -0.5454545  0.3838384\n [7] -1.5555556  1.3939394  1.4343434  0.4646465\n\n\nlets define our \\(f\\)–population parameters \\(\\beta_0\\) and \\(\\beta_1\\)\n\\[\nf(X) = 2 + 3\\times X\n\\] Now lets create our \\(Y\\) values from this function but we also want to add random error values as well\n\nset.seed(11)\ny = 2 + 3*x + rnorm(100)\ny[1:10]\n\n [1] -0.5910311  2.6932610 -2.6074622 -3.5444715  1.5421255  2.2173638\n [7] -1.3430610  6.8067360  6.2573073  2.3898188\n\n\nSo we have a data set\n\ndata = tibble(\n  y = y, x = x\n)\n\nNow lets plot this data points\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() \n\n\n\n\nNow, even though we already know \\(f\\) and population parameters \\(\\beta_0 = 2\\) and \\(\\beta_1 = 3\\), lets estimate them:\n\nsummary(lm(y ~ x, data = data))\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.88077    0.09180   20.49   &lt;2e-16 ***\nx            3.05893    0.07608   40.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSo our least squares estimation is\n\\[\n\\hat{y_i} = 1.88 + 3.06 x_i\n\\] Lets draw this least squares regression line to our plot\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept=1.88, slope = 3.06, size = 1.2) + theme_par()\n\n\n\n\nWhat about the population regression line\n\ndata %&gt;% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept = 1.9, slope = 3.06, size = 1.2) + geom_abline(intercept = 2, slope = 3, color =\"red\") + theme_par()\n\n\n\n\nThey are not the same! If we were to have another data from the same data generation process other estimates of parameters would result with different least squares regression lines:\n\nset.seed(111)\nx_r1 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r1 = 2 + 3*x_r1 + rnorm(100)\nset.seed(1111)\nx_r2 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r2 = 2 + 3*x_r2 + rnorm(100)\nset.seed(11111)\nx_r3 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r3 = 2 + 3*x_r3 + rnorm(100)\nset.seed(111111)\nx_r4 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r4 = 2 + 3*x_r4 + rnorm(100)\nset.seed(1111111)\nx_r5 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r5 = 2 + 3*x_r5 + rnorm(100)\nset.seed(11111111)\nx_r6 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r6 = 2 + 3*x_r6 + rnorm(100)\n\nLets now estimate population parameters for each of these data and plot them\n\ndata_r = tibble(\n  y_r1,x_r1,y_r2,x_r2,y_r3,x_r3,y_r4,x_r4, y_r5,x_r5,y_r6,x_r6\n)\ndata_r\n\n# A tibble: 100 × 12\n     y_r1   x_r1   y_r2    x_r2   y_r3   x_r3  y_r4   x_r4   y_r5    x_r5  y_r6\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1  5.99   1.11   0.807 -0.263   8.82   1.80  -3.05 -1.52   7.64   1.68   6.49 \n 2  6.53   1.35   3.74   0.141   6.23   1.92   5.11  1.27   8.71   2      7.75 \n 3  6.28   1.31   2.16  -0.0202 -0.272 -0.707  2.39 -0.101  4.59   0.909  1.21 \n 4  1.52  -0.141 -0.178 -0.990   5.45   0.788 -4.55 -1.84  -1.41  -1.11   0.478\n 5  0.708 -1.03  -0.464 -0.586   0.361 -0.343 -3.31 -1.15   9.59   1.88   2.64 \n 6  2.05   0.343  4.12   0.788  -1.09  -1.11  -5.13 -1.96   0.723 -0.586  4.10 \n 7  4.54   0.747  5.44   1.60    0.479 -0.707  2.27  0.182  2.29  -0.263  3.68 \n 8  1.69  -0.626  8.02   1.80   -2.23  -1.64   1.55  0.222 -1.45  -0.990  5.08 \n 9  2.84   0.869  1.02   0.384   4.04   0.949  3.34 -0.747 -2.21  -0.828  8.33 \n10 -0.933 -0.990  3.52   0.505  -0.471 -0.505 -1.07 -0.869  1.47   0.0606 2.45 \n# ℹ 90 more rows\n# ℹ 1 more variable: x_r6 &lt;dbl&gt;\n\n\n\ndata %&gt;% \n  ggplot() + aes(x,y) + geom_point(size = 0, color = \"white\")  + \n  geom_abline(intercept = lm(y_r1 ~ x_r1)$coefficients[1], slope = lm(y_r1 ~ x_r1)$coefficients[2], color =\"#29019F\") +\n  geom_abline(intercept = lm(y_r2 ~ x_r2)$coefficients[1], slope = lm(y_r2 ~ x_r2)$coefficients[2], color =\"#0A04BF\") +\n  geom_abline(intercept = lm(y_r3 ~ x_r3)$coefficients[1], slope = lm(y_r3 ~ x_r3)$coefficients[2], color =\"#0930DF\") +\n  geom_abline(intercept = lm(y_r4 ~ x_r4)$coefficients[1], slope = lm(y_r4 ~ x_r4)$coefficients[2], color =\"#0E6DFF\") +\n  geom_abline(intercept = lm(y_r5 ~ x_r5)$coefficients[1], slope = lm(y_r5 ~ x_r5)$coefficients[2], color =\"#2BA8FF\") +\n  geom_abline(intercept = lm(y_r6 ~ x_r6)$coefficients[1], slope = lm(y_r6 ~ x_r6)$coefficients[2], color =\"#48D9FF\") +\n  geom_abline(intercept =2, slope =3, color = \"red\") +\n  \n  theme_par()\n\n\n\n\nSo, different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.\nThis is because we are using a sample, and estimating characteristics of the population. Usually these characteristics are different, but generally sample characteristics will provide a good estimate to the population characteristics.\nComputing \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) from different sets of sample data provide different but similar results. And we are trying to estimate population parameters \\(\\beta_0\\) and \\(\\beta_1\\) with these. Some of these \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) will overestimate, some will underestimate \\(\\beta_0\\), and \\(\\beta_1\\). But if we could average all these estimated parameters and take the average, than this average should be equal to population parameters; if this is the case this estimator is called unbiased estimator. So an unbiased estimator does not systematically over- or under-estimate the true parameter.\nOkay but how close \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are to the true values \\(\\beta_0\\) and \\(\\beta_1\\). We want to compute the standard errors associated with \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). Standard error telss us the average amount of estimate differes from the actual value.\n\\[\n\\begin{align}\n\\text{SE}(\\hat{\\beta_0})^2 &= \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x}^2)}\\right] \\\\\n\\text{SE}(\\hat{\\beta_1})^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\end{align}\n\\] (3.8)\nWhere \\(\\sigma^2=\\text{Var}(\\epsilon)\\).\nNotice that formula of \\(\\text{SE}(\\hat{\\beta_1})\\) is smaller when \\(x_i\\) are more spread out; intutively we have more leverage to estimate a slope when this is the case.\nIn general \\(\\sigma^2\\) is not known, but can be estimated from the data. The estimate of \\(\\sigma\\) is known as the residual standard error, and given by the formula\n\\[\n\\text{RSE} = \\sqrt{\\text{RSS}/(n-2)}\n\\] So, when \\(\\sigma^2\\) is estimated fro mthe data we should write \\(\\hat{\\text{SE}}(\\hat{\\beta_1})\\) to indicate that an estimate has been made, but usually we drop this extra hat.\nStandard errors can be used to compute confidence intervals. A 95% confidence interval is defines as a range of values such that with 95% probability, the rage will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for \\(\\beta_1\\) approximately takes the form\n\\[\n\\hat{\\beta_1} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_1})\n\\] (3.9)\nSo there is approximately a 95% chance that the interval \\[[\\hat{\\beta_1} - 1.96 \\cdot \\text{SE}(\\hat{\\beta_1}), \\hat{\\beta_1} + 1.96 \\cdot \\text{SE}(\\hat{\\beta-1})]\\] (3.10) will contain the true value of \\(\\beta_1\\). Same is true for \\(\\beta_0\\)\n\\[\n\\hat{\\beta_0} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_0})\n\\] (3.11)\nLets calculate the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) from our original data and model\n\\[\n\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\cdot TV\n\\] We first need to calculate RSS and RSE:\n\\[\n\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\n\\]\n\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n\n[1] 2102.531\n\n\n\\[\n\\text{RSE} = \\sigma = \\sqrt{RSS/(n-2)}\n\\]\n\nRSE = sqrt((RSS / (length(advertising$sales) -2)))\nRSE\n\n[1] 3.258656\n\n\nFor \\(\\beta_0\\)\n\\[\n\\text{SE}(\\hat{\\beta_0}) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]\n\\]\n\nse_beta_0_adv =  sqrt(RSE^2 * (1/length(advertising$sales) + (mean(advertising$TV)^2 / sum((advertising$TV - mean(advertising$TV))^2))))\nse_beta_0_adv\n\n[1] 0.4578429\n\n\nSo we can calculate the confidence interval for \\(\\beta_0\\)\n\ncat(\"In the absence of any advertising, sales will on average, fall somewhere between\",beta_0_hat_adv - 1.96 * se_beta_0_adv, \"and\", beta_0_hat_adv + 1.96 * se_beta_0_adv)\n\nIn the absence of any advertising, sales will on average, fall somewhere between 6.135221 and 7.929966\n\n\nFor \\(\\hat{\\beta_1}\\):\n\\[\n\\text{SE}(\\hat{\\beta_1})^2 =\\frac{\\sigma^2}{\\sum(x_i - \\bar{x})^2}\n\\]\n\nse_beta_1_adv = sqrt(RSE^2 / (sum((advertising$TV - mean(advertising$TV))^2)))\nse_beta_1_adv\n\n[1] 0.002690607\n\n\n\ncat(\"For each $1,000 increase in TV advertising, average increase in sales will be between\",(beta_1_hat_adv - 1.96 * se_beta_1_adv) * 1000, \"and\", (beta_1_hat_adv + 1.96 * se_beta_1_adv)*1000, \"by 95% confidence\")\n\nFor each $1,000 increase in TV advertising, average increase in sales will be between 42.26305 and 52.81023 by 95% confidence\n\n\nLets confirm our results\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nconfint(lm(sales ~ TV, advertising))\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\nSo standard errors of our estimated parameters tells us the average amount of difference from the true population parameters. And using the confidence intervals we can tell a range of the true population parameters’ interval with a percentage (usually 95%).\nLets do this for our data as well, which we know has the form\n\\[\ny_i = 2 + 3 x_i + \\epsilon_i\n\\] Lets calculate \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) first\n\nbeta_1_hat_data = sum((data$x - mean(data$x)) * (data$y - mean(data$y))) / sum((data$x - mean(data$x))^2)\nbeta_1_hat_data\n\n[1] 3.058925\n\n\n\nbeta_0_hat_data = mean(data$y) - beta_1_hat_data * mean(data$x)\nbeta_0_hat_data\n\n[1] 1.880772\n\n\n\\[\n\\hat{y_i} = 1.88 + 3.05 x_i\n\\]\nlets confirm this\n\nlm(y~x, data)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n      1.881        3.059  \n\n\nLets calculate the residual sum of squares, residual sum of errors, standard errors, and confidence intervals\n\ndata %&gt;% \n  mutate(\n    y_hat = beta_0_hat_data + beta_1_hat_data * x\n  ) -&gt; data\n\n\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n\n[1] 82.28514\n\n\n\nRSE_data = sqrt((RSS_data/(length(data$y) -2)))\nRSE_data\n\n[1] 0.9163211\n\n\nso \\(\\sigma_{data} = 0.9163\\)\nWe can now compute the standard errors of estiamed coefficients\n\nse_beta_0_data = sqrt(((1/length(data$y)) + (mean(data$x)^2 / sum((data$x - mean(data$x))^2))) * RSE_data^2)\nse_beta_0_data\n\n[1] 0.09179903\n\n\n\nse_beta_1_data = sqrt(RSE_data^2 / (sum((data$x - mean(data$x))^2)))\nse_beta_1_data\n\n[1] 0.07608457\n\n\nLets confirm\n\nsummary(lm(y~x,data))\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.88077    0.09180   20.49   &lt;2e-16 ***\nx            3.05893    0.07608   40.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,    Adjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSo we can say that on average our \\(\\hat{\\beta_0}\\)s are 0.091 differ from \\(\\beta_0\\), and our \\(\\hat{\\beta_1}\\)s differ 0.076 from \\(\\beta_1\\). To make more sense of it we can calculate the confidence intervals\n\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_0_hat_data - 1.96 * se_beta_0_data, \"and\", beta_0_hat_data + 1.96 * se_beta_0_data )\n\nBy 95% confidence we can say that the true beta_0 is between 1.700846 and 2.060698\n\n\n\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_1_hat_data - 1.96 * se_beta_1_data, \"and\", beta_1_hat_data + 1.96 * se_beta_1_data)\n\nBy 95% confidence we can say that the true beta_0 is between 2.909799 and 3.208051\n\n\nLets confirm this\n\nconfint(lm(y~x,data))\n\n               2.5 %   97.5 %\n(Intercept) 1.698600 2.062944\nx           2.907938 3.209912\n\n\nLets plot this confidence interval\n\nlm(y~x,data) %&gt;% \n  dwplot(ci = 0.95,dot_args = list(size=2), vline = geom_vline(xintercept = 0, color = \"grey50\", linetype =2))\n\n\n\n\nSince standard error tells us the range of the \\(\\beta\\) values via confidence interval, we can infer that if this range does not include 0, than our \\(\\beta\\) values are statistically significant; x is assocaited with y.\nLets do this for advertising data as well\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  tidy() %&gt;% \n  dwplot(ci = 0.95,dot_args = list(size=2), vline = geom_vline(xintercept = 0, color = \"grey50\", linetype =2))\n\n\n\n\nthe 95% confidence interval does not include 0; TV is statistically significant\n\n\n\n\nOr we can use standard erros to perform hypothesis tests on the coefficients. We usually don’t care about the intercept, so lets do the hypothesis test on only \\(\\hat{\\beta_1}\\).\n\\[\n\\begin{align}\nH_0 &: \\beta_1 = 0 \\to \\text{there is no relationship between X and Y} \\\\\nH_1 &: \\beta_1 \\neq 0 \\to \\text{there is some relationship between X and Y}\n\\end{align}\n\\] If the null-hypothesis is true =&gt; $ _1 = 0$ =&gt; \\(Y = \\beta_0 + \\epsilon\\) =&gt; \\(X\\) is not associated with \\(Y\\).\nTo test the null-hypothessi, we need to determine whether our estimate \\(\\hat{\\beta_1}\\) is sufficiently far from zero that we can be confident that \\(\\beta_1\\) is non-zero. How far is enough? This depends on the accuracy of \\(\\hat{\\beta_1}\\)–that is it depends on \\(\\text{SE}(\\hat{\\beta_1})\\). If \\(\\text{SE}(\\hat{\\beta_1})\\) is small, then even relatively small values of \\(\\hat{\\beta_1}\\) may provide strong evidence that \\(\\beta_1 \\neq 0\\). If \\(\\text{SE}(\\hat{\\beta_1})\\) is large, then \\(\\hat{\\beta_1}\\) must be large in absolute value in order for us to reject the null hypothessis. In practice we compute a t-statistic given by\n\\[\nt = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}\n\\] (3.14)\nwhich measures the number of standard deviations that \\(\\hat{\\beta_1}\\) is away from zero. From the t-statistic we can compute the p-value; a small p value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in absence of any real association between the predictor and the response. So if p value is small we infer that there is assocaition between the predictor and the response =&gt; we reject the null hypothesis. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1%. When \\(n=30\\) these correspond to tstatsitcs of around 2 and 2.75.\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales~TV,advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  confint() %&gt;% \n  kableExtra::kable(format = \"latex\")\n\n\n\n\nHere we see that t statistics are very high, and p values are very low =&gt; reject the null hypothesis for both \\(\\beta\\) values; they are statistically significant.\n\n\n2.1.3 Assessing the Accuracy of the Model\nOnce we concluded the statistically significant variable–rejecting the null hypothesis, we want to quantify the extend to which the model fits the data. We can use either\n\nResidual standard error\n\\(R^2\\)\n\nResidual standard error\nRecall from \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) that associated with each observation is an error term \\(\\epsilon\\). Because of these error terms even if we knew the true regression line, we would not be able to predict \\(Y\\) from \\(X\\). The RSE is an estimate of the standard deviation of \\(\\epsilon\\). It is the average amount that the response will deviate from the true regression line, computed by\n\\[\n\\begin{align}\n\\text{RSE} &= \\sqrt{\\frac{1}{n-2}\\text{RSS}} \\\\\n&= \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2}\n\\end{align}\n\\] (3.15)\nIn the advertising data, RSE was 3.26; actual sales in each market deviate from the true regression line by approximately 3,269 units, on average. This also means that; if the model were correct and the true values of the unknown coefficients \\(\\beta_0\\) and \\(\\beta_1\\) were known exaclty, any predcition of sales on the basis of TV advertising would still be off by about 3,260 units on average. Is this prediction error accaptable? Depends on the data: in the advertising data set the mean value of sales is \\(\\approx 14,000\\) units, and so the percentage error is \\(3,260 / 14,000 = 23%\\).\nThe RSE is considered a measure of the lack of fit of the model \\(Y=\\beta_0 + \\beta_1 + \\epsilon\\) to the data. If the predictions from the model are very close to the true outcome values–\\(\\hat{y_i} \\approx y_i\\) then RSE will be small, and we can concldue that the model fits the data very well. Otherwise, if \\(\\hat{y_i}\\) is very far from \\(y_i\\) then RSE may be quite large, indicating the model doesn’t fit the data well.\n\\(R^2\\)Statistic\nThe RSE provides an absolute measure of lack of fit of the model to the data. \\(R^2\\) provides an alternative measure of fit. It takes the form of a proportion–the proportion of variance explained–and so its always \\(0\\leq R^2 \\leq 1\\) and is independent of the scale of \\(Y\\)–as opposed to RSE.\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\] (3.17)\nwhere \\(\\text{TSS} = \\sum(y_i - \\bar{y})^2\\) is the total sun of squares and \\(\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\\). TSS measures the total variaance in the response \\(Y\\); and can be thought of as the amount of varaiblity ingerent in the response before the regression is performed. RSS measures the amount of varaiblity that is left unexplained after performing the regression. So TSS - RSS measures the amount of variability in the response that is explained by performing the regression, and \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\). As \\(R^2\\) gets closer to 1, a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variablity in the response; this might occur because the linear model is wrong, or the inherit error \\(\\sigma^2 = \\text{RSE}^2\\) is high, or both.\nLets calculate \\(R^2\\) of our estimation on advertising data with the model \\(\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}TV_i\\)\n\nadvertising %&gt;% \n  mutate(sales_hat = beta_0_hat_adv + beta_1_hat_adv * TV) -&gt; advertising\nadvertising\n\n# A tibble: 200 × 5\n      TV radio newspaper sales sales_hat\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 230.   37.8      69.2  22.1     18.0 \n 2  44.5  39.3      45.1  10.4      9.15\n 3  17.2  45.9      69.3   9.3      7.85\n 4 152.   41.3      58.5  18.5     14.2 \n 5 181.   10.8      58.4  12.9     15.6 \n 6   8.7  48.9      75     7.2      7.45\n 7  57.5  32.8      23.5  11.8      9.77\n 8 120.   19.6      11.6  13.2     12.7 \n 9   8.6   2.1       1     4.8      7.44\n10 200.    2.6      21.2  10.6     16.5 \n# ℹ 190 more rows\n\n\n\nRSS = sum((advertising$sales - advertising$sales_hat)^2)\nRSS\n\n[1] 2102.531\n\n\n\nRSE = sqrt(RSS/(length(advertising$sales) -2))\nRSE\n\n[1] 3.258656\n\n\n\nTSS = sum((advertising$sales - mean(advertising$sales))^2)\nTSS\n\n[1] 5417.149\n\n\n\nR2 = (TSS - RSS) / TSS\nR2\n\n[1] 0.6118751\n\n\n61% of the variablity in sales is explained by a linear regression on TV.\nwhat about our data\n\ndata\n\n# A tibble: 100 × 3\n        y      x  y_hat\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.591 -0.667 -0.159\n 2  2.69   0.222  2.56 \n 3 -2.61  -1.03  -1.27 \n 4 -3.54  -1.39  -2.38 \n 5  1.54  -0.545  0.212\n 6  2.22   0.384  3.05 \n 7 -1.34  -1.56  -2.88 \n 8  6.81   1.39   6.14 \n 9  6.26   1.43   6.27 \n10  2.39   0.465  3.30 \n# ℹ 90 more rows\n\n\n\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n\n[1] 82.28514\n\n\n\nRSE_data = sqrt(RSS_data/(length(data$y) -2))\nRSE_data\n\n[1] 0.9163211\n\n\n\nTSS_data = sum((data$y - mean(data$y))^2)\nTSS_data\n\n[1] 1439.473\n\n\n\nR2_data = (TSS_data - RSS_data) / TSS_data\nR2_data\n\n[1] 0.9428366\n\n\n94% of the variablity in y is explained by x; very good fit of the model.\n\\(R^2\\) is better to interpret than RSE."
  },
  {
    "objectID": "Chapter3.html#multiple-linear-regression",
    "href": "Chapter3.html#multiple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.2 Multiple Linear Regression",
    "text": "2.2 Multiple Linear Regression\nIn practice we have more than one predictor to explain \\(Y\\).\nHow can we extend our analysis of the advertising order to accomodate the other two (radio and newspaper) additional predictors?\n=&gt; We can run three separate simple linear regressions, each of which uses a different advertising medium as a predictor:\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ radio, advertising))\n\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.31164    0.56290  16.542   &lt;2e-16 ***\nradio        0.20250    0.02041   9.921   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\nWe find that on average, $1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\nWe find that on average, $1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\nWe find that on average, $1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\nHowever this approach is not good. First of all it is unclear to make a sinlge prediction of sales given levesl of the three advertising media budgets, since each has their own regression equation. Second, each of these three regression equations ignores the other two medi in forming estimates for the regression coefficients. Especially if these media budgets are correalted, this can lead to very misleading estimates of the individaul media effects on sales.\nInstead of the seperate linear regressions for each predictor, better approach is to extend the simple linear regression setting \\(Y = \\beta_0 + \\beta_1 X\\) to\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon\n\\]\n(3.19)\nHere we interpret \\(B_j\\) as the average effect of a one unit increase in \\(X_j\\), holding all other predictors fixed.\nFor the advertising example;\n\\[\nsales = \\beta_0 + \\beta_1 TV + \\beta_2 radio + \\beta_3 newspaper + \\epsilon\n\\]\n\n2.2.1 Estimating the Regression Coefficients\nAgain, regression coefficients in (3.19) are unknown, and must be estimated from the data. And with these estimates we can make predictions\n\\[\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\dots + \\hat{\\beta_p}x_p\n\\] (3.21)\nThe parameters are estimated using the same least squares approach with simple linear regression. We choose \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) to minimize the sum of squared residuals\n$$ \\[\\begin{align}\n\n\\text{RSS} &= \\sum_{i = 1}^n(y_i - \\hat{y_i})^2 \\\\\n&= \\sum_{i = 1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_{i1} - \\beta_2x_{i2} - \\dots - \\beta_px_{ip})^2\n\n\\end{align}\\] $$ (3.22)\n\\(\\hat{\\beta_0}, \\hat{\\beta_1},\\dots, \\hat{\\beta_p}\\) values minimize RSS.\nWe are not going to calculate these estimates with our hands, R does that.\nLets see our model results with the three predictors.\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation: for a given amount of Tv and newspaper advertising, spending additional $1,000 on radio(TV)(newspaper) advertising leads to an increase in sales approximately by 189(46)(-1) units.\nIf we compare these effects with one predictor regressions\n\nWe find that on average, $1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\n\n\nWe find that on average, $1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\n\n\nWe find that on average, $1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\n\nFor tv and radio coefficients are similar, but for newspaper: from the simple linear regression coefficient of newspaper was significant, but in multiple linear regression it is not; p value is very high.\n\nconfint(lm(sales ~ TV + radio + newspaper, advertising))\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n\n\nIts confidence interval contains 0.\nThis difference between simple linear regression and multiple linear regression coefficients stems from the fact that in the simple regression, the slope term represents the average effet of a one dollar increase in newspaper advertising, ignoring other preditors such as tv and radio. In contrsat, in the multiple regression setting, the coefficient for newspaper represents the average effect of increeasing newapper spending by one dollar, while holding tv and radio fixed.\nDoes it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite? Yes!\nTake a look at this correlation matrix:\n\ncor(advertising[1:4])\n\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n\n\nWe can also make it a plot out of this:\n\ncorrplot(cor(advertising[1:4]), method = \"number\")\n\n\n\n\nNotice that correlation between radio and newspaper is 0.35. This reveals a tendency to spend more on newspaper advertising in markets where more is spent on radio advertising. Now suppose the multiple regression is correct and newspaper advertising has no direct impact on sales, but radio advertising does increase sales. Then in markets where we spend more on radio, our sales will tend to be higher, adn as our correaltion matrix shows, we also tend to spend more on newspaper advertising in those same markets. Hence, in a simple linaer regresion which only examines sales vs newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales. So newspaper sales are proxy for radio advertising; newspaper gets credit for the effect of radio on sales.\nThis is a very common issue. Consider running a regression of shark attack versus ice cream sales for data collected at a given beach community. We would see a positive relationship, similar to that seen between sales and newspaper. Of course ice creams doesnt cause shark attacks. In reality higher temperatures cause more people to visit the beach in trun results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature revals that, the former predictr is no longer significant after adjusting for temperature.\n\n\n2.2.2 Some important Questions\nWhen we perform MLR, we usually are interested answering a few important questions\n\nIs at least one of the predictors \\(x_1, x_2, \\dots, x_p\\) useful in predicting the response?\nDo all predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\n\n4 Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\nLets answer these questions:\n\nIs at least one of the predictors \\(x_1, x_2, \\dots, x_p\\) useful in predicting the response?\nIn SLR we simply checked whether \\(\\beta_1 = 0\\) or not. In MLR, we need to ask whether all of the regression coefficients are zero \\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\). So our null hypothesis is\n\\[\n\\begin{align}\nH_0 &: \\beta_1 = \\beta_2 = \\dots = \\beta_o = 0 \\\\\nH_\\alpha &: \\text{at least one} \\space B_j \\space \\text{is non-zero}\n\\end{align}\n\\] This hypothesis test is performed by computing the F-statistic,\n\\[\nF = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\n\\] (3.23)\nSo, if there is no relationship between the resposne and predictors, we expect F-statistic to take on value close to 1. if \\(H_\\alpha\\) is true then \\(F\\) should be greater than 1.\n\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nF statistic is 570 and is far from 1. But it is best to have a look at the p-value of the F statistic which is also very small.\nThis means that at least one of the media is associated with increase sales.\nIn (3.23) we are testing \\(H_0\\) that all the coefficients are zero. Sometimes we want to test that a particular subset of \\(q\\) of the coefficients are zero. This corresponesd to a null hypothesis\n\\[\n  H_0 : \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_p\n  \\]\nIn this case we fit a second model that uses all the variables except those last \\(q\\). suppose that residual sum of squares for that model is \\(RSS_0\\). Then the appropriate F-statistic is\n\\[\n  F = \\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}\n  \\] (3.24)\nOn advertising MLR we saw that newspaper is not significant from its p value. Then why do we need to look at the overall F-statistic? after all, it seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is realted to the respose. This is not true usually, especially when \\(p\\) is large.\nSo after estimating the model first look at the F-statistic, than to the individual t statistic p values.\n\nDo all predictors help to explain \\(Y\\), or is only a subset of the predictors useful? =&gt; Deciding on important variables\nAfter lookig at the F statistic, we can look at the individual p values. But if *p$ is large, we are going to make false discoveries.\nUsually not all predictors are associated with the response. This task of determining which predictors are associated with the response in order to fit a single model involving only those predictors is refered to as variable selection. Check out Chapter 6 for more detail. But here is a breif outline of some of the classical approaches.\nIdeally we want to perform variable selection by trying out a lot of different models, each containing different subset of the predictors. For instance if our \\(p=2\\) then we can consider four models\n\n\na model containing no variables\n\n\na model containing \\(x_1\\) only\n\n\na model containnig \\(x_2\\) only\n\n\na model containing \\(x1\\) and \\(x_2\\).\n\n\nWe can then select the best model out of all the models by looking at some statistics we can use to judge the quality of the model. These are\n\nMallow’s \\(C_p\\)\nAkaike information creterion (AIC)\nBayesian information criterion(BIC)\nadjusted \\(R^2\\)\n\nThese are discussed in more detail in chapter 6.\nWe can also determine which model is the best by plotting various model outputs, such as the residuals, in order to search for patterns.\nBut we cannot consider all models, especially when \\(p\\) is high. There are three classical approaches for this task:\n\nForward selection\n\nbegin with null model a model that contains an intercept but no predictors.\nThen fit p simple linear regressions and add to the null model the variable that results in the lowest RSS.\nThen add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n\nBackward selection\n\nPut all varaibles in the model.\nremove the least statistically significant predictor.\nestimate the new regression with \\(p-1\\) variable, remove the largest p-value predictor. This procedure continues until a stopping rule is reached =&gt; stop after all remaining variables have p value &lt; 0.02\n\nMixed selection\n\nCombination of forward selection and backward selection\nStart with no variables in the model\nadd the varaible that provides the best fit\nadd varaibles one-by-one\nat one point if the p-value for one of the variables in the model rises above a certain treshold, then we remove that variabel from the model.\nContinue untill all variables have sufficiently low p value, and all vairables in the model woudl have a large p-value if added to the model\n\n\nBackwar slecetion cannot be used if \\(p&gt;n\\), forward selection can always be used.\nHow well does the model fit the data? Model Fit\n\nTwo of the most common numerical measures of model fit are RSE and \\(R^2\\).\nIn SLR \\(R^2\\) is equal to \\(cor(Y,X)\\). In MLR \\(R^2 = cor(Y,\\hat{Y})\\).\n\\(R^2\\) will always increase as you add more variable, even though that varaible is not statistically significant. This is because adding another variable must allow us to fit the trainig data(not necessarly test data) more accurately. But this increase in \\(R^2\\) after adding a statistically not-significant varible is very low =&gt; evidence that you can drop the not significant variable. Check out the \\(R^2\\) variables of the following models\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nThey are almost the same.\nBut lets see the \\(R^2\\) of the model containing only Tv\n\nsummary(lm(sales ~ TV, advertising))\n\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***\nTV          0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nit is 0.611.\nIf we add radio\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nIt increaes dramatically. This implies that model that uses TV and radio to predict sales is better than only using Tv. also radio is statistically signifiacnt.\n\nsummary(lm(sales ~ TV + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6231 -1.7346 -0.0948  1.8926  8.4512 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.774948   0.525338  10.993  &lt; 2e-16 ***\nTV          0.046901   0.002581  18.173  &lt; 2e-16 ***\nnewspaper   0.044219   0.010174   4.346 2.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 197 degrees of freedom\nMultiple R-squared:  0.6458,    Adjusted R-squared:  0.6422 \nF-statistic: 179.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nNot with newspaper though.\nOr the opposite\n\nsummary(lm(sales ~ radio, advertising))\n\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.31164    0.56290  16.542   &lt;2e-16 ***\nradio        0.20250    0.02041   9.921   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales~radio+TV, advertising))\n\n\nCall:\nlm(formula = sales ~ radio + TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ radio + newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5289  -2.1449   0.7315   2.7657   7.9751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.188920   0.627672  14.640   &lt;2e-16 ***\nradio       0.199045   0.021870   9.101   &lt;2e-16 ***\nnewspaper   0.006644   0.014909   0.446    0.656    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.284 on 197 degrees of freedom\nMultiple R-squared:  0.3327,    Adjusted R-squared:  0.3259 \nF-statistic: 49.11 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nWhat about RSE:\n\nsummary(lm(sales ~ TV + radio, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nadding newspaper increased the RSE =&gt; no need to add newspaper. Adding newspaper increases RSE because\n\\[\nRSE = \\sqrt{\\frac{1}{n-p-1}RSS}\n\\]\nModels with more variables can have higher RSE if the decrease in RSS is small relative to the increase in \\(p\\).\nSo we can look at both the RSE and \\(R^2\\).\nFour: Predictions\nAfter fitting the model we can predict \\(Y\\) =&gt; \\(\\hat{y}\\) with estimated coefficients. However, there are three sorts of uncertainty associated with this prediction:\n\nThe coefficient esstimates \\(\\hat{\\beta_0}, \\hat{\\beta_1},\\dots,\\hat{\\beta_p}\\) are estimates for \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\):\nThat is, the least squares plane\n\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\dots +  \\hat{\\beta_p}x_p\n\\]\nwhich is only an estimate for the true population regression plane \\[\nf(X) = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p\n\\]\nSo there is an inaccuracy in the coefficient estimates =&gt; this is the reducible error from Chapter 2. We can compute a confidence interval to determine how close \\(\\hat{y}\\) will be to \\(f(X)\\).\n\nAssuming a linear model for \\(f(X)\\) is almost always an aaproximation of reality (usually relationships are not linear), so ther is an additional source of potentially reducible error =&gt; this is the model bias.\nWhen we are using a linear model, we are in fact estimating the best linear approximation to the true surface. However, we will ignore this discrepancy and operate as if the linear model is correct\nEven if we knew \\(f(X)\\)–that is even if we knew the true values of \\(\\beta\\)–the response value cannot be predicted perfectly because of the random error \\(\\epsilon\\) in the model =&gt; irreducable error. How much will \\(Y\\) vary from \\(\\hat{y}\\) =&gt; we use prediction intervals to answer this question.\nPredicion intervals are always wider than confidence intervals, because they contain both the reducible error(error from estimating coefficients of \\(f(X)\\)) and irreducible error.\n\nWe use a confidence interval to quantify the uncertainty surrounding the average sales over a large number of cities. For example given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in each city, the 95% confidence interval is \\([10,985, 11,528]\\). We interpret this to mean that 95% of intervals of this form will contain the true value of \\(f(X)\\).\nOn the other hand, a prediction interval can be used to quantify the uncertainty surrounding sales for a particular city. Given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in that city the 95% prediction interval is \\([7,930, 14,580]\\). We interpret this to mean that 95% of intervals of this form will contain the true value of \\(Y\\) for this city. Note that both intervals are centered at 11,256, but that the prediction intervaş is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations."
  },
  {
    "objectID": "Chapter3.html#other-considerations-in-the-regression-model",
    "href": "Chapter3.html#other-considerations-in-the-regression-model",
    "title": "2  Linear Regression",
    "section": "2.3 Other Considerations in the Regression Model",
    "text": "2.3 Other Considerations in the Regression Model\n\n2.3.1 Qualitative Predictors\nIn practice not all variables are quantitative; some predictors are qualitative.\nCheck out the Credit data set\n\nCredit = read.csv(\"./data/Credit.csv\") %&gt;% as_tibble\nCredit\n\n# A tibble: 400 × 12\n      ID Income Limit Rating Cards   Age Education Gender   Student Married\n   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  \n 1     1   14.9  3606    283     2    34        11 \" Male\"  No      Yes    \n 2     2  106.   6645    483     3    82        15 \"Female\" Yes     Yes    \n 3     3  105.   7075    514     4    71        11 \" Male\"  No      No     \n 4     4  149.   9504    681     3    36        11 \"Female\" No      No     \n 5     5   55.9  4897    357     2    68        16 \" Male\"  No      Yes    \n 6     6   80.2  8047    569     4    77        10 \" Male\"  No      No     \n 7     7   21.0  3388    259     2    37        12 \"Female\" No      No     \n 8     8   71.4  7114    512     2    87         9 \" Male\"  No      No     \n 9     9   15.1  3300    266     5    66        13 \"Female\" No      No     \n10    10   71.1  6819    491     3    41        19 \"Female\" Yes     Yes    \n# ℹ 390 more rows\n# ℹ 2 more variables: Ethnicity &lt;chr&gt;, Balance &lt;int&gt;\n\n\nLets do a scatterplot of all variables\n\nCredit %&gt;% select(where(is.numeric)) %&gt;% pairs(.)\n\n\n\n\nFig 3.6 The credit data set contains infortmation about blaance, age, cards, education income, limit and rating for a number of potential customers\n\n\n\n\nPredictors with Only Two Levels\nWe want to investigate differences in credit card balance between maels and females, ignoring other variables for the moment. If a qualitative predictors (also known as factor) only has two levels, then incorporating it into a regression model is very simple. We create a dummy variable that takes on two possible numerical values. For example based on Gender varaible, we can create a new varaible that takes the form\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if}\\space i\\text{th} \\space\\text{person is female} \\\\\n0 & \\text{if}\\space i\\text{th} \\space\\text{person is male}\n\\end{cases}\n\\] (3.26)\nand use this variable as a predictor in the regression equation. This results in the model\n\\[\nY_i = \\beta_0 + \\beta_1x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & i\\text{th} \\space\\text{person is female} \\\\\n\\beta_0 + \\epsilon_i & i\\text{th} \\space\\text{person is male}\n\\end{cases}\n\\] (3.27)\nNow \\(\\beta_0\\) can be interpreted as the average credit card balance among males, \\(\\beta_0 + \\beta_1\\) as the average credit card among females, and \\(\\beta_1\\) as the average difference in credit card balance between females and males.\nHere is the regression results:\n\nCredit$Gender &lt;- factor(Credit$Gender, levels = c(\" Male\",\"Female\"))\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Gender, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = Balance ~ Gender, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    509.80      33.13  15.389   &lt;2e-16 ***\nGenderFemale    19.73      46.05   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n\n\nR converts all “Female” values to 1 automatically. If we were to do this manually\n\nCredit %&gt;% \n  mutate(Gender = as.character(Gender)) %&gt;% \n  mutate(Gender = ifelse(Gender == \"Female\",1,0)) %&gt;% \n  lm(Balance ~ Gender, .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   509.80      33.13  15.389   &lt;2e-16 ***\nGender         19.73      46.05   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n\n\nThis means that average credit card debt for males is estimated to be $509.80, whereas females are estimated to carry $19.73 in additional debt for a total of \\(\\$509.80 + \\$19.73 = \\$529.53\\). However, the coefficient of the dummy variable is not significant; there is no statistical evidence of a difference in average credit card balance between the genders.\n\nCredit %&gt;% \n  ggplot() + aes(x=Gender, y =Balance, fill = Gender) + geom_boxplot(show.legend = F) + geom_point(data =(Credit %&gt;% group_by(Gender) %&gt;% summarise(Balance = mean(Balance))),shape = 4, show.legend = F) + theme_clean()\n\n\n\n\nThe desicion to code females as 1 and males as 0 in (3.27) is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we had coded males as 1 and females as 0:\n\nCredit %&gt;% \n  mutate(Gender = factor(Gender, levels = c(\"Female\", \" Male\"))) %&gt;% \n  lm(Balance ~ Gender,.) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   529.54      31.99  16.554   &lt;2e-16 ***\nGender Male   -19.73      46.05  -0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n\n\nThen we would say that estimated average debt for females is $529.54, and for males is \\(\\$529.52 - \\$19.73 = \\$509.80\\).\nAlternatively, instead of 0/1 coding scheme, we could create a dummy variable\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is female} \\\\\n-1 & \\text{if}\\space i\\text{th}\\space \\text{person is male}\n\\end{cases}\n\\] and use this variable in the regression equation. This results in the model\n\\[\nY_i = \\beta_0 + \\beta_1x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is female} \\\\\n\\beta_0 - \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is male}\n\\end{cases}\n\\] Now \\(\\beta_0\\) can be interpreted as the overall average credit card balance (ignoring the gender effect), and \\(\\beta_1\\) is the amount that females are above the average and males are below the average.\n\nCredit %&gt;% \n  mutate(Gender = as.character(Gender)) %&gt;% \n  mutate(Gender = ifelse(Gender == \"Female\",1,-1)) %&gt;% \n  lm(Balance ~ Gender,.) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  519.670     23.026  22.569   &lt;2e-16 ***\nGender         9.867     23.026   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611, Adjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n\n\nNow \\(\\beta_0\\) is $ 519.670 which is the halfway between the male and female averages of $509.80 and $529.53. The estimate for \\(\\beta_1\\) is $9.865, which is half of $19.74, the average difference between females and males.\nQualitative Predictors with More than Two levels\nIn this case we need to create an additional dummy. For example have a look at the Ethnicity variable\n\nCredit %&gt;% select(Ethnicity) %&gt;% unique()\n\n# A tibble: 3 × 1\n  Ethnicity       \n  &lt;chr&gt;           \n1 Caucasian       \n2 Asian           \n3 African American\n\n\nHas three possible values. Then we need to create two dummies\n\\[\nx_{i1} =\n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is Asian} \\\\\n0 & \\text{if}\\space i\\text{th}\\space \\text{person is not Asian}\n\\end{cases}\n\\]\nand\n\\[\nx_{i2} =\n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is Caucasian} \\\\\n0 & \\text{if}\\space i\\text{th}\\space \\text{person is not Caucasian}\n\\end{cases}\n\\]\nThen both these varaibles can be used in the regression equation, in order to obtain the model\n\\[\nY_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is Asian} \\\\\n\\beta_0 + \\beta_2 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is Caucasian} \\\\\n\\beta_0 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is African American}\n\\end{cases}\n\\] Now \\(\\beta_0\\) can be interpreted as the average credit card balance for African Americans, \\(\\beta_1\\) can be interpreted as the difference in the average balance between Asian and African american categories, and \\(\\beta_2\\) can be interpreted as the difference in average balance between the Caucasian and African American categories.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Ethnicity, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = Balance ~ Ethnicity, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-531.00 -457.08  -63.25  339.25 1480.50 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          531.00      46.32  11.464   &lt;2e-16 ***\nEthnicityAsian       -18.69      65.02  -0.287    0.774    \nEthnicityCaucasian   -12.50      56.68  -0.221    0.826    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.9 on 397 degrees of freedom\nMultiple R-squared:  0.0002188, Adjusted R-squared:  -0.004818 \nF-statistic: 0.04344 on 2 and 397 DF,  p-value: 0.9575\n\n\nThere will always be one fewer dummy variable than the number of levels. The level with no dummy variable–African American in this example–is known as the baseline.\nFrom the regression results we see that the estimated balance for the baseline, African American, is $531.00. It is estimated that Asian category will have $18.69 less debt than the African American category on average, and the Caucasian category will have $12.50 less debt that the African American category. However, p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities. The coefficients will change bassed on the baseline and coding.\nRather than relying on the individual coefficients, we can use F-test to test \\(H_0 : \\beta_1 = \\beta_2 = 0\\); this does not depend on the coding. The F-test has a p-value of 0.9575, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.\nUsing this dummy variable approach presents no difficulties when using both quantitative and qualitative predictors. For example, to regress balance on both a quantitative varaible such as income and a qualitative variable such as student, we must simply create a dummy varaible for student and then fit a multiple linear regression model using income and the dummy variable as the predictors for credit card balance.\n\n\n2.3.2 Extensions of the Linear Model\nLinear regression model is very interpretable and works quite well on many real-world problems. However, it makes several highly restrictive assumptions that are ofthen violated in practice. Two of them ost important assumptions state that the relationship between the predictors and response are additive and linear.\nThe additive assumption means that the effect of changes in a predictor \\(x_j\\) on the response \\(Y\\) is independent of the vlaues of the other predictors.\nThe linear assumption means thatt the change in the response \\(Y\\) due to one-unit change in \\(x_j\\) is constant, regardless of the vlaue of \\(x_j\\).\nWe can relax these assumptions. Here some classical approaches to do that\nRemoving the Additive Assumption\nFrom Advertising data, we concluded that both TV and radio seem to be associated with sales. Our model was linear; the effect of TV and radio advertising spending on sales is independent of each other, and their effect is constant no matter the level of spending.\n\\[\nY_i = \\beta_0 + \\beta_1 TV_i + \\beta_2 radio_i + \\epsilon_i\n\\] this model means that, the average effect on sales of a one unit increase in tv is always \\(\\beta_1\\) regardless of the amount spent on radio.\nHowever, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases. In this situation, given a fixed budget $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. In marketing this is known as the synergy effect, in statistics interaction effect.\nConsider the standard linear regression with two variables,\n\\[\nY = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\n\\] Here if we increaes \\(x_1\\) by one unit, \\(Y\\) will increase by an average of \\(\\beta_1\\) units. The presence of \\(x_2\\) does not alter this statement-regardless of the value of \\(x_2\\), a one-unit increase in \\(x_1\\) will lead to \\(\\beta_1\\) unit increae in \\(Y\\).\nWe can extend this model by allowing interaction effects by including a third predictor, called an interaction term, which is constructed by computing the product of \\(x_1\\) and \\(x_2\\):\n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\n\\] (3.31)\nWe can write this equation\n\\[\n\\begin{align}\nY &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2x_2 +\\epsilon \\\\\n&= \\beta_0 + \\tilde{\\beta_1}x_1 + \\beta_2x_2 + \\epsilon\n\\end{align}\n\\] where \\(\\tilde{\\beta_1} = \\beta_1 + \\beta_3x_2\\). Since \\(\\tilde{\\beta_1}\\) changes with \\(x_2\\), the effect of \\(x_1\\) on \\(Y\\) is no longer constant: adjusting \\(x_2\\) will change the impact of \\(x_1\\) on \\(Y\\).\n\n\n\n\n\n\nProductivy of a factory\n\n\n\nSuppose that we are interested in studying the productivity of a factory. We want to predict the number of units produced on the basis of the number of production lines and the total number of workers. Probably the effect of increasing lines on units produced will depend on the number of workers, since if no workers are available to operate the lines, then increasing the number of lines will not increase the production. This suggest to include an interaction term between lines and workers in a linear model to predict units.\nSuppose when we fit the model, we obtain\n\\[\n\\begin{align}\nunits &\\approx 1.2 + 3.4 \\times lines + 0.22 \\times workers + 1.4 \\times (lines \\times workers) \\\\\n&= 1.2 + (3.4 + 1.4 \\times workers) \\times lines + 0.22 \\times workers\n\\end{align}\n\\] Adding an addtional line will increase the number of units produced by \\(3.4 + 1.4 \\times workers\\). Hence, the more workers we have, the stronger will be the effect of lines.\n\n\nFor the advitising, a linear model that uses radio, TV and interaction between the two to predict sales takes the form\n\\[\n\\begin{align}\nsales &= \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times(radio \\times TV) + \\epsilon \\\\\n&= \\beta_0 + (\\beta_1 + \\beta_3 \\times radio)\\times Tv + \\beta_2 \\times radio + \\epsilon\n\\end{align}\n\\] (3.33) We can interpret \\(\\beta_3\\) as the increase in the effectiveness of TV advertising for a one unit increse in radio advertising (or vice versa). The coefficients that result from fitting the model (3.33) are\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio + TV*radio, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = sales ~ TV + radio + TV * radio, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.750e+00  2.479e-01  27.233   &lt;2e-16 ***\nTV          1.910e-02  1.504e-03  12.699   &lt;2e-16 ***\nradio       2.886e-02  8.905e-03   3.241   0.0014 ** \nTV:radio    1.086e-03  5.242e-05  20.727   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nAdding the interaction term increased the \\(R^2\\) significantly from 0.8972 to 0.9678, RSE from 1.681 to 0.9435.\nThe p-value for the interaction term is very low; \\(H_\\alpha : \\beta_3 \\neq 0\\). This means that the relationship is not additive.\nTo interpret the coefficients:\n$1,000 increse in TV advertising results with \\(\\hat{\\beta_1} +\\hat{\\beta_3}\\times radio = \\$19.1 + \\$1.09 \\times radio\\) increase in sales on average.\n$1,000 invrease in radio advertising results with \\(\\hat{\\beta_2} + \\hat{\\beta_3} \\times TV = \\$28.9 + \\$1.09 \\times TV\\) increase in sales on average.\nAll p-values are significant =&gt; all three varibles should be included in the model.\nSometimes interaction term has a very small pvalue but the associated main effects(in this case TV and radio) do not. The hieararchial principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\nHere both our varaibles were quantitative. However, the consept of interaction applies to qualitative varaibles, or combination of qualitative and quantitative varibles. Actually interaction between a quantitative and qualitative variable has a particularly nice interpreateion:\nConsider the Credit data set. Suppose we want to predict balance using income(quantitative) and student(qualitative) varaibles. In the absence of an interaction term the model takes the form\n\\[\nbalance_i  \\approx \\beta_0 + \\beta_1 \\times income_i +\n\\begin{cases}\n\\beta_2 & \\text{if} \\space i\\text{th} \\space \\text{person is student} \\\\\n0 & \\text{if} \\space i\\text{th} \\space \\text{person is not student}\n\\end{cases}\n\\]\n\\[\nbalance_i = \\beta_1 \\times income +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if} \\space i\\text{th} \\space \\text{person is student} \\\\\n\\beta_0 &\\text{if} \\space i\\text{th} \\space \\text{person is student}\n\\end{cases}\n\\] (3.34)\nNotice that this amounts to fitting two parallel lines to the data, one for students and one for non-students. The lines for students and non students have different intercepts, \\(\\beta_0 + \\beta_2\\) versus \\(\\beta_0\\), but the slope \\(\\beta_1\\) is the same.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Income + Student, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = Balance ~ Income + Student, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-762.37 -331.38  -45.04  323.60  818.28 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 211.1430    32.4572   6.505 2.34e-10 ***\nIncome        5.9843     0.5566  10.751  &lt; 2e-16 ***\nStudentYes  382.6705    65.3108   5.859 9.78e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 391.8 on 397 degrees of freedom\nMultiple R-squared:  0.2775,    Adjusted R-squared:  0.2738 \nF-statistic: 76.22 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\nAverage Non student balance is $211.1430; students have $382.6705 more balance on average compared to non students. $1,000 increase in income increases balance by $5,984 on average. All coefficients are statistically significant. \\(R^2 = 0.2775\\)\n\nCredit %&gt;% \n  ggplot() + aes(y=Balance, x=Income) + geom_point(shape =NA) + geom_abline(intercept = 211.1430, slope = 5.9843, color = \"black\", size=1.2) + geom_abline(intercept = 593.8135, slope = 5.9843, color = \"darkred\", size= 1.2) + coord_cartesian(xlim = c(0,150), ylim=c(200,1400)) + scale_y_continuous(breaks = seq(0,1400,400)) + theme_clean() + geom_text(data = tibble(x = c(80,80), y =c(1130,640), z = c(\"Student\",\"non-student\")), inherit.aes = F, aes(x=x, y=y, label=z), angle=25.9) -&gt; p1\np1\n\n\n\n\nNotice the since the slopes are same, two lines are parallel; average effect of income on balance does not depend on whether or not the individual is a student. This is a limitation; change in income may have a very different effect on the credit card balance of a student versus non student.\nThis limitation can be addressed by adding an interaction varaible, created by multiplying student and income with the dummy for student. Our model now becomes\n\\[\nbalance_i \\approx \\beta_0 + \\beta_1 \\times income_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times income_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\] \\[\nbalance_i =\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times income_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times income_i & \\text{if not student}\n\\end{cases}\n\\] (3.35)\nAgain, our intercepts differ based on whether individual is student or not. However, this time slope also differs!\nLets estimate this model\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Income + Student + Income * Student, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = Balance ~ Income + Student + Income * Student, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-773.39 -325.70  -41.13  321.65  814.04 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       200.6232    33.6984   5.953 5.79e-09 ***\nIncome              6.2182     0.5921  10.502  &lt; 2e-16 ***\nStudentYes        476.6758   104.3512   4.568 6.59e-06 ***\nIncome:StudentYes  -1.9992     1.7313  -1.155    0.249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 391.6 on 396 degrees of freedom\nMultiple R-squared:  0.2799,    Adjusted R-squared:  0.2744 \nF-statistic:  51.3 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\nThis results in\n\ntibble(\n  intercept = c(200.6232, 200.6232+476.6758),\n  income = c(6.2182, 6.2182 - 1.9992),\n  type = c(\"Non-student\", \"student\")\n)\n\n# A tibble: 2 × 3\n  intercept income type       \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1      201.   6.22 Non-student\n2      677.   4.22 student    \n\n\nand becomes\n\nCredit %&gt;% \n  ggplot() + aes(x = Income, y = Balance) + geom_point(shape =NA)+\n  geom_abline(intercept = 201, slope = 6.22, color = \"black\", size =1.2) +\n  geom_abline(intercept = 677, slope = 4.22, size = 1.2, color = \"darkred\") +  coord_cartesian(xlim = c(0,150), ylim=c(200,1400)) + scale_y_continuous(breaks = seq(0,1400,400)) + theme_clean() + geom_text(data = tibble(x = c(80,80), y =c(1130,640), z = c(\"Student\",\"non-student\")), inherit.aes = F, aes(x=x, y=y, label=z), angle=25.9) -&gt; p2\np2\n\n\n\n\nLets merge this two plots together\n\ngridExtra::grid.arrange(p1,p2, ncol =2)\n\n\n\n\nSlope for studens is lower than the slope for non-students =&gt; increases in income are associated with smaller increases in credit card balance among students as copared to non-students.\nNot-linear Relationships\nLinear model assumes linear relationship between the response and the predictors. However, this is often not true. We can relax this assumption using polynomial regression.\nConsider Auto\n\nAuto %&gt;% as_tibble() %&gt;% \n  ggplot() + aes(x=horsepower, y = mpg) + geom_point(shape =1, size =1.4) + \n  geom_smooth(method = \"lm\", se = F, color =\"#FEBF63\") +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x,2), color =\"#7FDBDA\", se = F) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x,5), color =\"#ADE498\", se=F) + theme_blank() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nlinear regresion fit is in orange, blue curve is a linear regression fit for a model that includes horsepower^2, and green curve includes up to horsepower^5\n\n\n\n\nIt is obvious that there is relationship between mpg and horsepower. But it seems like this relationship is not linear: the data suggest a curved relationship.\nA simple approach for incorporating non linear associations in a linear model is to include transformed versions of the predictors in the model. For example figure above seem to have a quadratic shape, suggesting that a model of the form\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\] May be a better fit. Equation\nYou can do it in different ways: 1.\n\nAuto %&gt;% \n  mutate(hp.sq = horsepower^2) %&gt;% \n  lm(mpg~horsepower + hp.sq,.) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = mpg ~ horsepower + hp.sq, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 56.9000997  1.8004268   31.60   &lt;2e-16 ***\nhorsepower  -0.4661896  0.0311246  -14.98   &lt;2e-16 ***\nhp.sq        0.0012305  0.0001221   10.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2), Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     56.9000997  1.8004268   31.60   &lt;2e-16 ***\nhorsepower      -0.4661896  0.0311246  -14.98   &lt;2e-16 ***\nI(horsepower^2)  0.0012305  0.0001221   10.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nsummary(lm(mpg ~ poly(horsepower,2, raw=T), Auto))\n\n\nCall:\nlm(formula = mpg ~ poly(horsepower, 2, raw = T), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   56.9000997  1.8004268   31.60   &lt;2e-16 ***\npoly(horsepower, 2, raw = T)1 -0.4661896  0.0311246  -14.98   &lt;2e-16 ***\npoly(horsepower, 2, raw = T)2  0.0012305  0.0001221   10.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nsummary(nls(mpg ~a + b * horsepower + c * horsepower^2, data = Auto, start = c(a=0,b=0,c=0)))\n\n\nFormula: mpg ~ a + b * horsepower + c * horsepower^2\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)    \na 56.9000997  1.8004268   31.60   &lt;2e-16 ***\nb -0.4661896  0.0311246  -14.98   &lt;2e-16 ***\nc  0.0012305  0.0001221   10.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 1.293e-09\n\n\n–\n\nsummary(lm(mpg ~ horsepower, Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(mpg ~ horsepower + I(horsepower^2),Auto))\n\n\nCall:\nlm(formula = mpg ~ horsepower + I(horsepower^2), data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7135  -2.5943  -0.0859   2.2868  15.8961 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     56.9000997  1.8004268   31.60   &lt;2e-16 ***\nhorsepower      -0.4661896  0.0311246  -14.98   &lt;2e-16 ***\nI(horsepower^2)  0.0012305  0.0001221   10.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.374 on 389 degrees of freedom\nMultiple R-squared:  0.6876,    Adjusted R-squared:  0.686 \nF-statistic:   428 on 2 and 389 DF,  p-value: &lt; 2.2e-16\n\n\nThe \\(R^2\\) of the quadratic fit is 0.688, compared to 0.606 for the linear fit. And the p-value is highly significant for quadratic model.\nIf adding \\(horsepower^2\\) increses the \\(R^2\\) why not include \\(^3\\) or \\(^4\\) or more?\nLets estimate the green curve on the previous plot\n\nsummary(lm(horsepower ~ poly(horsepower,5, raw=T),Auto))\n\nWarning in summary.lm(lm(horsepower ~ poly(horsepower, 5, raw = T), Auto)):\nessentially perfect fit: summary may be unreliable\n\n\n\nCall:\nlm(formula = horsepower ~ poly(horsepower, 5, raw = T), data = Auto)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-7.256e-13 -1.620e-15 -1.000e-16  3.540e-15  3.819e-14 \n\nCoefficients:\n                                Estimate Std. Error    t value Pr(&gt;|t|)    \n(Intercept)                    2.739e-13  2.468e-13  1.110e+00    0.268    \npoly(horsepower, 5, raw = T)1  1.000e+00  1.125e-14  8.887e+13   &lt;2e-16 ***\npoly(horsepower, 5, raw = T)2  1.496e-16  1.946e-16  7.690e-01    0.442    \npoly(horsepower, 5, raw = T)3 -1.196e-18  1.598e-18 -7.480e-01    0.455    \npoly(horsepower, 5, raw = T)4  4.368e-21  6.255e-21  6.980e-01    0.485    \npoly(horsepower, 5, raw = T)5 -5.894e-24  9.373e-24 -6.290e-01    0.530    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.736e-14 on 386 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 8.299e+31 on 5 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nThe green fit seems unnecassasrliy wiggly- that is it is unclear includig the additional terms really has led to a better fit to the data.\nThe approach we have just described for extending the linear model to accomodate non-linear relationships is known as polynomial regression, since we have included polynomial functions of the predictors in the regression model. Check out Chapter 7.\n\n\n2.3.3 Potential Problems\nWhen we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following\n\nNon-linearity of the response-predictor relationships.\nCorrealation of error terms\nNon-constant variance of error terms\nOutliers\nHigh-leverage points\nCollinearity\n\nLets breifly discuss each\n1.Non-linearity of the Data\nLinear regression assumes there is a straight line relationship between the predictors and the respose. If the true relationship is far from linear, then our estimations are not good. Also our prediction accuracy is reduced.\nResidual plots are a useful graphical tool for identifying non-linearity.\nWe can plot the residuals \\(e_i = y_i - \\hat{y_i}\\) versus \\(x_i\\) for Simple linear regression.\nFor multiple linear regression since we have lost of \\(x_i\\) we plot residuals versus predicted values(\\(e_i\\) vs \\(\\hat{y_i}\\)).\nWe want to have no visible pattern. If there is a pattern =&gt; problem!\nSo what do we expect a plot of “fitted” vs “resid”; if the relationship is linear as the “fitted” values increase “resid” should not increase or decrease, should swing with no relationship: linear.\nIf the relationship is not linear then we expect ot see a pattern because as the fitted values increase the resiudls will increase.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(mpg ~ horsepower, Auto) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() \n\n# A tibble: 392 × 9\n   .rownames   mpg horsepower .fitted .resid    .hat .sigma   .cooksd .std.resid\n   &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1            18        130   19.4  -1.42  0.00368   4.91   1.54e-4    -0.289 \n 2 2            15        165   13.9   1.11  0.00888   4.91   2.31e-4     0.227 \n 3 3            18        150   16.3   1.74  0.00613   4.91   3.91e-4     0.356 \n 4 4            16        150   16.3  -0.259 0.00613   4.91   8.66e-6    -0.0530\n 5 5            17        140   17.8  -0.838 0.00473   4.91   6.96e-5    -0.171 \n 6 6            15        198    8.68  6.32  0.0177    4.90   1.52e-2     1.30  \n 7 7            14        220    5.21  8.79  0.0256    4.89   4.33e-2     1.82  \n 8 8            14        215    6.00  8.00  0.0236    4.89   3.30e-2     1.65  \n 9 9            14        225    4.42  9.58  0.0276    4.89   5.57e-2     1.98  \n10 10           15        190    9.95  5.05  0.0152    4.91   8.31e-3     1.04  \n# ℹ 382 more rows\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(mpg ~ horsepower, Auto) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x=horsepower, y = .resid) + geom_point() + geom_smooth(se=F) + theme_light()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere we observe a U-shape trend; indicating non-linear relationship between horsepower and mpg.\nLets do a plot of fitted vs residual for linear and non linear regression:\n\ngridExtra::grid.arrange(\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(mpg ~ horsepower, Auto) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x=.fitted, y=.resid) + geom_point() + geom_smooth(se=F) + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_light()\n,\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(mpg ~ horsepower + I(horsepower^2), Auto) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x=.fitted, y=.resid) + geom_point() + geom_smooth(se=F) + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_light(), ncol=2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere is the plots. On the left fitted vs resid for regressing horsepower onto mpg, on the right regressing horsepower and horsepower^2 onto mpg.\nBlue line is a smooth fit to the residuals to identify trends easily. The residuals on the left panel exhibit a clear U shape, which is a strong indication that the true relationship is not linear.\nRight hand panel displays a little pattern, suggesting that the quadratic term improves the fit to the data.\nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as \\(\\log X\\), \\(\\sqrt{X}\\), and \\(X^2\\), in the regression model. We will discuss more later on.\n2.Correlation of Error Terms\nAn important assumption of the linear regression is that error terms \\(\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n\\) are uncorrelated. This means that \\(\\epsilon_{i+1} \\neq f(\\epsilon_i)\\). The standard errors that are computed for the estimated regression coeffieints or the fitted values are based on this assumption. If there is correaltion among the error terms, the estimated standard errors will tend to underestimate the true standard errors =&gt; confidence and prediction intervals will be narrower than they should be. P-values will be lower as well, causing false discoveries perhaps.\nUsually correlation among error terms occur in time series data. In many cases, observations that are obtained at adjacent time points will have positevely correalated errors. To test this we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then we expect no pattern. If the error terms are positevly correlated, then we may see tracking in the residuals–adjacent residuals may have similar values.\n Check out Figure 3.10. In the top panel, we see the residuals from a linear regression fit to data generated with uncorrelated errors. Adjacent years don’t take similar values; there is no evidence of a time-related trend in the residuals. But in the bottom panel, we see plotted residuals which have correlation 0.9 with time: now there is a clear pattern in the residuals–adjacent residuals tend to take on similar values. Finally, the center panel illustrates a more moderate case in which residuals had a correlation of 0.5. There is still evidence of tracking, but the pattern is less clear.\nCorrelation among the error terms can also occur outside of time series data. For instance, consider a study where we predict people’s hights from their weights. The assumption of uncorrelated errors could be violated if some of the individauls in the study are members of the same family, or eat the same diet, or have been exposed to same environmental factors.\n3.Non-constant Variance of Error Terms\nAnother important assumption of the linear regression model is that error terms have a constant variance, \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). The standard errors, confidence intervals, and hypothesis tests rely upon this assumption.\nUnfortunately, often variances of the error terms are non-constant =&gt; heteroscedasticity. For instance, variances of the error terms may increase with the value of the response. We can identify non-constant variances in the errors from the presence of a funnel shape in the residual plot.\n On the left panel magnitude of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response \\(Y\\) using a concave function such as \\(\\log Y\\) or \\(\\sqrt{Y}\\). Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. On the right hand panel we see that residuals apper to have constant variance.\n4.Outliers\nAn outlier is a point for which \\(y_i\\) is far from the vlaue predicted by the model.\n\nThe red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removel of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a minuscule reduction in the intercept. This is normal. However, it can cause other problems: RSE is 1.09 when outlier is included, but it is only 0.77 when excluded =&gt; RSE is used to calculate all confidence intervals, p values =&gt; bias. \\(R^2\\) also decreases from 0.829 to 0.805 when included outlier.\nResidual plots can be used to identify outliers =&gt; center panel of Figure 3.12. But it can be difficult to decide how large a residual needs to before we consider the point to be an outlier. To address this problem instead of plotting the residuals we can plot the studentized residuals computed by dividing each residuls \\(e_i\\) by its estimated standard error.\n\\[\ne_i^*=\\frac{e_i}{sd(e_i)} = \\frac{e_i}{\\sqrt{MSE(1-h_{ii})}}\n\\] \\(h_{ii}\\) is leverage point.\nObservations whose studentized residuals are greater than 3 in absolute value are possible outliers. In the right-hand panel of Figure 3.12 the outlier’s studentized residual exceeds 6; while all other observations have studentized residauls between -2 and 2.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales~TV + radio,advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  mutate(.std.resid_3 = ifelse(abs(.std.resid) &gt;=3,T,F), id = 1:200) %&gt;% \n  print %&gt;% \n  ggplot() + aes(x=.fitted, y =.std.resid, color = .std.resid_3) + geom_point() + scale_color_manual(values = c(\"black\",\"red\")) + \n  geom_hline(yintercept = 0, linetype=\"dashed\") +\n  geom_text(aes(label = ifelse(.std.resid_3 == T,id,\"\")), nudge_x = 0.5, show.legend = F)+ theme_light() \n\n# A tibble: 200 × 11\n   sales    TV radio .fitted  .resid    .hat .sigma   .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1  22.1 230.   37.8   20.6   1.54   0.0140    1.68 0.00406       0.925 \n 2  10.4  44.5  39.3   12.3  -1.95   0.0188    1.68 0.00871      -1.17  \n 3   9.3  17.2  45.9   12.3  -3.04   0.0295    1.67 0.0341       -1.83  \n 4  18.5 152.   41.3   17.6   0.883  0.0124    1.68 0.00117       0.528 \n 5  12.9 181.   10.8   13.2  -0.324  0.00951   1.69 0.000120     -0.194 \n 6   7.2   8.7  48.9   12.5  -5.31   0.0347    1.64 0.124        -3.22  \n 7  11.8  57.5  32.8   11.7   0.0818 0.0129    1.69 0.0000105     0.0490\n 8  13.2 120.   19.6   12.1   1.09   0.00576   1.68 0.000823      0.653 \n 9   4.8   8.6   2.1    3.71  1.09   0.0271    1.68 0.00401       0.658 \n10  10.6 200.    2.6   12.6  -1.95   0.0171    1.68 0.00797      -1.17  \n# ℹ 190 more rows\n# ℹ 2 more variables: .std.resid_3 &lt;lgl&gt;, id &lt;int&gt;\n\n\n\n\n\nor\n\nplot(lm(sales ~ TV + radio,advertising), which = 3)\n\n\n\n\nWe have spotted that 6th and 131 are possible outliers.\nHere is all the data regression result:\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = sales ~ TV + radio, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  car::outlierTest()-&gt; outs\nouts # 131th value is outlier\n\n     rstudent unadjusted p-value Bonferroni p\n131 -5.714235         4.0499e-08   8.0998e-06\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  car::outlierTest(., cutoff = 0.30)\n\n     rstudent unadjusted p-value Bonferroni p\n131 -5.714235         4.0499e-08   8.0998e-06\n6   -3.295069         1.1678e-03   2.3356e-01\n\n\nHere is regression result after removing outliers\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio, advertising[-c(6,131),]) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = sales ~ TV + radio, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3938 -0.8195  0.2003  1.0785  2.8134 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.051908   0.265279   11.51   &lt;2e-16 ***\nTV          0.044220   0.001269   34.84   &lt;2e-16 ***\nradio       0.195295   0.007315   26.70   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.511 on 195 degrees of freedom\nMultiple R-squared:  0.9146,    Adjusted R-squared:  0.9138 \nF-statistic:  1045 on 2 and 195 DF,  p-value: &lt; 2.2e-16\n\n\n\\(R^2\\) has increased, residual standard errors has decreased, standard errors of estimates decreased and coefficients has changed.\n5.High Leverage Points\nOutliers are observations for which the response \\(y_i\\) is unusual given the predictor \\(x_i\\). In contrast, observations with high leverage have an unusual value for \\(x_i\\).\n\nFor example observation 41 in the left-hand panel of Figure 3.13 has high leverage =&gt; predictor value for this observation is large relative to other observations. The red solid line is the least squares fit to the data, the blue dashed line is the fit when obs 41 is removed.\nRemoving the high leverage observation has a much more substantial impact on the least squares line than removing an outlier. High leverage obss have a big impact on the estiamted regression line. It is important to identify them.\nFor SLR =&gt; look at the range of \\(x_i\\) and spot the out of range observations.\nMLR =&gt; it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.\nHave a look at the center panel in Figure 3.13. Most of the observations’ predictor values fall within the blue dashed ellipse, but the red observation is well outside of this range. But neither its value for \\(x_1\\) nor \\(x_2\\) is unusual.\nWe can quantify an observation’s leverage using the leverage statistic. A large value indicates a high leverage.\nFor SLR:\n\\[\n\\begin{align}\nh_i &= \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^n(x_{i'} - \\bar{x})^2} \\\\\n&1/n \\leq h_i \\leq 1 \\\\\n&\\bar{h_i} = (p+1)/n\n\\end{align}\n\\] as \\(x_i\\) increases its distance from \\(\\bar{x}\\) \\(h_i\\) increases.\n\\(h_i\\) is always between \\(1/n\\) and \\(1\\), and the average leverage for all the observations is always equal to \\((p+1)/n\\). If an observation has a \\(h\\) that greatly exceeds \\((p+1)/n\\) then we may suspect of high leverage.\nRight hand panel of Figure 3.13 shows studentized residuals versus \\(h_i\\). Obs 41 stands out as having a very high leverage statistic as well as a high studentized residual =&gt; It is an outlier as well as a high leverage observaiton. This is very dangerous. This plot also shows the reason that obs 20 had relatively little effect on the least sqaures fit in Figure 3.12: it has low leverage.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(sales ~ TV + radio, advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment()\n\n# A tibble: 200 × 9\n   sales    TV radio .fitted  .resid    .hat .sigma   .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1  22.1 230.   37.8   20.6   1.54   0.0140    1.68 0.00406       0.925 \n 2  10.4  44.5  39.3   12.3  -1.95   0.0188    1.68 0.00871      -1.17  \n 3   9.3  17.2  45.9   12.3  -3.04   0.0295    1.67 0.0341       -1.83  \n 4  18.5 152.   41.3   17.6   0.883  0.0124    1.68 0.00117       0.528 \n 5  12.9 181.   10.8   13.2  -0.324  0.00951   1.69 0.000120     -0.194 \n 6   7.2   8.7  48.9   12.5  -5.31   0.0347    1.64 0.124        -3.22  \n 7  11.8  57.5  32.8   11.7   0.0818 0.0129    1.69 0.0000105     0.0490\n 8  13.2 120.   19.6   12.1   1.09   0.00576   1.68 0.000823      0.653 \n 9   4.8   8.6   2.1    3.71  1.09   0.0271    1.68 0.00401       0.658 \n10  10.6 200.    2.6   12.6  -1.95   0.0171    1.68 0.00797      -1.17  \n# ℹ 190 more rows\n\n\nleverages are shown in .hat here.\n\nplot(lm(sales~TV,advertising), which = 5)\n\n\n\n\n6.Collinearity\nCollinearity refers to the situation in which two or more predictor variables are closely related to one another.\n\ngridExtra::grid.arrange(\n  Credit %&gt;% ggplot() + aes(x=Limit, y = Age) + geom_point(color =\"red\", alpha =0.4),\n  Credit %&gt;% ggplot() + aes(x=Limit, y= Rating) + geom_point(color =\"red\", alpha = 0.4),\n  ncol=2\n)\n\n\n\n\nFigure 3.14\n\n\n\n\nIn the left panel of Figure 3.14 we see no relationship, but on the right hand panel predicors are highly correlated: they are collinear =&gt; it is difficult to seperate out the individual effects of colliniar varaibles on the response.\nCollinearity reduces the accuracy of the estiamtes of the regression coefficients, it causes the standard error for \\(\\hat{\\beta_j}\\) to grow. t-statistic for each predictor is calculated by dividing \\(\\hat{\\beta_j}\\) by its standard error =&gt; collinearity declines the t-statistics =&gt; we may fail to reject \\(H_0:\\beta_j = 0\\).\nLets do two regressions:\n\nmodel1: balance regressed onto age and limit which has no collinearity\nmodel2: balance regressed onto rating and limit which are collinear.\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Age + Limit, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  tidy() %&gt;% mutate(model = 1) %&gt;% \n  bind_rows(\n    linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Rating + Limit, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  tidy() %&gt;% mutate(model =2)\n  )\n\n# A tibble: 6 × 6\n  term         estimate std.error statistic   p.value model\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept) -173.      43.8        -3.96  9.01e-  5     1\n2 Age           -2.29     0.672      -3.41  7.23e-  4     1\n3 Limit          0.173    0.00503    34.5   1.63e-121     1\n4 (Intercept) -378.      45.3        -8.34  1.21e- 15     2\n5 Rating         2.20     0.952       2.31  2.13e-  2     2\n6 Limit          0.0245   0.0638      0.384 7.01e-  1     2\n\n\nIn the first model both age and limit are statistically significant. In the second, collinearity between limit and raiting has caused standard error for the limit coefficient estimate to increase by a factor of 12 and p-value to increase to 0.701. The importance of the limit variable has been masked due to presence of collinearity.\nA simple way to detect collinearity is to look at the correlation matrix of the predictors. Correlation matrix shows the relatinship of two variables but, it is possible for colşinearty to exist between three or more varaibles even if no pair of variables has a particularly high correlation. This situation is called multicollinaerity.\nInstead of inspecting the correlatino matrix, better way to assess multicollinaerity is to compute the variance inflation factor (VIF). The VIF is the raio of the variance of \\(\\hat{\\beta_j}\\) when fitting the model divided by the variance of \\(\\hat{\\beta_j}\\) if fit on its own.\n\\[\n1 \\leq VIF\n\\] Smallest possible value of VIF is 1: complete absence of collinearity. Usuall there is always some degree of collinearity among the predictors. A VIF that exceeds 5 or 10 indicates a problematic amount of collinaerity.\nThe VIF for each vairable can be computed using the formula\n\\[\nVIF(\\hat{\\beta_j}) = \\frac{1}{1-R^2_{x_j | x_{-j}}}\n\\] \\(R^2_{x_j | x_{-j}}\\) is the \\(R^2\\) from a regression of \\(x_j\\) onto all of the other predictors. If \\(R^2_{x_j | x_{-j}}\\) is close to one, then collinearity is present, so VIF will be large.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(Balance ~ Age + Rating + Limit, Credit) %&gt;% \n  pluck(\"fit\") %&gt;% \n  car::vif() \n\n       Age     Rating      Limit \n  1.011385 160.668301 160.592880 \n\n\nA regression of balance on age, rating, and limit indicates that the predictors have VIF Values of 1.01, 160.67, 160.59. There is considerable collinearity in the data.\nWhen faced with collinearity, two simple solutions exist: * drop one of the problematic variables from the regression * combine collinear variables together into a single predictor. We might take the average of standardized versions of limit and rating in order to create a new varaible that measures credit worthiness.\n\nlm(sales~TV + radio + newspaper,advertising) %&gt;% summary()\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nTo get a anova table;\n\nanova(lm(sales~TV + radio + newspaper,advertising))\n\nAnalysis of Variance Table\n\nResponse: sales\n           Df Sum Sq Mean Sq   F value Pr(&gt;F)    \nTV          1 3314.6  3314.6 1166.7308 &lt;2e-16 ***\nradio       1 1545.6  1545.6  544.0501 &lt;2e-16 ***\nnewspaper   1    0.1     0.1    0.0312 0.8599    \nResiduals 196  556.8     2.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ncoef(lm(sales~TV + radio + newspaper,advertising))\n\n (Intercept)           TV        radio    newspaper \n 2.938889369  0.045764645  0.188530017 -0.001037493 \n\n\n\nlm(sales~TV + radio + newspaper,advertising) %&gt;% confint()\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n\n\n\nvcov(lm(sales~TV + radio + newspaper,advertising))\n\n              (Intercept)            TV         radio     newspaper\n(Intercept)  0.0972867479 -2.657273e-04 -1.115489e-03 -5.910212e-04\nTV          -0.0002657273  1.945737e-06 -4.470395e-07 -3.265950e-07\nradio       -0.0011154895 -4.470395e-07  7.415335e-05 -1.780062e-05\nnewspaper   -0.0005910212 -3.265950e-07 -1.780062e-05  3.446875e-05"
  },
  {
    "objectID": "analysis.html#multiple-linear-regression",
    "href": "analysis.html#multiple-linear-regression",
    "title": "3  Exercise",
    "section": "3.2 Multiple linear Regression",
    "text": "3.2 Multiple linear Regression\n\\[\nmedv = \\beta_0 + \\beta_1 lstat + \\beta_2 age\n\\]\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat + age, data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat + age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nLets do a regression including all varaibles\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv~., data) %&gt;% \n  pluck(\"fit\") -&gt; mlr\nmlr %&gt;% summary()\n\n\nCall:\nstats::lm(formula = medv ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nLets calculate the VIF values\n\nmlr %&gt;% car::vif()\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n\nage has the highest p-value, lets remove that from the regression\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv~. - age, data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ . - age, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\n\n3.2.1 Interaction Terms\nfollowing model\n\\[\nmedv = \\beta_0 + \\beta_1 lstat + \\beta_2 age + \\beta_3 lstat \\cdot age + \\epsilon\n\\]\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat * age, data) %&gt;%  # short for fit(medv~ lstat + age + lstat * age)\n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat * age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.2.2 Non-linaer Transformation of the predictors\n\\[\nmedv = \\beta_0 + \\beta_1 lstat + \\beta_2 lstat^2 + \\epsilon\n\\]\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat + I(lstat^2), data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat + I(lstat^2), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficient of the quadratic term is statistically significant: model is improved.\nWe use anova function to further quantify the extent to which quadratic fit is superior to linear fit. We It takes models as its arguments. We want to compare two models, lets create them again and compare them\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat, data) %&gt;% \n  pluck(\"fit\") -&gt; modelLin\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat + I(lstat^2), data) %&gt;% \n  pluck(\"fit\") -&gt; modelQuad\n\nanova(modelLin, modelQuad)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe annova function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here we reject \\(H_0\\); and the model indcludes quadratic term is superior to the linaer odel. This is not a suprise to us, because we already suspected this.\nLets do the plots again\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat, data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x = .fitted, y =.resid) + geom_point() + geom_smooth(se =F) + ggtitle(\"Linear model residual plot\") -&gt; p1\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat + I(lstat^2), data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x = .fitted, y =.resid) + geom_point() + geom_smooth(se =F) + ggtitle(\"Quadratic model residual plot\") -&gt; p2\n\np1 + p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNow on the quadratic model it looks like there is still some pattern in the residuals. Should we create a cubic fit? Now what we can do is actually insert like 10 polynomial terms and check whether they are significant or not\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ poly(lstat,10),data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ poly(lstat, 10), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.5340  -3.0286  -0.7507   2.0437  26.4738 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         22.5328     0.2311  97.488  &lt; 2e-16 ***\npoly(lstat, 10)1  -152.4595     5.1993 -29.323  &lt; 2e-16 ***\npoly(lstat, 10)2    64.2272     5.1993  12.353  &lt; 2e-16 ***\npoly(lstat, 10)3   -27.0511     5.1993  -5.203 2.88e-07 ***\npoly(lstat, 10)4    25.4517     5.1993   4.895 1.33e-06 ***\npoly(lstat, 10)5   -19.2524     5.1993  -3.703 0.000237 ***\npoly(lstat, 10)6     6.5088     5.1993   1.252 0.211211    \npoly(lstat, 10)7     1.9416     5.1993   0.373 0.708977    \npoly(lstat, 10)8    -6.7299     5.1993  -1.294 0.196133    \npoly(lstat, 10)9     8.4168     5.1993   1.619 0.106116    \npoly(lstat, 10)10   -7.3351     5.1993  -1.411 0.158930    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.199 on 495 degrees of freedom\nMultiple R-squared:  0.6867,    Adjusted R-squared:  0.6804 \nF-statistic: 108.5 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that up to 5 polynomial terms are statistically significant\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ poly(lstat,5),data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ poly(lstat, 5), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ poly(lstat,5),data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x = .fitted, y =.resid) + geom_point() + geom_smooth(se =F) + ggtitle(\"poly 5 model residual plot\")  -&gt; p3\n\np1 + p2 + p3\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nInstead of adding the polynomial transformations, we could try log transformation\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ log(lstat),data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  augment() %&gt;% \n  ggplot() + aes(x = .fitted, y =.resid) + geom_point() + geom_smooth(se =F) + ggtitle(\"log model residual plot\") \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is actually very nice.\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ log(rm),data) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ log(rm), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv~.,data) %&gt;% \n  pluck(\"fit\") -&gt; lmm\n\n\nlmm %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: medv\n           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \ncrim        1  6440.8  6440.8 286.0300 &lt; 2.2e-16 ***\nzn          1  3554.3  3554.3 157.8452 &lt; 2.2e-16 ***\nindus       1  2551.2  2551.2 113.2984 &lt; 2.2e-16 ***\nchas        1  1529.8  1529.8  67.9393 1.543e-15 ***\nnox         1    76.2    76.2   3.3861 0.0663505 .  \nrm          1 10938.1 10938.1 485.7530 &lt; 2.2e-16 ***\nage         1    90.3    90.3   4.0087 0.0458137 *  \ndis         1  1779.5  1779.5  79.0262 &lt; 2.2e-16 ***\nrad         1    34.1    34.1   1.5159 0.2188325    \ntax         1   329.6   329.6  14.6352 0.0001472 ***\nptratio     1  1309.3  1309.3  58.1454 1.266e-13 ***\nblack       1   593.3   593.3  26.3496 4.109e-07 ***\nlstat       1  2410.8  2410.8 107.0634 &lt; 2.2e-16 ***\nResiduals 492 11078.8    22.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlmm %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: medv\n           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \ncrim        1  6440.8  6440.8 286.0300 &lt; 2.2e-16 ***\nzn          1  3554.3  3554.3 157.8452 &lt; 2.2e-16 ***\nindus       1  2551.2  2551.2 113.2984 &lt; 2.2e-16 ***\nchas        1  1529.8  1529.8  67.9393 1.543e-15 ***\nnox         1    76.2    76.2   3.3861 0.0663505 .  \nrm          1 10938.1 10938.1 485.7530 &lt; 2.2e-16 ***\nage         1    90.3    90.3   4.0087 0.0458137 *  \ndis         1  1779.5  1779.5  79.0262 &lt; 2.2e-16 ***\nrad         1    34.1    34.1   1.5159 0.2188325    \ntax         1   329.6   329.6  14.6352 0.0001472 ***\nptratio     1  1309.3  1309.3  58.1454 1.266e-13 ***\nblack       1   593.3   593.3  26.3496 4.109e-07 ***\nlstat       1  2410.8  2410.8 107.0634 &lt; 2.2e-16 ***\nResiduals 492 11078.8    22.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlmm %&gt;% stats::step(direction = \"backward\") %&gt;% summary()\n\nStart:  AIC=1589.64\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n\n          Df Sum of Sq   RSS    AIC\n- age      1      0.06 11079 1587.7\n- indus    1      2.52 11081 1587.8\n&lt;none&gt;                 11079 1589.6\n- chas     1    218.97 11298 1597.5\n- tax      1    242.26 11321 1598.6\n- crim     1    243.22 11322 1598.6\n- zn       1    257.49 11336 1599.3\n- black    1    270.63 11349 1599.8\n- rad      1    479.15 11558 1609.1\n- nox      1    487.16 11566 1609.4\n- ptratio  1   1194.23 12273 1639.4\n- dis      1   1232.41 12311 1641.0\n- rm       1   1871.32 12950 1666.6\n- lstat    1   2410.84 13490 1687.3\n\nStep:  AIC=1587.65\nmedv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + \n    ptratio + black + lstat\n\n          Df Sum of Sq   RSS    AIC\n- indus    1      2.52 11081 1585.8\n&lt;none&gt;                 11079 1587.7\n- chas     1    219.91 11299 1595.6\n- tax      1    242.24 11321 1596.6\n- crim     1    243.20 11322 1596.6\n- zn       1    260.32 11339 1597.4\n- black    1    272.26 11351 1597.9\n- rad      1    481.09 11560 1607.2\n- nox      1    520.87 11600 1608.9\n- ptratio  1   1200.23 12279 1637.7\n- dis      1   1352.26 12431 1643.9\n- rm       1   1959.55 13038 1668.0\n- lstat    1   2718.88 13798 1696.7\n\nStep:  AIC=1585.76\nmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + \n    black + lstat\n\n          Df Sum of Sq   RSS    AIC\n&lt;none&gt;                 11081 1585.8\n- chas     1    227.21 11309 1594.0\n- crim     1    245.37 11327 1594.8\n- zn       1    257.82 11339 1595.4\n- black    1    270.82 11352 1596.0\n- tax      1    273.62 11355 1596.1\n- rad      1    500.92 11582 1606.1\n- nox      1    541.91 11623 1607.9\n- ptratio  1   1206.45 12288 1636.0\n- dis      1   1448.94 12530 1645.9\n- rm       1   1963.66 13045 1666.3\n- lstat    1   2723.48 13805 1695.0\n\n\n\nCall:\nstats::lm(formula = medv ~ crim + zn + chas + nox + rm + dis + \n    rad + tax + ptratio + black + lstat, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\n\nlmm %&gt;% stats::step(direction = \"forward\") %&gt;% summary()\n\nStart:  AIC=1589.64\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n\n\n\nCall:\nstats::lm(formula = medv ~ crim + zn + indus + chas + nox + rm + \n    age + dis + rad + tax + ptratio + black + lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\n\nlmm %&gt;% stats::step(direction = \"both\") %&gt;% summary()\n\nStart:  AIC=1589.64\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n\n          Df Sum of Sq   RSS    AIC\n- age      1      0.06 11079 1587.7\n- indus    1      2.52 11081 1587.8\n&lt;none&gt;                 11079 1589.6\n- chas     1    218.97 11298 1597.5\n- tax      1    242.26 11321 1598.6\n- crim     1    243.22 11322 1598.6\n- zn       1    257.49 11336 1599.3\n- black    1    270.63 11349 1599.8\n- rad      1    479.15 11558 1609.1\n- nox      1    487.16 11566 1609.4\n- ptratio  1   1194.23 12273 1639.4\n- dis      1   1232.41 12311 1641.0\n- rm       1   1871.32 12950 1666.6\n- lstat    1   2410.84 13490 1687.3\n\nStep:  AIC=1587.65\nmedv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + \n    ptratio + black + lstat\n\n          Df Sum of Sq   RSS    AIC\n- indus    1      2.52 11081 1585.8\n&lt;none&gt;                 11079 1587.7\n+ age      1      0.06 11079 1589.6\n- chas     1    219.91 11299 1595.6\n- tax      1    242.24 11321 1596.6\n- crim     1    243.20 11322 1596.6\n- zn       1    260.32 11339 1597.4\n- black    1    272.26 11351 1597.9\n- rad      1    481.09 11560 1607.2\n- nox      1    520.87 11600 1608.9\n- ptratio  1   1200.23 12279 1637.7\n- dis      1   1352.26 12431 1643.9\n- rm       1   1959.55 13038 1668.0\n- lstat    1   2718.88 13798 1696.7\n\nStep:  AIC=1585.76\nmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + \n    black + lstat\n\n          Df Sum of Sq   RSS    AIC\n&lt;none&gt;                 11081 1585.8\n+ indus    1      2.52 11079 1587.7\n+ age      1      0.06 11081 1587.8\n- chas     1    227.21 11309 1594.0\n- crim     1    245.37 11327 1594.8\n- zn       1    257.82 11339 1595.4\n- black    1    270.82 11352 1596.0\n- tax      1    273.62 11355 1596.1\n- rad      1    500.92 11582 1606.1\n- nox      1    541.91 11623 1607.9\n- ptratio  1   1206.45 12288 1636.0\n- dis      1   1448.94 12530 1645.9\n- rm       1   1963.66 13045 1666.3\n- lstat    1   2723.48 13805 1695.0\n\n\n\nCall:\nstats::lm(formula = medv ~ crim + zn + chas + nox + rm + dis + \n    rad + tax + ptratio + black + lstat, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\n\nlmm\n\n\nCall:\nstats::lm(formula = medv ~ ., data = data)\n\nCoefficients:\n(Intercept)         crim           zn        indus         chas          nox  \n  3.646e+01   -1.080e-01    4.642e-02    2.056e-02    2.687e+00   -1.777e+01  \n         rm          age          dis          rad          tax      ptratio  \n  3.810e+00    6.922e-04   -1.476e+00    3.060e-01   -1.233e-02   -9.527e-01  \n      black        lstat  \n  9.312e-03   -5.248e-01"
  },
  {
    "objectID": "analysis.html#slr",
    "href": "analysis.html#slr",
    "title": "3  Exercise",
    "section": "3.1 SLR",
    "text": "3.1 SLR\n\ndata = MASS::Boston\ndata %&lt;&gt;% as_tibble() %&gt;% print()\n\n# A tibble: 506 × 14\n      crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio black\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 0.00632  18    2.31     0 0.538  6.58  65.2  4.09     1   296    15.3  397.\n 2 0.0273    0    7.07     0 0.469  6.42  78.9  4.97     2   242    17.8  397.\n 3 0.0273    0    7.07     0 0.469  7.18  61.1  4.97     2   242    17.8  393.\n 4 0.0324    0    2.18     0 0.458  7.00  45.8  6.06     3   222    18.7  395.\n 5 0.0690    0    2.18     0 0.458  7.15  54.2  6.06     3   222    18.7  397.\n 6 0.0298    0    2.18     0 0.458  6.43  58.7  6.06     3   222    18.7  394.\n 7 0.0883   12.5  7.87     0 0.524  6.01  66.6  5.56     5   311    15.2  396.\n 8 0.145    12.5  7.87     0 0.524  6.17  96.1  5.95     5   311    15.2  397.\n 9 0.211    12.5  7.87     0 0.524  5.63 100    6.08     5   311    15.2  387.\n10 0.170    12.5  7.87     0 0.524  6.00  85.9  6.59     5   311    15.2  387.\n# ℹ 496 more rows\n# ℹ 2 more variables: lstat &lt;dbl&gt;, medv &lt;dbl&gt;\n\n\nWe are going to predict medv(median house value) in Boston, using 13 predictors such as rm(average number of rooms per house), age(avarege age of houses), and lstat (percent of household with low socioeconomics status).\nDo a linear regression of lstat on medv\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat, data) %&gt;%\n  pluck(\"fit\") -&gt; slr_res \n\nslr_res %&gt;% summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\nslr_res %&gt;% names()\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nGet the confidence intervals\n\nslr_res %&gt;% confint() %&gt;% print()\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\nslr_res %&gt;% dotwhisker::dwplot()\n\n\n\n\nLets do some predictions from this model. Lets create predictions for cases when lstat is equal to 5,10, and 15. We can use predict function for this. Here is a confidence interval for our p\n\nslr_res %&gt;% \n  predict(., tibble(lstat = c(5,10,15)), interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\n\n\nslr_res %&gt;% \n  predict(., tibble(lstat = c(5,10,15)), interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nLets plot medv and lstat along with the least squares regression line.\nWe can either:\n\nplot(data$medv, data$lstat)\nabline(slr_res)\n\n\n\n\n\nslr_res %&gt;% augment %&gt;% print() %&gt;% \n  ggplot() + aes(x = lstat, y = medv) + \n  geom_point() + \n  geom_abline(intercept = slr_res$coefficients[1], slope = slr_res$coefficients[2], size =1.1)\n\n# A tibble: 506 × 8\n    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 \n 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 \n 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 \n 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 \n 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  \n 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 \n 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250\n 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  \n 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  \n10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954\n# ℹ 496 more rows\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis looks like not a linear relationship actually; lets confirm this with our residual plot. Since this is a SLR i can plot \\(x\\) vs \\(e_i\\).\n\nslr_res %&gt;% augment %&gt;% print() %&gt;% \n  ggplot() + aes(x = .fitted, y = .resid) + geom_point() + geom_smooth(se=F) -&gt; p1\n\n# A tibble: 506 × 8\n    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 \n 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 \n 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 \n 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 \n 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  \n 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 \n 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250\n 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  \n 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  \n10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954\n# ℹ 496 more rows\n\nslr_res %&gt;% augment %&gt;% print() %&gt;% \n  ggplot() + aes(x = .fitted, y = .std.resid) + geom_point() + geom_smooth(se=F) -&gt; p2\n\n# A tibble: 506 × 8\n    medv lstat .fitted .resid    .hat .sigma     .cooksd .std.resid\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1  24    4.98   29.8  -5.82  0.00426   6.22 0.00189        -0.939 \n 2  21.6  9.14   25.9  -4.27  0.00246   6.22 0.000582       -0.688 \n 3  34.7  4.03   30.7   3.97  0.00486   6.22 0.00100         0.641 \n 4  33.4  2.94   31.8   1.64  0.00564   6.22 0.000198        0.264 \n 5  36.2  5.33   29.5   6.71  0.00406   6.21 0.00238         1.08  \n 6  28.7  5.21   29.6  -0.904 0.00413   6.22 0.0000440      -0.146 \n 7  22.9 12.4    22.7   0.155 0.00198   6.22 0.000000620     0.0250\n 8  27.1 19.2    16.4  10.7   0.00362   6.20 0.00544         1.73  \n 9  16.5 29.9     6.12 10.4   0.0136    6.20 0.0194          1.68  \n10  18.9 17.1    18.3   0.592 0.00274   6.22 0.0000125       0.0954\n# ℹ 496 more rows\n\np1 + p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nOur suspicions are true, it looks like there may not be a linear relationship, this shows a quadratic relationship.\nOur leverage statistics are given in the .hat column, but we can also get them using hatvalues\n\nplot(hatvalues(slr_res))\n\n\n\n\n\nwhich.max(hatvalues(slr_res))\n\n375 \n375 \n\n\nLets check the outliers by plotting standardized residuals vs fitted\n\nslr_res %&gt;% \n  augment() %&gt;% \n  mutate(id = 1:506) %&gt;% \n  filter(abs(.std.resid) &gt; 3) %&gt;% \n  select(id,.std.resid) %&gt;% print() %&gt;% \n  pull(\"id\") -&gt; outliers\n\n# A tibble: 10 × 2\n      id .std.resid\n   &lt;int&gt;      &lt;dbl&gt;\n 1   164       3.00\n 2   167       3.06\n 3   187       3.17\n 4   226       3.20\n 5   258       3.27\n 6   263       3.20\n 7   268       3.63\n 8   370       3.06\n 9   372       3.95\n10   373       3.85\n\n# visualize the outliers\n\nslr_res %&gt;% \n  augment() %&gt;% \n  mutate(id = 1:506) %&gt;% \n  ggplot() + aes(y =.std.resid, x = .fitted, color = ifelse(abs(.std.resid) &gt; 3, F,T)) + geom_point(size =2, show.legend = F) + geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggrepel::geom_text_repel(aes(label = ifelse(abs(.std.resid) &gt; 3,id,\"\")), max.overlaps = 100, show.legend = F) + scale_color_ipsum()\n\n\n\n\nLeverage points: we plot std res vs leverage\n\nslr_res %&gt;% \n  augment() %&gt;% mutate(id = 1:506) %&gt;% \n  filter(.hat &gt; 0.02) %&gt;% \n  select(id, .std.resid,.hat) %&gt;% \n  print() %&gt;% pull(\"id\") -&gt; highlevgs\n\n# A tibble: 5 × 3\n     id .std.resid   .hat\n  &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1   142       2.04 0.0204\n2   374       2.00 0.0210\n3   375       2.50 0.0269\n4   413       2.60 0.0203\n5   415       1.23 0.0250\n\n# high leverage points\n\nslr_res %&gt;% \n  augment() %&gt;% mutate(id = 1:506) %&gt;% \n  ggplot() + aes(x=.hat, y = .std.resid, color = ifelse(.hat &gt; 0.02,F,T)) + geom_point(show.legend = F) + geom_hline(yintercept =0, linetype = \"dashed\") + \n  ggrepel::geom_text_repel(aes(label = ifelse(.hat &gt; 0.02,id,\"\")), show.legend = F) +scale_color_ipsum()\n\n\n\n\n\nslr_res %&gt;% summary() %&gt;% print()\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nlinear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit(medv ~ lstat, data[-c(outliers,highlevgs),]) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = medv ~ lstat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.633  -3.465  -1.012   1.937  19.060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.0494     0.5110   66.63   &lt;2e-16 ***\nlstat        -0.9538     0.0358  -26.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.376 on 489 degrees of freedom\nMultiple R-squared:  0.5921,    Adjusted R-squared:  0.5912 \nF-statistic: 709.7 on 1 and 489 DF,  p-value: &lt; 2.2e-16\n\n\nRemoving high leverages and outliers improeves our result."
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "2  Linear Regression",
    "section": "",
    "text": "3 Selecting best regression varaibles\nYou are creating a new regression mdoel or improving an exiting model. You have many regression varaibles and you want to select the vest subset of those varaibles.\nThe step function can perform stepwise regression, either forward or backward. Backward stepwise regression starts with many varaibles and removes the underperformeners:\nlm &lt;- lm(sales ~ TV + radio + newspaper, advertising) # full model\nstats::step(lm, direction = \"backward\") %&gt;% summary() # best model\n\nStart:  AIC=212.79\nsales ~ TV + radio + newspaper\n\n            Df Sum of Sq    RSS    AIC\n- newspaper  1      0.09  556.9 210.82\n&lt;none&gt;                    556.8 212.79\n- radio      1   1361.74 1918.6 458.20\n- TV         1   3058.01 3614.8 584.90\n\nStep:  AIC=210.82\nsales ~ TV + radio\n\n        Df Sum of Sq    RSS    AIC\n&lt;none&gt;                556.9 210.82\n- radio  1    1545.6 2102.5 474.52\n- TV     1    3061.6 3618.5 583.10\n\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\nbackward stepwise regression is the easiest approach.\nBackward stepwise is easy but sometimes its not feasible to start with everything because you have too many candidate variables. In that case use forward stepwise regression; which will start with nothing and incrementatlly add variables that improve regression. It stops when no further improvement is possible\nlm(sales ~ 1,advertising)\n\n\nCall:\nlm(formula = sales ~ 1, data = advertising)\n\nCoefficients:\n(Intercept)  \n      14.02\nwe start with minimum model. We must tell step which candidate varaibels are available for inclusing in the model. We use scope argument for that.\nlm(sales~1,advertising) %&gt;% stats::step(direction = \"forward\", scope = (~ TV + radio + newspaper), trace = 0) %&gt;% summary() \n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\nwe have the same result with backward, in reality these results may differ.\nstats::step(lm, direction = \"both\") %&gt;% summary() \n\nStart:  AIC=212.79\nsales ~ TV + radio + newspaper\n\n            Df Sum of Sq    RSS    AIC\n- newspaper  1      0.09  556.9 210.82\n&lt;none&gt;                    556.8 212.79\n- radio      1   1361.74 1918.6 458.20\n- TV         1   3058.01 3614.8 584.90\n\nStep:  AIC=210.82\nsales ~ TV + radio\n\n            Df Sum of Sq    RSS    AIC\n&lt;none&gt;                    556.9 210.82\n+ newspaper  1      0.09  556.8 212.79\n- radio      1   1545.62 2102.5 474.52\n- TV         1   3061.57 3618.5 583.10\n\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919   &lt;2e-16 ***\nTV           0.04575    0.00139  32.909   &lt;2e-16 ***\nradio        0.18799    0.00804  23.382   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 2.2e-16\nMASS::boxcox(lm)\nDetecting the outliers :\ncar::outlierTest(lm)\n\n     rstudent unadjusted p-value Bonferroni p\n131 -5.757983          3.267e-08    6.534e-06"
  },
  {
    "objectID": "Chapter3.html#testing-residuals-for-autocorrelation",
    "href": "Chapter3.html#testing-residuals-for-autocorrelation",
    "title": "2  Linear Regression",
    "section": "3.1 testing residuals for autocorrelation",
    "text": "3.1 testing residuals for autocorrelation\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndwtest(lm) \n\n\n    Durbin-Watson test\n\ndata:  lm\nDW = 1.9685, p-value = 0.4097\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nif $ p &gt; 0.05$ no evidence of correaltion."
  },
  {
    "objectID": "Chapter3.html#the-marketing-plan",
    "href": "Chapter3.html#the-marketing-plan",
    "title": "2  Linear Regression",
    "section": "3.2 The Marketing Plan",
    "text": "3.2 The Marketing Plan\nLets go back to our 7 questions about the advertisingdata:\n\nIs there a relationship between advertising sales and budget?\n\nTo get an answer to this question, we regress `sales` onto our budget related varaibles `TV`, `radio`, and `newspaper` and test the hypothesis $H_0: \\beta_{TV}=\\beta_{radio}=\\beta_{newspaper} = 0$ with F-statistic.\n\nadvertising %&lt;&gt;% select(-\"sales_hat\")\nlinear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\") %&gt;% \n  fit(sales ~., advertising) %&gt;% \n  pluck(\"fit\") %&gt;% \n  summary()\n\n\nCall:\nstats::lm(formula = sales ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case the p-value corresponding to the F-statistic is very low, indicating a clear evidence of a relationship between advertising and sales.\n\nHow strong is the relationship?\nThis is about model accuracy. We can check \\(R^2\\); the predictors explain almost 90% of the variance in sales.\nWhich media contribute to sales?\nWe check p values of the coefficients. newspaper is not statistically significant; only TV and radio are realated to sales.\nHow large is the effect of each medium on sales?\n\n\nlm(sales~.,advertising) %&gt;%\n    dwplot(ci = 0.95,dot_args = list(size=2), vline = geom_vline(xintercept = 0, color = \"grey50\", linetype =2))\n\n\n\n\nWe use standard errors to construct confidence intervals for the coefficients. If any of the confidence intervals include 0 that predictor is not statistically significant. Collinearty can result in very wide standard errors; making confint to include zero. Was the collineary the reason that confint of newspaper to be so wide? Lets check VIF scores\n\ncar::vif(lm(sales~.,advertising))\n\n       TV     radio newspaper \n 1.004611  1.144952  1.145187 \n\n\nNo all the VIF scores suggest no evidence of collinearity.\n\nHow accurately can we predict future sales?\nAfter deciding on our model\n\n\nlm(sales~TV+radio + TV * radio, advertising) %&gt;% summary()\n\n\nCall:\nlm(formula = sales ~ TV + radio + TV * radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.750e+00  2.479e-01  27.233   &lt;2e-16 ***\nTV          1.910e-02  1.504e-03  12.699   &lt;2e-16 ***\nradio       2.886e-02  8.905e-03   3.241   0.0014 ** \nTV:radio    1.086e-03  5.242e-05  20.727   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nThe accuracy of our estimation depends on whether we wish to predict an individual response, $Y = f(X) + \\epsilon$, or the average response, $f(X)$. If the former we use a prediction interval, and if the latter, we use a confidecen interval. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty assocaited with $\\epsilon$, the irreducible error.\n\nIs the relationship linear?\nWe can use residual plots to identify non-linearity: the plot will show a pattern.\n\n\nlm(sales~TV + radio, advertising) %&gt;% augment() %&gt;% \n  ggplot() + aes(x = .hat, y = .fitted) + geom_point() + geom_smooth(se = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nthe model seems okay. If we had encountered a non-linearity we would have to make transformations to the predictors.\n\nIs there synergy among the advertising media?\nLR assumes additive relationship. When we account for interacation term our model is improved and the interaction term was statistically significant.\n\n\nlm(sales~TV + radio + radio * TV, advertising) %&gt;% summary()\n\n\nCall:\nlm(formula = sales ~ TV + radio + radio * TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.750e+00  2.479e-01  27.233   &lt;2e-16 ***\nTV          1.910e-02  1.504e-03  12.699   &lt;2e-16 ***\nradio       2.886e-02  8.905e-03   3.241   0.0014 ** \nTV:radio    1.086e-03  5.242e-05  20.727   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3.html#comparison-of-linear-regression-with-k-nearest-neighbors",
    "href": "Chapter3.html#comparison-of-linear-regression-with-k-nearest-neighbors",
    "title": "2  Linear Regression",
    "section": "3.3 Comparison of Linear Regression with K-Nearest Neighbors",
    "text": "3.3 Comparison of Linear Regression with K-Nearest Neighbors\nLinear regression is a parametric approach because it assumes a linear functinoal form for \\(f(x)\\). Parametric approaches are easy to fit, coefficients have simple interpretations and test of statistical significance can be easily performed. But they make strong assumptions about the form of \\(f(X)\\). If the assumed functional for is not correct, and the prediciton is our goal, then we see a poor performance of fit.\nIn contrast, non-parametric methods do not explicity assume a parametric for of \\(f(X)\\), so they provide a more flexible approach for performing regression. One of a non-parametric method is K-nearest neighbors regressin (KNN regression) .\nThe KNN regression method is closely related to the KNN classifier in Chapter 2. Given a value for K and a prediciton point \\(x_0\\), KNN regression first identifies the K traiinng observations that are closest to \\(x_0\\) represented by \\(N_0\\). It then estimates \\(f(x_0)\\) using the average of all the training responses in \\(N_0\\). In other words:\n\\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x \\in N_0} y_i\n\\]\nThe optimal \\(K\\) value will depend on the bias-variance tradeoff. A small value for \\(K\\) provides the most flexile fit, which will have low bias but high variance. This variance is due to the fact that the prediciton in a given region is entirely dependent on just one obsrvation. In contrast, larger values of \\(K\\) will provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in \\(f(x)\\). Chapter 5 will introduce several approaches for estiamting test error rates which can be used to identify the optimal \\(K\\) in KNN regression.\nThe parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true for of \\(f\\).\n\nin Figure one true relationship is linear, represented by the black line. Blue curves response to the KNN fits using \\(K=1\\) and \\(K=9\\). \\(K=9\\) is much closer to \\(f(x)\\). However, since the true relationship is linear, KNN approach cannot compete with a linear regression; a non-parametric approach incurs a cost in variance that is not offset by reduction in bias.\n\nThe blue dashed line is the OLS regression which outperfroms KNN for this data. Green sloid line, plotted as a function of \\(1/K\\) represents the test set mean squared error(MSE) for KNN. The KNN errors are wlell above the black dashed line, which is the test MSE for linear regression. When K is large KNN performs only a little worse that least squares regression in terms of MSE. It performs far worse when K is small.\nIn practice true relationship between \\(X\\) and \\(Y\\) is rarely exactly linear.\n\nFig 3.10 examines the relative performances of OLS regression and KNN under increasing levels of non-linearity in the relationship between \\(X\\) and \\(Y\\). In the top row, the true relationship is nearly linear. In this case we see that test MSE for linear regression is still superior to that of KNN for low values of K. However, fro \\(4 \\leq K\\) KNN outperforms linear regression. The secodn row has more non-linearity. In this case KNN is better for all valeus of K.\nSo, KNN performs slightly worse than linear regression when the relationship is linear, but much better for non-linear situations. Does this mean we should favour KNN In relaity because the relationships are rarely linear? No, in relaity, even when the true relationship is highly non linear, KNN may still provide inferior resutls to linear regression. Fig 3-18 and 19 had \\(p=1\\), but for larger \\(p\\) KNN often performs worse than linear regression.\n\nFig 20 shows a strong non-linear situation. When \\(p=1\\)or \\(p=2\\) KNN is better, but for \\(p=3\\) results are mixed, for \\(4\\leqp\\) LR is superior. This decrease in performance as the dimensions increases is a common problem for KNN\nAs a general rule parametric methods will tend to outperform non-parametric approaches when there is small number of observations per predictor."
  },
  {
    "objectID": "Chapter4.html#an-overview-of-classsification",
    "href": "Chapter4.html#an-overview-of-classsification",
    "title": "4  Classification",
    "section": "4.1 An overview of Classsification",
    "text": "4.1 An overview of Classsification\nHere some clasffication problems\n\nA person arrives at the emergency room with a set of symptoms that could possible be attributed to one f three medical conditions. Which of the three conditions does the individual have?\nAn online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP adress, past transaction history and so forth.\nIn the bais of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease causing and which are not.\n\nJust like in LR , in the classification setting we have a set of training observations \\((x_1,y_1), \\dots, (x_n,y_n)\\) what we can use to build a classfier. We want our classifier to perform not only on th training data, but also on test observations that are not used to train the classier.\n\nWe are going to use Default data set.\n\n\ndefault = read_csv(\"./data/Default.csv\")\n\nRows: 10000 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): default, student\ndbl (2): balance, income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefault %&lt;&gt;% \n  mutate_if(is.character, ~as.factor(.)) %&gt;% \n  print()\n\n# A tibble: 10,000 × 4\n   default student balance income\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 No      No         730. 44362.\n 2 No      Yes        817. 12106.\n 3 No      No        1074. 31767.\n 4 No      No         529. 35704.\n 5 No      No         786. 38463.\n 6 No      Yes        920.  7492.\n 7 No      No         826. 24905.\n 8 No      Yes        809. 17600.\n 9 No      No        1161. 37469.\n10 No      No           0  29275.\n# ℹ 9,990 more rows\n\n\nWe are interested in predicting whether an individual will default on his or her credit card balance.\n\nskimr::skim(default)\n\n\nData summary\n\n\nName\ndefault\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndefault\n0\n1\nFALSE\n2\nNo: 9667, Yes: 333\n\n\nstudent\n0\n1\nFALSE\n2\nNo: 7056, Yes: 2944\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbalance\n0\n1\n835.37\n483.71\n0.00\n481.73\n823.64\n1166.31\n2654.32\n▆▇▅▁▁\n\n\nincome\n0\n1\n33516.98\n13336.64\n771.97\n21340.46\n34552.64\n43807.73\n73554.23\n▂▇▇▅▁\n\n\n\n\n\n\nGGally::ggpairs(mapping = aes(color = default), data = default)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\ndefault %&gt;% \n  ggplot() + aes(x = balance, y = income, color = default, shape = default) + geom_point() + scale_color_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) + scale_shape_manual(values = c(1,3)) -&gt; p1\n\ndefault %&gt;% \n  ggplot() + aes(x = default, y = balance, fill = default) + geom_boxplot() + \n  scale_fill_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) -&gt; p2\n\ndefault %&gt;% \n  ggplot() + aes(x = default, y = income, fill = default) + geom_boxplot() + \n  scale_fill_manual(values = c(\"#6CA2C9\",\"#BD5E2A\")) -&gt; p3\n\n# gridExtra::grid.arrange(p1,p2,p3, nrow=1)\ngridExtra::grid.arrange(p1,p2,p3, nrow=3)\n\n\n\n# Top: The aanual incomes and montly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: boxplots of balances as a function of default status. Bottom: boxplots of income as a functino of default status.\n\npeople who default tend to have high credit card balances compared to not defaulted.\n\ndefault %&gt;% \n  count(default) %&gt;% \n  mutate(port = n/sum(n))\n\n# A tibble: 2 × 3\n  default     n   port\n  &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 No       9667 0.967 \n2 Yes       333 0.0333\n\n# default rate is 3%\n\n\ndefault %&gt;% \n  group_by(student,default) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(port = count/sum(count))\n\n`summarise()` has grouped output by 'student'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   student [2]\n  student default count   port\n  &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 No      No       6850 0.971 \n2 No      Yes       206 0.0292\n3 Yes     No       2817 0.957 \n4 Yes     Yes       127 0.0431\n\n# student are 2 times more likely to default\n\nIn this chapter we laern how to build a model to predict default(\\(y\\)), for any given value of balance (\\(x_1\\)), and income (\\(x_2\\)). Since \\(Y\\) is not quantitative, SLR is not appropriate."
  },
  {
    "objectID": "Chapter4.html#why-not-linear-regression",
    "href": "Chapter4.html#why-not-linear-regression",
    "title": "4  Classification",
    "section": "4.2 Why Not Linear Regression?",
    "text": "4.2 Why Not Linear Regression?\nWhy is LR not appropriate here?\nSuppose that we are trying to predict the mdeical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, therea rea three possible diagnoses: stroke, drug overdose, epileptic seizure. We could consider encoding these values as a quantitative respose variable \\(Y\\):\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke}; \\\\\n2 & \\text{if drug overdose}; \\\\\n3 & \\text{if epileptic seizure}.\n\\end{cases}\n\\]\nWe can now predict \\(Y\\) using \\(x_1, \\dots, \\x_p\\). However, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure. In practice there is no particular reason that this needs to be the case. We could have ordered the cases differently, i.e. stroke to the 3 etc. Which implies a totally different relationship among the three conditions. All these combinations would produce different linear models that would lead to different set of predictions on test observations.\nHowever, if the response variable’s values take on a natural ordering, such as mild, moderate, and severe, and we felt the gap between mild and moderate was similar to the gap between moderate and sever, then a 1,2,3 coding would be reasonable. **Unfortunaltely, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantittative response that is ready for linear regression*.\nFor a binary(two level) qualitative response, the situation is easier. For instance consider only two possiblities for \\(Y\\):\n\\[\nY =\n\\begin{cases}\n0 & \\text{if stroke} \\\\\n1 & \\text{if drug overdose}\n\\end{cases}\n\\]\nwe can create a dummy varaible and fit a linar regression to this binary response and predict drug overdose if \\(\\hat{y}&gt;0.5\\) and stroke otherwise. However, if we use a linear regression our estimates might be outside of \\([0,1]\\), aking them hard to interpret as probabilities.\nThe dummy varaible approach coonot be easliy extended to accommodate qualitative responses with more than two levels. We prefer classification methods:\n## Logistic Regression\nOur default variable can have two values: Yes or No. Rather than modeling this response \\(Y\\) directly, logistic regression models the probability that \\(Y\\) belongs to a particular category.\nFor the default data logistic regression models the probability of default. For example, the probability of default given balance can be written as\n\\[\nPr(default = Yes | balance)\n\\] The values of \\(Pr(default = Yes|balance)\\), which we abbreviate \\(p(balance)\\), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, we can predict \\(default = Yes\\) or any individaul for whom \\(p(balance)&gt;0.5\\). Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(balance)&gt;0.1\\).\n\n4.2.1 The Logistic Model\nHow should we model the relationship between \\(p(x) = Pr(y = 1| x)\\) and \\(x\\)? (when we coded 0/1 for response values)\nIn section 4.2 we talked of using a linear regression model to represent these probabilities:\n\\[\np(x) = \\beta_0 + \\beta_1 x\n\\] (4.1)\n\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  print() %&gt;% \n  lm(default~balance,.) %&gt;% \n  summary()\n\n# A tibble: 10,000 × 4\n   default student balance income\n     &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1       0 No         730. 44362.\n 2       0 Yes        817. 12106.\n 3       0 No        1074. 31767.\n 4       0 No         529. 35704.\n 5       0 No         786. 38463.\n 6       0 Yes        920.  7492.\n 7       0 No         826. 24905.\n 8       0 Yes        809. 17600.\n 9       0 No        1161. 37469.\n10       0 No           0  29275.\n# ℹ 9,990 more rows\n\n\n\nCall:\nlm(formula = default ~ balance, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23533 -0.06939 -0.02628  0.02004  0.99046 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.519e-02  3.354e-03  -22.42   &lt;2e-16 ***\nbalance      1.299e-04  3.475e-06   37.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1681 on 9998 degrees of freedom\nMultiple R-squared:  0.1226,    Adjusted R-squared:  0.1225 \nF-statistic:  1397 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  lm(default~balance,.) -&gt; lm_res\n\ndefault %&gt;% \n  mutate(default = ifelse(default == \"Yes\",1,0)) %&gt;% \n  ggplot() + aes(x = balance, y = default) + geom_point(shape = 4, color = \"brown4\") + \n  geom_abline(intercept = lm_res$coefficients[1], slope = lm_res$coefficients[2], color = \"blue\") + scale_y_continuous(expand = c(0,0.25), breaks = seq(-0.2,1,by=0.2) )\n\n\n\n\nThe plot above is the same as plot on the left pane of 4.2. Here we see the problem with this approach, for balances close to zero, we predict a negative probability of default and if we predict for very large balances we would get values bigger than 1. These predictions are not sensible. This always happen for any time a straight line is fit to a binary response that is coded as 0 or 1, in pricpile we can always predict \\(p(x)&lt;0\\) for some values of \\(x\\) and \\(p(x)&gt;1\\) for others.\nTo avoid this problem we must model \\(p(x)\\) using a function that gives outputs between 0 and 1 for all values of \\(x\\). Many functions meet this description, however in logistis regression, we use logistic function\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\n\\] (4.2)\nTo fit the mode (4.2) we use a method called maximum likelihood, which we discuss in the next section. The right-hand panel of Figure 4.2 sows then fit of the logistic regression model to the defaultdata.\nNotice that for low balances we now predict the probability of default as close to , but never below zero. Likewise for high balances we predict a default probability close to, but never above, one.\nThe logistic function will always produce an S-shaped curve of this form, and so regardless of the value of \\(X\\) we will obtain a sensible prediction.\nAfter a bit of manipulation of (4.2) we find that\n$$ \\[\\begin{align}\np(x) &= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\\n\n1-p(x) &= 1- \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} \\\\\n\n1- p(x) &= \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x}}  \\\\\n\n1- p(x) \\cdot e^{\\beta_0 + \\beta_1x} &= \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\\\\n\n1-p(x) \\cdot e^{\\beta_0 + \\beta_1 x} &= p(x) \\\\\n\n\\frac{p(x)}{1-p(x)} &= e^{\\beta_0 + \\beta_1} \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space (4.3)\n\n\\end{align}\\] $$\nthe quantity \\(p(x)/(1 - p(x))\\) is called the odds, and can take on any value between 0 and \\(\\infty\\). Values of the odds close to 0 and \\(\\infty\\) indicate very low and very high probabilities of default, respectively.\nFor example, on average 1 in 4 people with an odds of 1/4 will default since \\(p(x) = 0.2\\) implies an ods of \\(\\frac{0.2}{1-0.2} = 1/4\\). Likewise on average 9/10 wpeople with an odds of 9 will default since \\(p(x) = 0.9\\) implies an odds of \\(\\frac{0.9}{1-0.9} = 9\\).\nBy taking the lograithm of both sides of (4.3) we arrive at\n\\[\n\\log(\\frac{p(x)}{1-p(x)}) = \\beta_0 + \\beta_1 x\n\\] (4.4)\nthe left-hand side is called the log-odds- or logit. We see that the logistic regression model (4.2) has a logit that is linear in \\(X\\).\nHere increasing \\(x\\) by one unit changes the log odds by \\(\\beta_1\\) (4.4), or it multiplies the odds by \\(e^{\\beta_1}\\). But because the relationship between \\(p(x)\\) and \\(x\\) in (4.2) is not a straight line, \\(\\beta_1\\) does not correspond to the change in \\(p(x)\\) associated with a one-unit increase in \\(x\\). The amount that \\(p(x)\\) changes due to a one-unit change in \\(x\\) will depend on the current value of \\(x\\). But regardless of the value of \\(x\\), if \\(\\beta_1\\) is positive, then increasing \\(x\\) will be associated with increasing \\(p(x)\\), vice versa. We can see that in the right hand panel of Figure 4.2.\n\n\n4.2.2 Estiamting the Regression Coefficients\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1}}\n\\] (4.2) The coefficients of \\(\\beta_0\\) and \\(\\beta_1\\) are unknown, must be estimated based on the available training data. We are going to use maximum likelyhood method. The basic intuition begind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\)} of default for each individual, using (4.2), corresponds as closely as possible to the individual’s observed default status. In other words, we try to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) such that plugging these estimates into the model for \\(p(x)\\),given in (4.2), yields a number close to 1 for all individuals who defaulted, and a number close to zero for all individuals who did not. We can formalize this mathematical equation with likelihood function\n$$\nl(0, 1) = {i:y_i = 1}p(x_i) {i’:y_{i’} = 0}(1-p(x_{i’})) $$ (4.5)\nThe estimates \\(\\hat{\\beta_0\\) and \\(\\hat{\\beta_1}\\) are chosen to maximize this likelihood function.\n\n# initialize model\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\nlog_model\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n# setup the recipe \ndefault_recipe &lt;- recipe(default ~ balance, data = default)\ndefault_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n# set up the workflow\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(default_recipe) -&gt; default_workflow\ndefault_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n## fit model\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;%\n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term         estimate std.error statistic   p.value  conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   \n2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594\n\n# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.\n\n\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  augment(new_data = default) %&gt;% \n  ggplot() + aes(x = balance, y = default, color = .pred_class) + geom_point() + scale_color_ipsum()\n\n\n\n\nLets have a look at the Logistic Regression results again:\n\n## fit model\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;%\n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term         estimate std.error statistic   p.value  conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   \n2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594\n\n# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.\n\nWe can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in the above plays the same role as t statistic in linear regression output. They are calculated from \\(\\hat{\\beta_i} / \\text{SE}(\\hat{\\beta_i})\\), and so a large(absolute) value of the z-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1 = 0\\). This null hypothesis implies that \\(p(x) = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\). In other words, that the probability of default does not depend on blaance. p value is very low, we can reject \\(H_0\\); there is relationship between balance and probability of default. Intercept is not important here.\n\n\n4.2.3 Making predictions\nOnce the coefficients have been estimated, we can compute the probability of default for any given credit card balance.\n\\[\n\\hat{p}(x) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1}x}}{1 +e^{\\hat{\\beta_0} + \\hat{\\beta_1}x}} = \\frac{e^{-10.6513 + 0.0055 x}}{1 + e^{-10.6513 + 0.0055 x}}\n\\]\nso for example given income $1,000 the predicted possiblity of default is\n\nexp(-10.6513 + 0.0055 * 1000) / (1 + exp(-10.6513 + 0.0055 * 1000))\n\n[1] 0.005758518\n\n\nwhich is below 1%. What about the probability of default for a person with $2,000 balance\n\nexp(-10.6513 + 0.0055 * 2000) / (1 + exp(-10.6513 + 0.0055 * 2000))\n\n[1] 0.5863023\n\n\nmuch higher 58%.\nWe can get all the predictions for our training data set form\n\ndefault_workflow %&gt;% \n  fit(data=default) %&gt;% \n  augment(new_data = default) %&gt;% print() %&gt;% \n  count(.pred_class)\n\n# A tibble: 10,000 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 No      No         730. 44362. No             0.999 0.00131  \n 2 No      Yes        817. 12106. No             0.998 0.00211  \n 3 No      No        1074. 31767. No             0.991 0.00859  \n 4 No      No         529. 35704. No             1.00  0.000434 \n 5 No      No         786. 38463. No             0.998 0.00178  \n 6 No      Yes        920.  7492. No             0.996 0.00370  \n 7 No      No         826. 24905. No             0.998 0.00221  \n 8 No      Yes        809. 17600. No             0.998 0.00202  \n 9 No      No        1161. 37469. No             0.986 0.0138   \n10 No      No           0  29275. No             1.00  0.0000237\n# ℹ 9,990 more rows\n\n\n# A tibble: 2 × 2\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 No           9858\n2 Yes           142\n\n# we classified 142 cases to default \n\n# originally \ndefault %&gt;% \n  count(default)\n\n# A tibble: 2 × 2\n  default     n\n  &lt;fct&gt;   &lt;int&gt;\n1 No       9667\n2 Yes       333\n\n# 333 cases are actually default\n\nWe can implement qualitative predictors to the logistic regression using the dummy variable approach.. Lets predict default by only student variable\n\\[\nx =\n\\begin{cases}\n1 & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\n\ndefault_recipe &lt;- recipe(default ~ student, data = default)\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(default_recipe) -&gt; default_workflow\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  tidy(conf.int = T)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -3.50     0.0707    -49.6  0          -3.65     -3.37 \n2 studentYes     0.405    0.115       3.52 0.000431    0.177     0.629\n\n\nThe coefficient is positive and the p value is statistically significant. This indicates that students tend to have higher default probabilities than non student:\n\\[\n\\widehat{Pr}(default = Yes | student = Yes) = \\frac{e^{-50 + 0.4049 \\times 1}}{1 + e^{-50 + 0.4049 \\times 1}} = 0.0431\n\\]\n\\[\n\\widehat{Pr}(default = Yes | student = No) = \\frac{e^{-50 + 0.4049 \\times 0}}{1 + e^{-50 + 0.4049 \\times 0}} = 0.0292\n\\]\n\n\n4.2.4 Multiple Logistic Regression\nWe now consider the problem of predicting a binary response using a multiple predictors. We can generalize (4.4) as follows:\n\\[\n\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p\n\\] (4.6)\n(4.6) can be written as\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p}}\n\\] (4.7)\nWe again use maximum likelihood method to estimate \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\).\nLets use all of our variables in our model\n\nlogistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") -&gt; log_model\n\nrecipe(default ~ balance + income + student, data = default) -&gt; default_recipe\n\nworkflow() %&gt;% \n  add_model(log_model) %&gt;% \n  add_recipe(default_recipe) -&gt; default_workflow\n\ndefault_workflow %&gt;% \n  fit(data = default) %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term            estimate  std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.9        0.492        -22.1   4.91e-108\n2 balance       0.00574    0.000232      24.7   4.22e-135\n3 income        0.00000303 0.00000820     0.370 7.12e-  1\n4 studentYes   -0.647      0.236         -2.74  6.19e-  3"
  }
]