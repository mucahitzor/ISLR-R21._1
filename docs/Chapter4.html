<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ISLR-R21._1 - 3&nbsp; Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./analysis.html" rel="next">
<link href="./Chapter3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Chapter4.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">ISLR-R21._1</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./ISLR-R21._1.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./ISLR-R21._1.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <a href="https://twitter.com/intent/tweet?url=|url|" rel="" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What Is Statistical Learning?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Chapter4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exercise</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#an-overview-of-classsification" id="toc-an-overview-of-classsification" class="nav-link active" data-scroll-target="#an-overview-of-classsification"><span class="header-section-number">3.1</span> An overview of Classsification</a></li>
  <li><a href="#why-not-linear-regression" id="toc-why-not-linear-regression" class="nav-link" data-scroll-target="#why-not-linear-regression"><span class="header-section-number">3.2</span> Why Not Linear Regression?</a>
  <ul class="collapse">
  <li><a href="#the-logistic-model" id="toc-the-logistic-model" class="nav-link" data-scroll-target="#the-logistic-model"><span class="header-section-number">3.2.1</span> The Logistic Model</a></li>
  <li><a href="#estiamting-the-regression-coefficients" id="toc-estiamting-the-regression-coefficients" class="nav-link" data-scroll-target="#estiamting-the-regression-coefficients"><span class="header-section-number">3.2.2</span> Estiamting the Regression Coefficients</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions"><span class="header-section-number">3.2.3</span> Making predictions</a></li>
  <li><a href="#multiple-logistic-regression" id="toc-multiple-logistic-regression" class="nav-link" data-scroll-target="#multiple-logistic-regression"><span class="header-section-number">3.2.4</span> Multiple Logistic Regression</a></li>
  <li><a href="#logistic-regression-for-2-response-classes" id="toc-logistic-regression-for-2-response-classes" class="nav-link" data-scroll-target="#logistic-regression-for-2-response-classes"><span class="header-section-number">3.2.5</span> Logistic Regression for &gt; 2 Response Classes</a></li>
  </ul></li>
  <li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis" class="nav-link" data-scroll-target="#linear-discriminant-analysis"><span class="header-section-number">3.3</span> Linear Discriminant Analysis</a>
  <ul class="collapse">
  <li><a href="#using-bayes-theorem-for-classification" id="toc-using-bayes-theorem-for-classification" class="nav-link" data-scroll-target="#using-bayes-theorem-for-classification"><span class="header-section-number">3.3.1</span> Using Bayes’ Theorem for Classification</a></li>
  <li><a href="#linear-discriminant-analysis-for-p1" id="toc-linear-discriminant-analysis-for-p1" class="nav-link" data-scroll-target="#linear-discriminant-analysis-for-p1"><span class="header-section-number">3.3.2</span> Linear Discriminant Analysis for p=1</a></li>
  <li><a href="#linear-discriminant-analysis-for-p-1" id="toc-linear-discriminant-analysis-for-p-1" class="nav-link" data-scroll-target="#linear-discriminant-analysis-for-p-1"><span class="header-section-number">3.3.3</span> Linear Discriminant Analysis for p &gt; 1</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressPackageStartupMessages</span>({</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggthemes)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dotwhisker)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hrbrthemes)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(showtext)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>extrafont<span class="sc">::</span><span class="fu">loadfonts</span>(<span class="at">quiet =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_ipsum_es</span>(<span class="at">axis_title_size =</span> <span class="dv">11</span> , <span class="at">axis_title_just =</span> <span class="st">"c"</span>) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">axis.line =</span> <span class="fu">element_line</span>(<span class="at">color =</span><span class="st">"black"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For many cases the response variable is <em>qualitative</em>, or <em>categorical</em>.</p>
<p>Clasification is the process for predicting qualitative responses; we are classifying an observation.</p>
<p>There are three main classifiers,<em>classification techniques</em>, mainly:</p>
<ul>
<li><em>Logistic regression</em></li>
<li><em>Linear discriminant analysis</em></li>
<li><em>K-nearest neighbors</em></li>
</ul>
<p>We discuss more computer intensive methods is later chapters such as GAM(ch 7), trees, random forests, and boosting(ch 8), and support vector machines (ch 9).</p>
<section id="an-overview-of-classsification" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="an-overview-of-classsification"><span class="header-section-number">3.1</span> An overview of Classsification</h2>
<p>Here some clasffication problems</p>
<ol type="1">
<li><p>A person arrives at the emergency room with a set of symptoms that could possible be attributed to one f three medical conditions. Which of the three conditions does the individual have?</p></li>
<li><p>An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP adress, past transaction history and so forth.</p></li>
<li><p>In the bais of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease causing and which are not.</p></li>
</ol>
<p>Just like in LR , in the classification setting we have a set of training observations <span class="math inline">\((x_1,y_1), \dots, (x_n,y_n)\)</span> what we can use to build a classfier. We want our classifier to perform not only on th training data, but also on test observations that are not used to train the classier.</p>
<blockquote class="blockquote">
<p>We are going to use <code>Default</code> data set.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>default <span class="ot">=</span> <span class="fu">read_csv</span>(<span class="st">"./data/Default.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Rows: 10000 Columns: 4
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr (2): default, student
dbl (2): balance, income

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&lt;&gt;%</span> </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.character, <span class="sc">~</span><span class="fu">as.factor</span>(.)) <span class="sc">%&gt;%</span> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10,000 × 4
   default student balance income
   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;
 1 No      No         730. 44362.
 2 No      Yes        817. 12106.
 3 No      No        1074. 31767.
 4 No      No         529. 35704.
 5 No      No         786. 38463.
 6 No      Yes        920.  7492.
 7 No      No         826. 24905.
 8 No      Yes        809. 17600.
 9 No      No        1161. 37469.
10 No      No           0  29275.
# ℹ 9,990 more rows</code></pre>
</div>
</div>
<p>We are interested in predicting whether an individual will default on his or her credit card balance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>skimr<span class="sc">::</span><span class="fu">skim</span>(default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">default</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">10000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">factor</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 18%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: left;">ordered</th>
<th style="text-align: right;">n_unique</th>
<th style="text-align: left;">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">default</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">No: 9667, Yes: 333</td>
</tr>
<tr class="even">
<td style="text-align: left;">student</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">No: 7056, Yes: 2944</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">balance</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">835.37</td>
<td style="text-align: right;">483.71</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">481.73</td>
<td style="text-align: right;">823.64</td>
<td style="text-align: right;">1166.31</td>
<td style="text-align: right;">2654.32</td>
<td style="text-align: left;">▆▇▅▁▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">income</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">33516.98</td>
<td style="text-align: right;">13336.64</td>
<td style="text-align: right;">771.97</td>
<td style="text-align: right;">21340.46</td>
<td style="text-align: right;">34552.64</td>
<td style="text-align: right;">43807.73</td>
<td style="text-align: right;">73554.23</td>
<td style="text-align: left;">▂▇▇▅▁</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">color =</span> default), <span class="at">data =</span> default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> income, <span class="at">color =</span> default, <span class="at">shape =</span> default) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#6CA2C9"</span>,<span class="st">"#BD5E2A"</span>)) <span class="sc">+</span> <span class="fu">scale_shape_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)) <span class="ot">-&gt;</span> p1</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> default, <span class="at">y =</span> balance, <span class="at">fill =</span> default) <span class="sc">+</span> <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#6CA2C9"</span>,<span class="st">"#BD5E2A"</span>)) <span class="ot">-&gt;</span> p2</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> default, <span class="at">y =</span> income, <span class="at">fill =</span> default) <span class="sc">+</span> <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#6CA2C9"</span>,<span class="st">"#BD5E2A"</span>)) <span class="ot">-&gt;</span> p3</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># gridExtra::grid.arrange(p1,p2,p3, nrow=1)</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1,p2,p3, <span class="at">nrow=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Top: The aanual incomes and montly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: boxplots of balances as a function of default status. Bottom: boxplots of income as a functino of default status.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>people who default tend to have high credit card balances compared to not defaulted.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(default) <span class="sc">%&gt;%</span> </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">port =</span> n<span class="sc">/</span><span class="fu">sum</span>(n))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 3
  default     n   port
  &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;
1 No       9667 0.967 
2 Yes       333 0.0333</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># default rate is 3%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(student,default) <span class="sc">%&gt;%</span> </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">count =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">port =</span> count<span class="sc">/</span><span class="fu">sum</span>(count))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'student'. You can override using the
`.groups` argument.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 4
# Groups:   student [2]
  student default count   port
  &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;
1 No      No       6850 0.971 
2 No      Yes       206 0.0292
3 Yes     No       2817 0.957 
4 Yes     Yes       127 0.0431</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># student are 2 times more likely to default</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this chapter we laern how to build a model to predict <code>default</code>(<span class="math inline">\(y\)</span>), for any given value of balance (<span class="math inline">\(x_1\)</span>), and income (<span class="math inline">\(x_2\)</span>). Since <span class="math inline">\(Y\)</span> is not quantitative, SLR is not appropriate.</p>
</section>
<section id="why-not-linear-regression" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="why-not-linear-regression"><span class="header-section-number">3.2</span> Why Not Linear Regression?</h2>
<p>Why is LR not appropriate here?</p>
<p>Suppose that we are trying to predict the mdeical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, therea rea three possible diagnoses: <code>stroke</code>, <code>drug overdose</code>, <code>epileptic seizure</code>. We could consider encoding these values as a quantitative respose variable <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
Y =
\begin{cases}
1 &amp; \text{if stroke}; \\
2 &amp; \text{if drug overdose}; \\
3 &amp; \text{if epileptic seizure}.
\end{cases}
\]</span></p>
<p>We can now predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(x_1, \dots, \x_p\)</span>. However, this coding implies an ordering on the outcomes, putting <code>drug overdose</code> in between <code>stroke</code> and <code>epileptic seizure</code> and insisting that the difference between <code>stroke</code> and <code>drug overdose</code> is the same as the difference between <code>drug overdose</code> and <code>epileptic seizure</code>. In practice there is no particular reason that this needs to be the case. We could have ordered the cases differently, i.e.&nbsp;stroke to the 3 etc. Which implies a totally different relationship among the three conditions. All these combinations would produce different linear models that would lead to different set of predictions on test observations.</p>
<p>However, if the response variable’s values take on a natural ordering, such as <em>mild, moderate, and severe</em>, and we felt the gap between mild and moderate was similar to the gap between moderate and sever, then a 1,2,3 coding would be reasonable. **Unfortunaltely, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantittative response that is ready for linear regression*.</p>
<p>For a <em>binary(two level)</em> qualitative response, the situation is easier. For instance consider only two possiblities for <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
Y =
\begin{cases}
0 &amp; \text{if stroke} \\
1 &amp; \text{if drug overdose}
\end{cases}
\]</span></p>
<p>we can create a dummy varaible and fit a linar regression to this binary response and predict <code>drug overdose</code> if <span class="math inline">\(\hat{y}&gt;0.5\)</span> and <code>stroke</code> otherwise. However, if we use a linear regression our estimates might be outside of <span class="math inline">\([0,1]\)</span>, aking them hard to interpret as probabilities.</p>
<p>The dummy varaible approach coonot be easliy extended to accommodate qualitative responses with more than two levels. We prefer classification methods:</p>
<p>## Logistic Regression</p>
<p>Our <code>default</code> variable can have two values: <code>Yes</code> or <code>No</code>. Rather than modeling this response <span class="math inline">\(Y\)</span> directly, logistic regression models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<p>For the <code>default</code> data logistic regression models the probability of default. For example, the probability of default given balance can be written as</p>
<p><span class="math display">\[
Pr(default = Yes | balance)
\]</span> The values of <span class="math inline">\(Pr(default = Yes|balance)\)</span>, which we abbreviate <span class="math inline">\(p(balance)\)</span>, will range between 0 and 1. Then for any given value of <code>balance</code>, a prediction can be made for <code>default</code>. For example, we can predict <span class="math inline">\(default = Yes\)</span> or any individaul for whom <span class="math inline">\(p(balance)&gt;0.5\)</span>. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as <span class="math inline">\(p(balance)&gt;0.1\)</span>.</p>
<section id="the-logistic-model" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="the-logistic-model"><span class="header-section-number">3.2.1</span> The Logistic Model</h3>
<p>How should we model the relationship between <span class="math inline">\(p(x) = Pr(y = 1| x)\)</span> and <span class="math inline">\(x\)</span>? (when we coded 0/1 for response values)</p>
<p>In section 4.2 we talked of using a linear regression model to represent these probabilities:</p>
<p><span class="math display">\[
p(x) = \beta_0 + \beta_1 x
\]</span> (4.1)</p>
<p><img src="fig4.2.png" class="img-fluid"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">default =</span> <span class="fu">ifelse</span>(default <span class="sc">==</span> <span class="st">"Yes"</span>,<span class="dv">1</span>,<span class="dv">0</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(default<span class="sc">~</span>balance,.) <span class="sc">%&gt;%</span> </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10,000 × 4
   default student balance income
     &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;
 1       0 No         730. 44362.
 2       0 Yes        817. 12106.
 3       0 No        1074. 31767.
 4       0 No         529. 35704.
 5       0 No         786. 38463.
 6       0 Yes        920.  7492.
 7       0 No         826. 24905.
 8       0 Yes        809. 17600.
 9       0 No        1161. 37469.
10       0 No           0  29275.
# ℹ 9,990 more rows</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = default ~ balance, data = .)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.23533 -0.06939 -0.02628  0.02004  0.99046 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -7.519e-02  3.354e-03  -22.42   &lt;2e-16 ***
balance      1.299e-04  3.475e-06   37.37   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1681 on 9998 degrees of freedom
Multiple R-squared:  0.1226,    Adjusted R-squared:  0.1225 
F-statistic:  1397 on 1 and 9998 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">default =</span> <span class="fu">ifelse</span>(default <span class="sc">==</span> <span class="st">"Yes"</span>,<span class="dv">1</span>,<span class="dv">0</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(default<span class="sc">~</span>balance,.) <span class="ot">-&gt;</span> lm_res</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">default =</span> <span class="fu">ifelse</span>(default <span class="sc">==</span> <span class="st">"Yes"</span>,<span class="dv">1</span>,<span class="dv">0</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> default) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">"brown4"</span>) <span class="sc">+</span> </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> lm_res<span class="sc">$</span>coefficients[<span class="dv">1</span>], <span class="at">slope =</span> lm_res<span class="sc">$</span>coefficients[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span> <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.25</span>), <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.2</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.2</span>) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The plot above is the same as plot on the left pane of 4.2. Here we see the problem with this approach, for balances close to zero, we predict a negative probability of default and if we predict for very large balances we would get values bigger than 1. These predictions are not sensible. This always happen for any time a straight line is fit to a binary response that is coded as 0 or 1, in pricpile we can always predict <span class="math inline">\(p(x)&lt;0\)</span> for some values of <span class="math inline">\(x\)</span> and <span class="math inline">\(p(x)&gt;1\)</span> for others.</p>
<p>To avoid this problem we must model <span class="math inline">\(p(x)\)</span> using a function that gives outputs between 0 and 1 for all values of <span class="math inline">\(x\)</span>. Many functions meet this description, however in logistis regression, we use <em>logistic function</em></p>
<p><span class="math display">\[
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}
\]</span> (4.2)</p>
<p>To fit the mode (4.2) we use a method called <em>maximum likelihood</em>, which we discuss in the next section. The right-hand panel of Figure 4.2 sows then fit of the logistic regression model to the <code>default</code>data.</p>
<p>Notice that for low balances we now predict the probability of default as close to , but never below zero. Likewise for high balances we predict a default probability close to, but never above, one.</p>
<p>The logistic function will always produce an <em>S-shaped</em> curve of this form, and so regardless of the value of <span class="math inline">\(X\)</span> we will obtain a sensible prediction.</p>
<p>After a bit of manipulation of (4.2) we find that</p>
<p>$$ <span class="math display">\[\begin{align}
p(x) &amp;= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1-p(x) &amp;= 1- \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1- p(x) &amp;= \frac{1}{1 + e^{\beta_0 + \beta_1 x}}  \\

1- p(x) \cdot e^{\beta_0 + \beta_1x} &amp;= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}\\

1-p(x) \cdot e^{\beta_0 + \beta_1 x} &amp;= p(x) \\

\frac{p(x)}{1-p(x)} &amp;= e^{\beta_0 + \beta_1} \space \space \space \space \space \space \space \space \space \space (4.3)

\end{align}\]</span> $$</p>
<p>the quantity <span class="math inline">\(p(x)/(1 - p(x))\)</span> is called the <em>odds</em>, and can take on any value between 0 and <span class="math inline">\(\infty\)</span>. Values of the odds close to 0 and <span class="math inline">\(\infty\)</span> indicate very low and very high probabilities of default, respectively.</p>
<p>For example, on average 1 in 4 people with an odds of 1/4 will default since <span class="math inline">\(p(x) = 0.2\)</span> implies an ods of <span class="math inline">\(\frac{0.2}{1-0.2} = 1/4\)</span>. Likewise on average 9/10 wpeople with an odds of 9 will default since <span class="math inline">\(p(x) = 0.9\)</span> implies an odds of <span class="math inline">\(\frac{0.9}{1-0.9} = 9\)</span>.</p>
<p>By taking the lograithm of both sides of (4.3) we arrive at</p>
<p><span class="math display">\[
\log(\frac{p(x)}{1-p(x)}) = \beta_0 + \beta_1 x
\]</span> (4.4)</p>
<p>the left-hand side is called the <em>log-odds-</em> or <em>logit</em>. We see that the logistic regression model (4.2) has a logit that is linear in <span class="math inline">\(X\)</span>.</p>
<p>Here increasing <span class="math inline">\(x\)</span> by one unit changes the log odds by <span class="math inline">\(\beta_1\)</span> (4.4), or it multiplies the odds by <span class="math inline">\(e^{\beta_1}\)</span>. But because the relationship between <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(x\)</span> in (4.2) is not a straight line, <span class="math inline">\(\beta_1\)</span> does not correspond to the change in <span class="math inline">\(p(x)\)</span> associated with a one-unit increase in <span class="math inline">\(x\)</span>. The amount that <span class="math inline">\(p(x)\)</span> changes due to a one-unit change in <span class="math inline">\(x\)</span> will depend on the current value of <span class="math inline">\(x\)</span>. But regardless of the value of <span class="math inline">\(x\)</span>, if <span class="math inline">\(\beta_1\)</span> is positive, then increasing <span class="math inline">\(x\)</span> will be associated with increasing <span class="math inline">\(p(x)\)</span>, vice versa. We can see that in the right hand panel of Figure 4.2.</p>
</section>
<section id="estiamting-the-regression-coefficients" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="estiamting-the-regression-coefficients"><span class="header-section-number">3.2.2</span> Estiamting the Regression Coefficients</h3>
<p><span class="math display">\[
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1}}
\]</span> (4.2) The coefficients of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown, must be estimated based on the available training data. We are going to use <em>maximum likelyhood</em> method. The basic intuition begind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span>} of default for each individual, using (4.2), corresponds as closely as possible to the individual’s observed default status. In other words, we try to find <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> such that plugging these estimates into the model for <span class="math inline">\(p(x)\)</span>,given in (4.2), yields a number close to 1 for all individuals who defaulted, and a number close to zero for all individuals who did not. We can formalize this mathematical equation with <em>likelihood function</em></p>
<p><span class="math display">\[
l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i) \prod_{i':y_{i'} = 0}(1-p(x_{i'}))
\]</span> (4.5)</p>
<p>The estimates <span class="math inline">\(\hat{\beta_0\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen to <em>maximize</em> this likelihood function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize model</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">logistic_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"glm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>) <span class="ot">-&gt;</span> log_model</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>log_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logistic Regression Model Specification (classification)

Computational engine: glm </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># setup the recipe </span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>default_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(default <span class="sc">~</span> balance, <span class="at">data =</span> default)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>default_recipe</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Recipe ──────────────────────────────────────────────────────────────────────</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Inputs </code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Number of variables by role</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>outcome:   1
predictor: 1</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set up the workflow</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(log_model) <span class="sc">%&gt;%</span> </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(default_recipe) <span class="ot">-&gt;</span> default_workflow</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>default_workflow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow ════════════════════════════════════════════════════════════════════
Preprocessor: Recipe
Model: logistic_reg()

── Preprocessor ────────────────────────────────────────────────────────────────
0 Recipe Steps

── Model ───────────────────────────────────────────────────────────────────────
Logistic Regression Model Specification (classification)

Computational engine: glm </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="do">## fit model</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>(<span class="at">conf.int =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  term         estimate std.error statistic   p.value  conf.low conf.high
  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   
2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> default, <span class="at">color =</span> .pred_class) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">scale_color_ipsum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Lets have a look at the Logistic Regression results again:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="do">## fit model</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>(<span class="at">conf.int =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  term         estimate std.error statistic   p.value  conf.low conf.high
  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept) -10.7      0.361        -29.5 3.62e-191 -11.4      -9.97   
2 balance       0.00550  0.000220      25.0 1.98e-137   0.00508   0.00594</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can measure the accuracy of the coefficient estimates by computing their standard errors. The <em>z-</em>statistic in the above plays the same role as <em>t</em> statistic in linear regression output. They are calculated from <span class="math inline">\(\hat{\beta_i} / \text{SE}(\hat{\beta_i})\)</span>, and so a large(absolute) value of the z-statistic indicates evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. This null hypothesis implies that <span class="math inline">\(p(x) = \frac{e^{\beta_0}}{1 + e^{\beta_0}}\)</span>. In other words, that the probability of default does not depend on blaance. p value is very low, we can reject <span class="math inline">\(H_0\)</span>; there is relationship between balance and probability of default. Intercept is not important here.</p>
</section>
<section id="making-predictions" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="making-predictions"><span class="header-section-number">3.2.3</span> Making predictions</h3>
<p>Once the coefficients have been estimated, we can compute the probability of default for any given credit card balance.</p>
<p><span class="math display">\[
\hat{p}(x) = \frac{e^{\hat{\beta_0} + \hat{\beta_1}x}}{1 +e^{\hat{\beta_0} + \hat{\beta_1}x}} = \frac{e^{-10.6513 + 0.0055 x}}{1 + e^{-10.6513 + 0.0055 x}}
\]</span></p>
<p>so for example given income $1,000 the predicted possiblity of default is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="sc">-</span><span class="fl">10.6513</span> <span class="sc">+</span> <span class="fl">0.0055</span> <span class="sc">*</span> <span class="dv">1000</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">10.6513</span> <span class="sc">+</span> <span class="fl">0.0055</span> <span class="sc">*</span> <span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.005758518</code></pre>
</div>
</div>
<p>which is below 1%. What about the probability of default for a person with $2,000 balance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="sc">-</span><span class="fl">10.6513</span> <span class="sc">+</span> <span class="fl">0.0055</span> <span class="sc">*</span> <span class="dv">2000</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">10.6513</span> <span class="sc">+</span> <span class="fl">0.0055</span> <span class="sc">*</span> <span class="dv">2000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5863023</code></pre>
</div>
</div>
<p>much higher 58%.</p>
<p>We can get all the predictions for our training data set form</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data=</span>default) <span class="sc">%&gt;%</span> </span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> <span class="fu">print</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(.pred_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10,000 × 7
   default student balance income .pred_class .pred_No .pred_Yes
   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;
 1 No      No         730. 44362. No             0.999 0.00131  
 2 No      Yes        817. 12106. No             0.998 0.00211  
 3 No      No        1074. 31767. No             0.991 0.00859  
 4 No      No         529. 35704. No             1.00  0.000434 
 5 No      No         786. 38463. No             0.998 0.00178  
 6 No      Yes        920.  7492. No             0.996 0.00370  
 7 No      No         826. 24905. No             0.998 0.00221  
 8 No      Yes        809. 17600. No             0.998 0.00202  
 9 No      No        1161. 37469. No             0.986 0.0138   
10 No      No           0  29275. No             1.00  0.0000237
# ℹ 9,990 more rows</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
  .pred_class     n
  &lt;fct&gt;       &lt;int&gt;
1 No           9858
2 Yes           142</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we classified 142 cases to default </span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># originally </span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>default <span class="sc">%&gt;%</span> </span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
  default     n
  &lt;fct&gt;   &lt;int&gt;
1 No       9667
2 Yes       333</code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 333 cases are actually default</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can implement qualitative predictors to the logistic regression using the dummy variable approach.. Lets predict default by only <code>student</code> variable</p>
<p><span class="math display">\[
x =
\begin{cases}
1 &amp; \text{if student} \\
0 &amp; \text{if not student}
\end{cases}
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logistic_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"glm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>) <span class="ot">-&gt;</span> log_model</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>default_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(default <span class="sc">~</span> student, <span class="at">data =</span> default)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(log_model) <span class="sc">%&gt;%</span> </span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(default_recipe) <span class="ot">-&gt;</span> default_workflow</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>(<span class="at">conf.int =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  term        estimate std.error statistic  p.value conf.low conf.high
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept)   -3.50     0.0707    -49.6  0          -3.65     -3.37 
2 studentYes     0.405    0.115       3.52 0.000431    0.177     0.629</code></pre>
</div>
</div>
<p>The coefficient is positive and the p value is statistically significant. This indicates that students tend to have higher default probabilities than non student:</p>
<p><span class="math display">\[
\widehat{Pr}(default = Yes | student = Yes) = \frac{e^{-3.50 + 0.4049 \times 1}}{1 + e^{-3.50 + 0.4049 \times 1}} = 0.0431
\]</span></p>
<p><span class="math display">\[
\widehat{Pr}(default = Yes | student = No) = \frac{e^{-3.50 + 0.4049 \times 0}}{1 + e^{-3.50 + 0.4049 \times 0}} = 0.0292
\]</span></p>
</section>
<section id="multiple-logistic-regression" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="multiple-logistic-regression"><span class="header-section-number">3.2.4</span> Multiple Logistic Regression</h3>
<p>We now consider the problem of predicting a binary response using a multiple predictors. We can generalize (4.4) as follows:</p>
<p><span class="math display">\[
\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1x_1 + \dots + \beta_p x_p
\]</span> (4.6)</p>
<p>(4.6) can be written as</p>
<p><span class="math display">\[
p(x) = \frac{e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}
\]</span> (4.7)</p>
<p>We again use maximum likelihood method to estimate <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>.</p>
<p>Lets use all of our variables in our model</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logistic_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"glm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>) <span class="ot">-&gt;</span> log_model</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(default <span class="sc">~</span> balance <span class="sc">+</span> income <span class="sc">+</span> student, <span class="at">data =</span> default)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(log_model) <span class="sc">%&gt;%</span> </span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(recipe) <span class="ot">-&gt;</span> default_workflow</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 4 × 5
  term            estimate  std.error statistic   p.value
  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 (Intercept) -10.9        0.492        -22.1   4.91e-108
2 balance       0.00574    0.000232      24.7   4.22e-135
3 income        0.00000303 0.00000820     0.370 7.12e-  1
4 studentYes   -0.647      0.236         -2.74  6.19e-  3</code></pre>
</div>
</div>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-10.8690452</td>
<td style="text-align: right;">0.4922555</td>
<td style="text-align: right;">-22.080088</td>
<td style="text-align: right;">0.0000000</td>
</tr>
<tr class="even">
<td style="text-align: left;">balance</td>
<td style="text-align: right;">0.0057365</td>
<td style="text-align: right;">0.0002319</td>
<td style="text-align: right;">24.737563</td>
<td style="text-align: right;">0.0000000</td>
</tr>
<tr class="odd">
<td style="text-align: left;">income</td>
<td style="text-align: right;">0.0000030</td>
<td style="text-align: right;">0.0000082</td>
<td style="text-align: right;">0.369815</td>
<td style="text-align: right;">0.7115203</td>
</tr>
<tr class="even">
<td style="text-align: left;">studentYes</td>
<td style="text-align: right;">-0.6467758</td>
<td style="text-align: right;">0.2362525</td>
<td style="text-align: right;">-2.737646</td>
<td style="text-align: right;">0.0061881</td>
</tr>
</tbody>
</table>
<p>Here there is a suprising result. According to the p-values balance and student variables are associted with the probability of default. However, coefficient for stundet dummy is negative; indicating that students are less likely to default than non-students. But this coefficint was positive in our previous analysis when we regressed probability of default by student: results showed that probability of default is twice as likely for students compared to non students.</p>
<p><img src="fig4.3.png" class="img-fluid"></p>
<p>Check out the fig 4.3. The orange and blue lines show the average default rates for students and non students, respectively, as a function of credit card balance. The nagive coefficient for student in the multiple logistic regression indicates that <strong>for a fixed value of balance and income</strong> a student is less likely to default than a non-student. We observe from the left hand panel that the student default ratge is at or below that of the nun student default rate for every value of balance. But the horizontal broken lines near the base of the plot, which show the default rates for students and non students averaged over all values of balance and income, suggest the opposite effect: the overall student default rate is higher than non student default rate. Consequently there is a positive coefficient for student in the single variable logistic regression output.</p>
<p>The right hand panel provies an explanation for this discrepancy: the varaibles student and blaance are correlated. Students tend to hold higher levels of debt, which is in turn assocaited with higer probability of default. In other words, students are more likely to have large credit card balances, which as we know from the left hand panel of figure 4.3 tend to be assocaited with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall students tend to default at a higher rate than non students. This is an important distinction. A student is riskier than a non student if no information about the student’s credit card balance is available. However, that student is less risky than a non student with the same credit card balance.</p>
<p>This simple example illustrates the dangers and subtleties assocaited with performing regressions involving only a single predictor when other predictors may als obe relevant. The results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon seen in Figure 4.3 is known as <em>confounding</em>.</p>
<p>Lets put the estimates to our estimated probability function</p>
<p>$$ (x) = </p>
<p>{ 1 + e^{-10.86 + 0.00574 balance + 3e-6 income - 0.646 Student} } $$ We can make predictions: a student with a credit card balnce of $1,500 and an income of $40,000 has an estimated probability of default:</p>
<p><span class="math display">\[
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
} = 0.05780859
\]</span></p>
<p>A non student with the same balance and income has an estiamted probability of default of</p>
<p><span class="math display">\[
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
} = 0.1048655
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>default_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> <span class="fu">tibble</span>(<span class="at">income =</span> <span class="dv">40000</span>, <span class="at">balance =</span> <span class="dv">1500</span>, <span class="at">student =</span> <span class="fu">c</span>(<span class="st">"Yes"</span>,<span class="st">"No"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 6
  income balance student .pred_class .pred_No .pred_Yes
   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;
1  40000    1500 Yes     No             0.942    0.0579
2  40000    1500 No      No             0.895    0.105 </code></pre>
</div>
</div>
</section>
<section id="logistic-regression-for-2-response-classes" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="logistic-regression-for-2-response-classes"><span class="header-section-number">3.2.5</span> Logistic Regression for &gt; 2 Response Classes</h3>
<p>What if our response variable has more than two classes. Like medical condition in the emergency room: <code>stroke</code>, <code>drug overdose</code>, <code>epileptic seizure</code>. In this setting we wish to model both <span class="math inline">\(Pr(Y = stroke | x)\)</span> and <span class="math inline">\(Pr(Y = drug \space overdose | x)\)</span>, with the remaining <span class="math inline">\(Pr(Y = epileptic \space seizure |x) = 1 - Pr(Y= stroke | x) - Pr(Y = drug \space overdose | x)\)</span>. We can do it my extending logistic regression but <em>Linear Discriminant Analysis</em> is much suitable for this task</p>
</section>
</section>
<section id="linear-discriminant-analysis" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="linear-discriminant-analysis"><span class="header-section-number">3.3</span> Linear Discriminant Analysis</h2>
<p>Logistic regression involves directly modeling <span class="math inline">\(Pr(Y = k | X = x)\)</span> using the logistic function, given by (4.7) for the case of two response classes. In statistical jargon, we model the conditional distribution of the response <span class="math inline">\(Y\)</span>, given the predictor(s) <span class="math inline">\(X\)</span>.</p>
<p>We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors <span class="math inline">\(X\)</span> separately in each of the response classes (i.e.&nbsp;given <span class="math inline">\(Y\)</span>), and then use Bayes’ theorem to flip these around into estimates for <span class="math inline">\(Pr(Y = k | X = x)\)</span>. When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression.</p>
<p>Why do we need another method when we have logistic regression?</p>
<ul>
<li><p>When the classes are well separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discrimnant analysis does not suffer from this problem.</p></li>
<li><p>If <em>n</em> is small and the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes the linear discriminant model is again more stable than th elogistic regression model</p></li>
<li><p>LDA is popular when we have more than two response classes.</p></li>
</ul>
<section id="using-bayes-theorem-for-classification" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="using-bayes-theorem-for-classification"><span class="header-section-number">3.3.1</span> Using Bayes’ Theorem for Classification</h3>
<p>We want to classify an observation into one of <span class="math inline">\(K\)</span> classes, where <span class="math inline">\(2 \leq K\)</span>. So <span class="math inline">\(Y\)</span> can take on <span class="math inline">\(K\)</span> possible distinct and unordered values. Let <span class="math inline">\(\pi_k\)</span> represent the overall or <em>prior</em> probability that a randomly chosen observation comes from the <span class="math inline">\(k\)</span>th class.; this is the probability that a given observation is associated with the <em>k</em>th category of the response variable <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(f_k(x) \equiv Pr(X = x | Y = k)\)</span> denote the <em>density</em> function of <span class="math inline">\(X\)</span> for an observation that comes from the <em>k</em> th class. In other words <span class="math inline">\(f_k(x)\)</span> is relatively large if there is a high probability that an observation in the <em>k</em> th class has <span class="math inline">\(X \approx x\)</span>, and <span class="math inline">\(f_k(x)\)</span> is small if it is very unlikely that an observation in the <em>k</em>th class has <span class="math inline">\(X \approx x\)</span>. Then <em>Bayes’s theorem</em> states that</p>
<p><span class="math display">\[
Pr(Y = k | X = x) = \frac{
\pi_k\,f_k(x)
}
{
\sum^K_{l = 1}\,\pi_l\,f_l(x)
}
\]</span> (4.10)</p>
<p>We will use the abbreviation <span class="math inline">\(p_k(X) = Pr(Y = k | X)\)</span>. This suggests that instead of directly computing <span class="math inline">\(p_k(X)\)</span> as Section 4.3.1, we can simply plug in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(X)\)</span> into (4.10). In general, estimating <span class="math inline">\(\pi_k\)</span> is easy if we have a random sample of <span class="math inline">\(Y\)</span>s from the population: we simply compute the fraction of the training observations that belong to the <em>k</em>th class. However, etimating <span class="math inline">\(f_k(X)\)</span> tends to be more challenging, unless we assume some simple forms for these densities. We refer to <span class="math inline">\(p_k(x)\)</span> as the <em>posterior</em> probability than an observation <span class="math inline">\(X=x\)</span> belongs to the <em>k</em>th class. That is, it is the probability that the observation belongs to the <em>k</em>th class, <em>given</em> the predictor value for that observation.</p>
<p>We know from Ch.2 that the Bayes classifier, which classifies an observation to the class for which <span class="math inline">\(p_k(X)\)</span> is largest, has the lowest possible error rate out of all classifiers. Therefore, if we can find a way to estimate <span class="math inline">\(f_k(X)\)</span> then we can develop a classifier that approximates the bayes classifier:</p>
</section>
<section id="linear-discriminant-analysis-for-p1" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="linear-discriminant-analysis-for-p1"><span class="header-section-number">3.3.2</span> Linear Discriminant Analysis for p=1</h3>
<p>Assume we have one predictor. We would like to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> that we can plug into (4.10) in order to estimate <span class="math inline">\(p_k(x)\)</span>. We will then classify an observation to the class for which <span class="math inline">\(p_k(x)\)</span> is greatests. In order to estimate <span class="math inline">\(f_k(x)\)</span> we will first make some assumptions about this form.</p>
<p>Suppose we assume <span class="math inline">\(f_k(x)\)</span> is <em>normal</em> or <em>Gaussian</em>. In the one dimensional setting, the normal density takes the form</p>
<p><span class="math display">\[
f_k(x) = \frac{
1
}
{
\sqrt{2\pi}\,\sigma_k
}\,exp\,\left(-\frac{1}{2\sigma^2_k}(x - \mu_k)^2\right)
\]</span> (4.11)</p>
<p>where <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\sigma^2_k\)</span> are the mean and variance parameters for the <em>k</em>th class. For now, let us further assume that <span class="math inline">\(\sigma_1^2=\dots=\sigma^2_K\)</span>: that is, there is shared variance term across all <span class="math inline">\(K\)</span> classes, which for simplicty we can denote by <span class="math inline">\(\sigma^2\)</span>. Plugging (4.11) into (4.10) we find that</p>
<p><span class="math display">\[
p_k(x) = \frac{
\pi_k\,\frac{1}{\sqrt{2\pi}\,\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_k)^2)
}
{
\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_l)^2)
}
\]</span> (4.12)</p>
<p>The bayes classifier involves assigning an observation <span class="math inline">\(X=x\)</span> to the class for which (4.12) is largest. Taking the log of (4.12) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which</p>
<p><span class="math display">\[
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} \,- \frac{\mu_k^2}{2\sigma^2}\,+ \log(\pi_k)
\]</span> (4.13)</p>
<p>is largest. For instance if <span class="math inline">\(K = 2\)</span> and <span class="math inline">\(\pi_1 = \pi_2\)</span>, then the Bayes clasffier assigns an obsetvation to classs 1 if <span class="math inline">\(2x(\mu_1 - \mu_2) &gt; \mu_1^2 - \mu_2^2\)</span>, ann to class 2 otherwise. In this case, the Bayes desicion boundary corresponds to the point where</p>
<p>$$ <span class="math display">\[\begin{align}
\delta_k - delta_j &amp;= 0 \\
x \cdot \frac{\mu_k}{\sigma} - \frac{\mu^2_k}{2\sigma^2} + \log(\pi_k) &amp;- x \cdot \frac{\mu_j}{\sigma}  \frac{\mu_j^2}{2\sigma^2} + \log(\pi_j) = 0 \\
x(\frac{\mu_k - \mu_j}{\sigma}) &amp;+ \frac{\mu_j^2 - \mu_k^2}{2\sigma^2} + \log(\frac{\pi_k}{\pi_j}) = 0 \\
x &amp;= \frac{\frac{\mu^2_k -\mu^2_j}{2\sigma^2} - log(\frac{\pi_k}{\pi_j})}{\frac{\mu_k - \mu_j}{\sigma}}
\end{align}\]</span></p>
<p><span class="math display">\[
(4.14)
for this spesific case:
\]</span> x = = $$</p>
<p>An example is shown in the left hand panel of fig 4.4</p>
<p><img src="fig4.4.png" class="img-fluid"></p>
<p>The two normal density functions that are displayed, <span class="math inline">\(f_1(x)\)</span> and <span class="math inline">\(f_2(x)\)</span>, represent two distinct classes. The mean and the variance parameters for the two desity functions are <span class="math inline">\(\mu_1 = -1.25, \space \mu_2 = 1.25\)</span>, and <span class="math inline">\(\sigma_1^2 = \sigma_2^2 = 1\)</span>. The two densities overlap, and so given that <span class="math inline">\(X=x\)</span>, there is some uncertainty about the class to which observation belongs. If we assume that an observation is equally likely to come from either class–that is <span class="math inline">\(\pi_1 = \pi_2 = 0.5\)</span>– then by inspection of (4.14), we see that the Bayes classfier assigns the observation to class 1 if <span class="math inline">\(x&lt;0\)</span> and class 2 otherwise. Note that in this case, we can compute the Bayes classfier because we know that <span class="math inline">\(X\)</span> is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation we are not able to calcualte Bayes classifier.</p>
<p>In practice, even if we are quite certain of our assumption that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, we still have to estimate the parameters <span class="math inline">\(\mu_1, \dots, \mu_K,\space \pi_1,\dots, \pi_K,\)</span> and <span class="math inline">\(\sigma^2\)</span>. The <em>linear discriminant analysis</em> (LDA) method approximates the bayes classfier vy plugging estimates fro <span class="math inline">\(\pi_k, \mu_k\)</span> and <span class="math inline">\(\sigma^2\)</span> into (4.13). In particular the following estimates are used.:</p>
<p>$$ <span class="math display">\[\begin{align}
\hat{\mu}_k &amp;= \frac{1}{n_k}\sum_{i:y_i=k} x_i \\

\hat{\sigma}^2 &amp;= \frac{1}{n-K}\sum^K_{k=1} \sum_{i:y_i =k}(x_i - \hat{\mu}_k)^2

\end{align}\]</span> $$ (4.15) <strong>Important</strong></p>
<p>where <em>n</em> is the total number of training observations, and <em>n_k</em> is the number of training observations in the <em>k</em>th class. The estimate for <span class="math inline">\(\mu_k\)</span> is simply the average of all the training observations from the <em>k</em>th class, <strong>while</strong> <span class="math inline">\(\hat{\sigma}^2\)</span> <strong>can be seen as a weighted average of the sample variances for each of the</strong> <span class="math inline">\(K\)</span> <strong>classes</strong>. Sometimes we have knowledge of the class membership probabilities <span class="math inline">\(\pi_1, \dots, \pi_K\)</span>, which can be used directly. In the absence of any additional information, LDA estimates <span class="math inline">\(\pi_K\)</span> using the proportion of the training observations that belong to the <em>k</em>th class. In other words,</p>
<p><span class="math display">\[
\pi_k = n_k /n
\]</span> The LDA classifer plugs the estimates given in (4.15) and (4.16) into (4.13), and assigns an observation <span class="math inline">\(X=x\)</span> to the class for which</p>
<p><span class="math display">\[
\hat{\delta}(x) = x \cdot \,\frac{\hat{u}_k}{\hat{\sigma}^2} \,-\,\frac{\hat{u}^2_k}{2\,\hat{\sigma}^2}\,+\,\log(\hat{\pi}_k)
\]</span> (4.17)</p>
<p>is largest. The word <em>linear</em> in classfier’s name stems from the fact that <em>discriminant functions</em> <span class="math inline">\(\hat{\delta}_k(x)\)</span> in (4.17) are linear functions of <span class="math inline">\(x\)</span>.</p>
<p>The right hand panel of Fig 4.4 displays a histogram of a random sample <span class="math inline">\(n=20\)</span> observations from each class. To implement LDA, we behan by estimating <span class="math inline">\(\pi_k, \mu_k, \sigma^2\)</span> using (4.15) and (4.16). We then computed the desicion boundy, shown as a black solid line, that results from assigning an observation to the clas for which (4.17) is largest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="dv">20</span>, <span class="at">mean =</span> <span class="sc">-</span><span class="fl">1.25</span>, <span class="at">sd =</span><span class="dv">1</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="dv">20</span>, <span class="at">mean=</span><span class="fl">1.25</span>, <span class="at">sd =</span><span class="dv">1</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">tibble</span>(<span class="at">x =</span> a, <span class="at">class =</span> <span class="dv">1</span>),<span class="fu">tibble</span>(<span class="at">x =</span> b, <span class="at">class =</span><span class="dv">2</span>))</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">color =</span> class, <span class="at">group =</span> class) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">show.legend =</span> F) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">d_x_1 =</span> x <span class="sc">*</span> <span class="sc">-</span><span class="fl">1.25</span><span class="sc">/</span><span class="dv">1</span> <span class="sc">-</span> (<span class="sc">-</span><span class="fl">1.25</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>),</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">d_x_2 =</span> x <span class="sc">*</span> <span class="fl">1.25</span><span class="sc">/</span><span class="dv">1</span> <span class="sc">-</span> (<span class="fl">1.25</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>),</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">.pred.class =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(d_x_2) <span class="sc">&gt;</span> <span class="fu">abs</span>(d_x_1) ,<span class="dv">1</span>,<span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">color =</span> class, <span class="at">group =</span> class) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">show.legend =</span> F)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-22-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>BDB <span class="ot">=</span> ((((<span class="sc">-</span><span class="fl">1.25</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="fl">1.25</span><span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">log</span>(<span class="fl">0.5</span><span class="sc">/</span><span class="fl">0.5</span>)) <span class="sc">/</span> ((<span class="sc">-</span><span class="fl">1.25</span> <span class="sc">-</span> <span class="fl">1.25</span>)<span class="sc">/</span><span class="dv">1</span>) <span class="co"># = 0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">d_x_1 =</span> x <span class="sc">*</span> <span class="sc">-</span><span class="fl">1.25</span><span class="sc">/</span><span class="dv">1</span> <span class="sc">-</span> (<span class="sc">-</span><span class="fl">1.25</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>),</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">d_x_2 =</span> x <span class="sc">*</span> <span class="fl">1.25</span><span class="sc">/</span><span class="dv">1</span> <span class="sc">-</span> (<span class="fl">1.25</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>),</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">.pred.class =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(d_x_2) <span class="sc">&gt;</span> <span class="fu">abs</span>(d_x_1) ,<span class="dv">1</span>,<span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">color =</span> class, <span class="at">group =</span> class) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">show.legend =</span> F)  <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Here in this case bayess classfier assigns the observation to class 1 if For any <span class="math inline">\(x &lt;0\)</span> and 2 if <span class="math inline">\(x&gt;0\)</span>. we can compute the Bayess classfier because we know that <span class="math inline">\(X\)</span> is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In real-life we cannot calculate bayes classfier</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(class) <span class="sc">%&gt;%</span> </span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean_hat =</span> <span class="fu">mean</span>(x), <span class="at">var_hat =</span> <span class="fu">sd</span>(x)<span class="sc">^</span><span class="dv">2</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 3
  class mean_hat var_hat
  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
1     1    -1.58   0.751
2     2     1.35   0.781</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(class) <span class="sc">%&gt;%</span> </span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">count =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pi_hat =</span> count<span class="sc">/</span><span class="fu">sum</span>(count))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 3
  class count pi_hat
  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;
1     1    20    0.5
2     2    20    0.5</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">d_k_c1 =</span> x <span class="sc">*</span> (<span class="sc">-</span><span class="fl">1.58</span><span class="sc">/</span><span class="fl">0.751</span>) <span class="sc">-</span> (<span class="sc">-</span><span class="fl">1.58</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span><span class="fl">0.751</span>)) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>),</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">d_k_c2 =</span> x <span class="sc">*</span> (<span class="fl">1.35</span> <span class="sc">/</span> <span class="fl">0.781</span>) <span class="sc">-</span> (<span class="fl">1.35</span><span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span><span class="fl">0.781</span>)) <span class="sc">+</span> <span class="fu">log</span>(<span class="fl">0.5</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pred.class =</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(d_k_c1 <span class="sc">&gt;</span> d_k_c2,<span class="dv">1</span>,<span class="dv">2</span>)), <span class="at">class =</span> <span class="fu">factor</span>(class)) <span class="sc">%&gt;%</span> </span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> class, <span class="at">estimate =</span> pred.class) <span class="sc">%&gt;%</span> </span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="at">type =</span> <span class="st">"heatmap"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(discrim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'discrim'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dials':

    smoothness</code></pre>
</div>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="fu">factor</span>(class)) <span class="ot">-&gt;</span> d</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="fu">discrim_linear</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"MASS"</span>) <span class="ot">-&gt;</span> lda_spec</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>lda_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(class<span class="sc">~</span>x,<span class="at">data =</span>d)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lda_spec) <span class="sc">%&gt;%</span> </span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(lda_recipe) <span class="ot">-&gt;</span> lda_workflow</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> d) <span class="sc">%&gt;%</span> </span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data=</span>d) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 40 × 5
        x class .pred_class .pred_1    .pred_2
    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         &lt;dbl&gt;      &lt;dbl&gt;
 1 -2.73  1     1             1.00  0.0000459 
 2  0.327 1     2             0.155 0.845     
 3 -2.21  1     1             1.00  0.000340  
 4 -2.17  1     1             1.00  0.000391  
 5 -3.25  1     1             1.00  0.00000637
 6 -1.52  1     1             0.995 0.00463   
 7 -1.57  1     1             0.996 0.00393   
 8 -1.88  1     1             0.999 0.00119   
 9 -1.36  1     1             0.991 0.00869   
10 -0.822 1     1             0.937 0.0633    
# ℹ 30 more rows</code></pre>
</div>
</div>
<p>To sum up; the LDA classfier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance <span class="math inline">\(\sigma^\)</span>, and plugging estimates for these parameters into the Bayes classfier. In section 4.4.4, we will consider a less stringent set of assumptions, by allowing the observations in the <em>k</em>th class to have a class specific variance, <span class="math inline">\(\sigma^2_k\)</span></p>
</section>
<section id="linear-discriminant-analysis-for-p-1" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="linear-discriminant-analysis-for-p-1"><span class="header-section-number">3.3.3</span> Linear Discriminant Analysis for p &gt; 1</h3>
<p>We are going to extend the LDA classfier to the case of multiple predictors. To do this, we will assume that <span class="math inline">\(X = (x_1, x_2, \dots, x_p)\)</span> is drawn from a <em>multip variate Gaussian</em> (or multivariate normal) distribution, with a class-specific meean vector and a common covariance matrix.</p>
<p>The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional distribution, as in (4.11), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with <span class="math inline">\(p=2\)</span> are shown in Figure 4.5.</p>
<p><img src="fig4.5.png" class="img-fluid"></p>
<p>The height of the surface at any particular point represnts the probability that bot <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> fall in a small region on that point. In either panel, if the surface is cut along the <span class="math inline">\(X_1\)</span> axis or along the <span class="math inline">\(X_2\)</span> axis, the resulting cross section will have the shape of a one dimensional normal distribution. The left hand panel illustrates an example in which <span class="math inline">\(var(x_1) = var(x_2)\)</span> and <span class="math inline">\(cor(x_1,x_2) = 0\)</span>; this surface has a charactersitic <em>bell shpae</em>. However, the bell shape will be distorted if the preditors are correlated or have unequal variances, as in illusterated in the right hand panel. In this case, the base of the bell will have an elliptical, rather than circular shape.</p>
<p>To indicate that a <em>p-dimnensional</em> random varaible <span class="math inline">\(X\)</span> has a multipvariate Gaussian distribution we write <span class="math inline">\(X \sim N(\mu,\sum)\)</span>. Here <span class="math inline">\(E(X) = \mu\)</span> is the mean of <span class="math inline">\(X\)</span> ( a vector with <em>p</em> components), and <span class="math inline">\(Cov(X) = \sum\)</span> is the $ p$ covariance matrix of <span class="math inline">\(X\)</span>. Formally, the multivariate Gaussian density is defined as</p>
<p><span class="math display">\[
f(x) = \frac{1}{(2\pi)^{p/2}|\sum|^{1/2}} \exp\left(-\frac{1}{2}(x -\mu)^T \textstyle\sum^{-1}(x -\mu) \right)
\]</span> (4.18)</p>
<p>In this case of <span class="math inline">\(p&gt;1\)</span> predictors, the LDA classfier assumes that the observations in the <em>k</em>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k,\textstyle\sum)\)</span>, where <span class="math inline">\(\mu_k\)</span> is class-specific mean vector, adn <span class="math inline">\(\textstyle\sum\)</span> is a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. Plugging the density function for the <em>k</em>th class, <span class="math inline">\(f_k(X=x)\)</span> into (4.10) and performing a little bit of algebra reveals that the Bayes classfier assigns an observation <span class="math inline">\(X=x\)</span> to the class for which</p>
<p><span class="math display">\[
\delta_k(x) = x^T \textstyle\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\textstyle\sum^{-1}\mu_k + \log \pi_k
\]</span> (4.19)</p>
<p>is largest. This is the vector/matrix version of 4.13.</p>
<p>An example is shown in the left-hand panel of Figure 4.6</p>
<p><img src="fig4.6.png" class="img-fluid"></p>
<p>Three equally sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represetn regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes desicion boundaries. In other words, they represent the set of values <span class="math inline">\(x\)</span> for which <span class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>; i.e.</p>
<p><span class="math display">\[
x^T\textstyle\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\textstyle\sum^{-1} = x^T\textstyle\sum^{-1}\mu_l - \frac{1}{2}\mu_l^T\textstyle\sum^{-1}\mu_l
\]</span> (4.20)</p>
<p>for <span class="math inline">\(k\neq1\)</span>. The <span class="math inline">\(\log \pi_k\)</span> term from 4.19 has dissapeared because each of the three classes has the same number ob observations; i.e.&nbsp;<span class="math inline">\(\pi_k\)</span> is the same for each class. No that there are three lines representing the Bayes desciion boundaries because there are three <em>pairs of clases</em> among the three classes. That is, on desicion boundary separets class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classfier will classify an observation according to the region in which it is located.</p>
<p>Once again, we need to estimate the unknown parametrs <span class="math inline">\(\mu_1, \dots, \mu_K, \pi_1, \dots, \pi_K\)</span> and <span class="math inline">\(\sum\)</span>; the formulas are similar to those used in the one dimensiona lcase, given in 4.15. To assign a new observation <span class="math inline">\(X=x\)</span>, LDA plugs these estiamtes into 4.19 and classfiers to the class for which <span class="math inline">\(\hat{\delta}(x)\)</span> is largest. Note that in 3.19 <span class="math inline">\(\delta_k(x)\)</span> is a linear function of <span class="math inline">\(x\)</span>; that is the LDA desicion rule depend on <span class="math inline">\(x\)</span> only through a linear combination of its elements. Once again, this is the reason for the word <em>linear</em> in LDA.</p>
<p>In the right hand panel of Fig 4.6, 20 observations drawn from each of the three classes are displayed, and the resulting LDA desicion boundaries are shown as solid black lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test erro rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively. This indicates that LDA is performing well on this data.</p>
<p>We can perfrom LDA on the <code>default</code> data in order to predict whether or not an individaul will default on the bassi of credit card balance and student status. The LDA model fit to the 10,000 training samples results in a <em>training</em> error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.</p>
<ul>
<li><p>First of all training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classfier to perform worse if we use it to predict whether or not. anew set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parametsr <em>p</em> to number of samples <em>n</em>, the more we expect this <em>overfitting</em> to play a role. For these data, we don’t expect tihs to be a problem, since <span class="math inline">\(p=2\)</span> and <span class="math inline">\(n = 10,000\)</span>.</p></li>
<li><p>Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individaul will not default, regardless of his or her credit car balance and student statsu, will result in an error rate of 3.33%. In other words, the trivial <em>null</em> classifier will achive an error rate thatis only a bit higher than the LDA training set error rate.</p></li>
</ul>
<p>In pracitice, a binay classifiar such as this one can make two types of errors: it can incorretly assign an individual who default to the <em>no default</em> category, or it can incorretly assign an individual who does not defaul to <em>default</em> category. It is often of interst to determine which of these two types of errors are being made. A <em>confusion matrix</em> show for the <code>default</code> data in Table 4.4 is a convinient way to dispaly this information.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">discrim_linear</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"MASS"</span>) <span class="ot">-&gt;</span> lda_spec</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>lda_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(default <span class="sc">~</span> balance <span class="sc">+</span> student, <span class="at">data =</span> default)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="fu">workflow</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(lda_spec) <span class="sc">%&gt;%</span> </span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(lda_recipe) <span class="ot">-&gt;</span> lda_workflow</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow [trained] ══════════════════════════════════════════════════════════
Preprocessor: Recipe
Model: discrim_linear()

── Preprocessor ────────────────────────────────────────────────────────────────
0 Recipe Steps

── Model ───────────────────────────────────────────────────────────────────────
Call:
lda(..y ~ ., data = data)

Prior probabilities of groups:
    No    Yes 
0.9667 0.0333 

Group means:
      balance studentYes
No   803.9438  0.2914037
Yes 1747.8217  0.3813814

Coefficients of linear discriminants:
                    LD1
balance     0.002244397
studentYes -0.249059498</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>one thing we can look in LDA outpit is the group means. We see that defaulted people have 2 times balance than not defaulted. % of students are higher for defaulted people.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span>default)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10,000 × 7
   default student balance income .pred_class .pred_No .pred_Yes
   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;
 1 No      No         730. 44362. No             0.997  0.00313 
 2 No      Yes        817. 12106. No             0.997  0.00281 
 3 No      No        1074. 31767. No             0.984  0.0156  
 4 No      No         529. 35704. No             0.999  0.00122 
 5 No      No         786. 38463. No             0.996  0.00407 
 6 No      Yes        920.  7492. No             0.995  0.00454 
 7 No      No         826. 24905. No             0.995  0.00491 
 8 No      Yes        809. 17600. No             0.997  0.00270 
 9 No      No        1161. 37469. No             0.977  0.0234  
10 No      No           0  29275. No             1.00   0.000102
# ℹ 9,990 more rows</code></pre>
</div>
</div>
<p>Lets have a confusion matrix</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction   No  Yes
       No  9644  252
       Yes   23   81</code></pre>
</div>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sens</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class, <span class="at">estimator =</span> <span class="st">"binary"</span>) <span class="sc">%&gt;%</span> <span class="fu">print</span>() <span class="co"># overall accuracy; specificity; the goal of LDA is to max this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 sens    binary         0.998</code></pre>
</div>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sens</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class,<span class="at">event_level =</span> <span class="st">"second"</span>) <span class="co"># accuracy of Yes predictions; 1-0.243 = 0.757 -&gt; sensitivity; the percentage o true defaulters that are identified</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 sens    binary         0.243</code></pre>
</div>
</div>
<p>This confusion matrix table shows 104 people would default. Of those people, 81 is actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorretly labeled. This looks like a pretty low error rate! However, of the 333 individuals who default, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who default is very high. From the perspective of a credit card company that is tryibng to identify high-risk individuals, an error rate of 75.7% among individauls who default may be well unaccapteble.</p>
<p>Class-specific performance is also important in medicine and biology where the terms <em>sensivity</em> and <em>specificty</em> characterize the performance of a classifer or screening test. In this case the sensivity is the percentage of true defaulters that are identified, a low 34.3% in this case. The specifity is the percentage of nond-defaulters that are correctly identified, here (<span class="math inline">\(1-23/9,667 = 99.8%\)</span>)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">accuracy</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy binary         0.972</code></pre>
</div>
</div>
<p>Why does LDA do such a poor job of classfiying the customers who default? In other words, why does it have such a low sensivity? AS we have seen, LDA is trying to approximate the Bayes classfier, which has the lowest <em>total</em> error rate out of all classfiers (if the Gaussian model is correct). That is the Bayes classfier will yield the smallest possible total number of misclassifed observaitons, irrespective of which class the errors come from. That is, some misclassfications will result from incorretly assigninga customer who does not default tot he default class, and others will result from incorreclty assigning a customer who defaults tot he non default class. In contrast a credt card company might particualrly wihch to aviod incorrectly classifying an individual who wilkl defualt, where as incorrectly classfiyng an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classfier that better meets the credit card company’s needs.</p>
<p>The Bayes classfier works by assignig an observation to the class for which the posterior probability <span class="math inline">\(p_k(X)\)</span> is greatest. In the two-class case, this amounts to assigning an observation to the <em>default</em> class if</p>
<p><span class="math display">\[
Pr(default = Yes | X = x) &gt; 0.5
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span>default) <span class="sc">%&gt;%</span> </span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(.pred_Yes <span class="sc">&gt;</span> <span class="fl">0.45</span>, .pred_Yes <span class="sc">&lt;</span> <span class="fl">0.55</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 39 × 7
   default student balance income .pred_class .pred_No .pred_Yes
   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;
 1 Yes     No        1964. 39055. Yes            0.488     0.512
 2 Yes     No        1992. 42133. Yes            0.456     0.544
 3 Yes     No        1981. 28128. Yes            0.468     0.532
 4 Yes     No        1964. 50554. Yes            0.489     0.511
 5 Yes     No        1972. 34363. Yes            0.479     0.521
 6 No      Yes       2027. 20470. No             0.545     0.455
 7 No      Yes       2052. 13131. No             0.516     0.484
 8 Yes     Yes       2036. 14436. No             0.535     0.465
 9 Yes     No        1923. 56203. No             0.537     0.463
10 No      No        1956. 45508. Yes            0.499     0.501
# ℹ 29 more rows</code></pre>
</div>
</div>
<p>Thus the bayes classfier, andby extension LDA, uses a threshold of 50% for the posterior probabiblity of default in order to assign an observation to the <em>default</em> class. However, if we are concerned about incorretly predicting the default status for individauls who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the <em>default</em> class. In other words, instead of assigning an observation to the <em>default</em> class if 4.21 holds, we could instead assign an observation to this class if</p>
<p><span class="math display">\[
Pr(default = Yes | X = x) &gt; 0.2
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.pred_class =</span> <span class="fu">ifelse</span>(.pred_Yes <span class="sc">&gt;</span> <span class="fl">0.2</span>, <span class="st">"Yes"</span>, <span class="st">"No"</span>), <span class="at">.pred_class =</span> <span class="fu">factor</span>(.pred_class)) <span class="sc">%&gt;%</span> </span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction   No  Yes
       No  9432  138
       Yes  235  195</code></pre>
</div>
</div>
<p>Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4%. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this imporvement comes at a cost: now 235 individauls who do not default are incorrectly classified. As a result, the overall error rate has increased in the total error rate to be a small price to pay for more accurate identification of individauls who do indeed default.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.pred_class =</span> <span class="fu">ifelse</span>(.pred_Yes <span class="sc">&gt;</span> <span class="fl">0.2</span>, <span class="st">"Yes"</span>, <span class="st">"No"</span>), <span class="at">.pred_class =</span> <span class="fu">factor</span>(.pred_class)) <span class="sc">%&gt;%</span> </span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sens</span>(<span class="at">truth =</span> default, <span class="at">estimate =</span> .pred_class, <span class="at">event_level =</span> <span class="st">"second"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 sens    binary         0.586</code></pre>
</div>
</div>
<p><img src="fig4.7.png" class="img-fluid"></p>
<p>Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. Using a threshold of 0.5 as in 4.21 minimizes th eoverall error rate, shown as a black solid line. This is to be expected since the Bayes classifier uses a threshold of 0.5 and is known to have the lowest overall error rate. But when a threshold of 0.5 is used the error rate among the individuals who default is quite high (blue dashed line). As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individiuals who do not increasess. How can we decide which threshold value is the best? Such a decision must be based on <em>domain knowledge</em> such as detailed information about the costs associated with default.</p>
<p>The <em>ROC curve</em> is a popular graphic for simultaneously displaying the two types of errors for all possible threshold. The name “ROC” means <em>receiver operating characteristics</em>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">roc_curve</span>(<span class="at">truth =</span> default,.pred_No) <span class="sc">%&gt;%</span> </span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"False positive rate : 1 - specifity"</span>, <span class="at">y =</span> <span class="st">"True positive rate: sensitivity"</span>, <span class="at">title =</span> <span class="st">"ROC curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Chapter4_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: A ROC curve for the LDA classifer on the default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctşy identified, using a given threshold value. The flase posiitve rate is 1- speficity: the fraction of non defaulters that we classify incorretly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner i dicating a high true positive rate and a low false positive rate. The dotted line represts the no information classifier: this is what we would expect if student statust and credit card balance are not associated with probability of default.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The overall performance of a classifier, summarized over all possible thresholds, is given by the <em>area under the ROC curve (AUC)</em> An ideal ROC curve will hug the top left corner so the larger the AUC the better the classifier. For this data the AUC is 0.95.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>lda_workflow <span class="sc">%&gt;%</span> </span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>(<span class="at">new_data =</span> default) <span class="sc">%&gt;%</span> </span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">roc_auc</span>(<span class="at">truth =</span> default,.pred_No)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 roc_auc binary         0.950</code></pre>
</div>
</div>
<p>which is close to maximum of 1 so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5. ROC curves are useful for comparing different classfiers since they take into account of all possible thresholds. It turns out that the ROC curve for the logistic regression model of section 4.3.4 fit to these data is virtually indistinguaslibe from this one for the LDA model.</p>
<p>As we have seen above varying the classfier threshold changes its true positive and false positive rate. These are also called the <em>sensivity</em> and one minus the <em>specificity</em> of our classifer. Since there is an almost bewildering array of terms used in this context, we now give a summary.</p>
<p><img src="tab4.6.png" class="img-fluid"></p>
<p>Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemology literature we think of “+” as “disease” that we are trying to detect, and “-” as the “non disease” state. To make the connection to the classical hypothessi testing literatureü, we think of “-” as the null hypothessi and “+”. as the alternative (non-null) hypothessi. In the context of the default data, “+” indicates an individual who defaults, and “-” indicates one who does not.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./Chapter3.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./analysis.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exercise</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>