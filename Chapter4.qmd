---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
suppressPackageStartupMessages({
library(ISLR)
library(tidyverse)
library(ggthemes)
library(sjPlot)
library(corrplot)
library(tidymodels)
library(magrittr)
library(dotwhisker)
library(hrbrthemes)
library(patchwork)
library(GGally)
library(showtext)
})
extrafont::loadfonts(quiet = TRUE)
theme_set(theme_ipsum_es(axis_title_size = 11 , axis_title_just = "c") + theme(axis.line = element_line(color ="black")))
```

# Classification

For many cases the response variable is *qualitative*, or *categorical*.

Clasification is the process for predicting qualitative responses; we are classifying an observation.

There are three main classifiers,*classification techniques*, mainly:

-   *Logistic regression*
-   *Linear discriminant analysis*
-   *K-nearest neighbors*

We discuss more computer intensive methods is later chapters such as GAM(ch 7), trees, random forests, and boosting(ch 8), and support vector machines (ch 9).

## An overview of Classsification

Here some clasffication problems

1.  A person arrives at the emergency room with a set of symptoms that could possible be attributed to one f three medical conditions. Which of the three conditions does the individual have?

2.  An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP adress, past transaction history and so forth.

3.  In the bais of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are disease causing and which are not.

Just like in LR , in the classification setting we have a set of training observations $(x_1,y_1), \dots, (x_n,y_n)$ what we can use to build a classfier. We want our classifier to perform not only on th training data, but also on test observations that are not used to train the classier.

> We are going to use `Default` data set.

```{r}
default = read_csv("./data/Default.csv")
default %<>% 
  mutate_if(is.character, ~as.factor(.)) %>% 
  print()
```

We are interested in predicting whether an individual will default on his or her credit card balance.

```{r}
skimr::skim(default)
```

```{r}
GGally::ggpairs(mapping = aes(color = default), data = default)
```

```{r}
default %>% 
  ggplot() + aes(x = balance, y = income, color = default, shape = default) + geom_point() + scale_color_manual(values = c("#6CA2C9","#BD5E2A")) + scale_shape_manual(values = c(1,3)) -> p1

default %>% 
  ggplot() + aes(x = default, y = balance, fill = default) + geom_boxplot() + 
  scale_fill_manual(values = c("#6CA2C9","#BD5E2A")) -> p2

default %>% 
  ggplot() + aes(x = default, y = income, fill = default) + geom_boxplot() + 
  scale_fill_manual(values = c("#6CA2C9","#BD5E2A")) -> p3

# gridExtra::grid.arrange(p1,p2,p3, nrow=1)
gridExtra::grid.arrange(p1,p2,p3, nrow=3)
# Top: The aanual incomes and montly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: boxplots of balances as a function of default status. Bottom: boxplots of income as a functino of default status.

```

people who default tend to have high credit card balances compared to not defaulted.

```{r}
default %>% 
  count(default) %>% 
  mutate(port = n/sum(n))
# default rate is 3%
```

```{r}
default %>% 
  group_by(student,default) %>% 
  summarise(count = n()) %>% 
  mutate(port = count/sum(count))

# student are 2 times more likely to default
```

In this chapter we laern how to build a model to predict `default`($y$), for any given value of balance ($x_1$), and income ($x_2$). Since $Y$ is not quantitative, SLR is not appropriate.

## Why Not Linear Regression?

Why is LR not appropriate here?

Suppose that we are trying to predict the mdeical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, therea rea three possible diagnoses: `stroke`, `drug overdose`, `epileptic seizure`. We could consider encoding these values as a quantitative respose variable $Y$:

$$
Y = 
\begin{cases}
1 & \text{if stroke}; \\
2 & \text{if drug overdose}; \\
3 & \text{if epileptic seizure}.
\end{cases}
$$

We can now predict $Y$ using $x_1, \dots, \x_p$. However, this coding implies an ordering on the outcomes, putting `drug overdose` in between `stroke` and `epileptic seizure` and insisting that the difference between `stroke` and `drug overdose` is the same as the difference between `drug overdose` and `epileptic seizure`. In practice there is no particular reason that this needs to be the case. We could have ordered the cases differently, i.e. stroke to the 3 etc. Which implies a totally different relationship among the three conditions. All these combinations would produce different linear models that would lead to different set of predictions on test observations.

However, if the response variable's values take on a natural ordering, such as *mild, moderate, and severe*, and we felt the gap between mild and moderate was similar to the gap between moderate and sever, then a 1,2,3 coding would be reasonable. \*\*Unfortunaltely, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantittative response that is ready for linear regression\*.

For a *binary(two level)* qualitative response, the situation is easier. For instance consider only two possiblities for $Y$:

$$
 Y = 
 \begin{cases}
 0 & \text{if stroke} \\
 1 & \text{if drug overdose}
 \end{cases}
 $$

we can create a dummy varaible and fit a linar regression to this binary response and predict `drug overdose` if $\hat{y}>0.5$ and `stroke` otherwise. However, if we use a linear regression our estimates might be outside of $[0,1]$, aking them hard to interpret as probabilities.

The dummy varaible approach coonot be easliy extended to accommodate qualitative responses with more than two levels. We prefer classification methods:

\## Logistic Regression

Our `default` variable can have two values: `Yes` or `No`. Rather than modeling this response $Y$ directly, logistic regression models the *probability* that $Y$ belongs to a particular category.

For the `default` data logistic regression models the probability of default. For example, the probability of default given balance can be written as

$$
Pr(default = Yes | balance)
$$ The values of $Pr(default = Yes|balance)$, which we abbreviate $p(balance)$, will range between 0 and 1. Then for any given value of `balance`, a prediction can be made for `default`. For example, we can predict $default = Yes$ or any individaul for whom $p(balance)>0.5$. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as $p(balance)>0.1$.

### The Logistic Model

How should we model the relationship between $p(x) = Pr(y = 1| x)$ and $x$? (when we coded 0/1 for response values)

In section 4.2 we talked of using a linear regression model to represent these probabilities:

$$
p(x) = \beta_0 + \beta_1 x
$$ (4.1)

![](fig4.2.png)

```{r}
default %>% 
  mutate(default = ifelse(default == "Yes",1,0)) %>% 
  print() %>% 
  lm(default~balance,.) %>% 
  summary()
```

```{r}
default %>% 
  mutate(default = ifelse(default == "Yes",1,0)) %>% 
  lm(default~balance,.) -> lm_res

default %>% 
  mutate(default = ifelse(default == "Yes",1,0)) %>% 
  ggplot() + aes(x = balance, y = default) + geom_point(shape = 4, color = "brown4") + 
  geom_abline(intercept = lm_res$coefficients[1], slope = lm_res$coefficients[2], color = "blue") + scale_y_continuous(expand = c(0,0.25), breaks = seq(-0.2,1,by=0.2) )
```

The plot above is the same as plot on the left pane of 4.2. Here we see the problem with this approach, for balances close to zero, we predict a negative probability of default and if we predict for very large balances we would get values bigger than 1. These predictions are not sensible. This always happen for any time a straight line is fit to a binary response that is coded as 0 or 1, in pricpile we can always predict $p(x)<0$ for some values of $x$ and $p(x)>1$ for others.

To avoid this problem we must model $p(x)$ using a function that gives outputs between 0 and 1 for all values of $x$. Many functions meet this description, however in logistis regression, we use *logistic function*

$$
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}
$$ (4.2)

To fit the mode (4.2) we use a method called *maximum likelihood*, which we discuss in the next section. The right-hand panel of Figure 4.2 sows then fit of the logistic regression model to the `default`data.

Notice that for low balances we now predict the probability of default as close to , but never below zero. Likewise for high balances we predict a default probability close to, but never above, one.

The logistic function will always produce an *S-shaped* curve of this form, and so regardless of the value of $X$ we will obtain a sensible prediction.

After a bit of manipulation of (4.2) we find that

\$\$ \begin{align}
p(x) &= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1-p(x) &= 1- \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}} \\

1- p(x) &= \frac{1}{1 + e^{\beta_0 + \beta_1 x}}  \\

1- p(x) \cdot e^{\beta_0 + \beta_1x} &= \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}\\

1-p(x) \cdot e^{\beta_0 + \beta_1 x} &= p(x) \\

\frac{p(x)}{1-p(x)} &= e^{\beta_0 + \beta_1} \space \space \space \space \space \space \space \space \space \space (4.3)

\end{align} \$\$

the quantity $p(x)/(1 - p(x))$ is called the *odds*, and can take on any value between 0 and $\infty$. Values of the odds close to 0 and $\infty$ indicate very low and very high probabilities of default, respectively.

For example, on average 1 in 4 people with an odds of 1/4 will default since $p(x) = 0.2$ implies an ods of $\frac{0.2}{1-0.2} = 1/4$. Likewise on average 9/10 wpeople with an odds of 9 will default since $p(x) = 0.9$ implies an odds of $\frac{0.9}{1-0.9} = 9$.

By taking the lograithm of both sides of (4.3) we arrive at

$$
\log(\frac{p(x)}{1-p(x)}) = \beta_0 + \beta_1 x
$$ (4.4)

the left-hand side is called the *log-odds-* or *logit*. We see that the logistic regression model (4.2) has a logit that is linear in $X$.

Here increasing $x$ by one unit changes the log odds by $\beta_1$ (4.4), or it multiplies the odds by $e^{\beta_1}$. But because the relationship between $p(x)$ and $x$ in (4.2) is not a straight line, $\beta_1$ does not correspond to the change in $p(x)$ associated with a one-unit increase in $x$. The amount that $p(x)$ changes due to a one-unit change in $x$ will depend on the current value of $x$. But regardless of the value of $x$, if $\beta_1$ is positive, then increasing $x$ will be associated with increasing $p(x)$, vice versa. We can see that in the right hand panel of Figure 4.2.

### Estiamting the Regression Coefficients

$$
p(x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1}}
$$ (4.2) The coefficients of $\beta_0$ and $\beta_1$ are unknown, must be estimated based on the available training data. We are going to use *maximum likelyhood* method. The basic intuition begind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for $\beta_0$ and $\beta_1$ such that the predicted probability $\hat{p}(x_i)$} of default for each individual, using (4.2), corresponds as closely as possible to the individual's observed default status. In other words, we try to find $\hat{\beta_0}$ and $\hat{\beta_1}$ such that plugging these estimates into the model for $p(x)$,given in (4.2), yields a number close to 1 for all individuals who defaulted, and a number close to zero for all individuals who did not. We can formalize this mathematical equation with *likelihood function*

$$
l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i) \prod_{i':y_{i'} = 0}(1-p(x_{i'}))
$$ (4.5)

The estimates $\hat{\beta_0$ and $\hat{\beta_1}$ are chosen to *maximize* this likelihood function.

```{r}
# initialize model
logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification") -> log_model
log_model
```

```{r}
# setup the recipe 
default_recipe <- recipe(default ~ balance, data = default)
default_recipe
```

```{r}
# set up the workflow
workflow() %>% 
  add_model(log_model) %>% 
  add_recipe(default_recipe) -> default_workflow
default_workflow
```

```{r}
## fit model
default_workflow %>% 
  fit(data = default) %>%
  tidy(conf.int = T)

# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.
```

```{r}
default_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  ggplot() + aes(x = balance, y = default, color = .pred_class) + geom_point() + scale_color_ipsum()
```

Lets have a look at the Logistic Regression results again:

```{r}
## fit model
default_workflow %>% 
  fit(data = default) %>%
  tidy(conf.int = T)

# for the default data, estiamted coefficients of the logistic regression model that predicts the probability of default using balance. A one unit increase in balance is assocaited witnh an increase in the log odds of default by 0.0055 units.
```

We can measure the accuracy of the coefficient estimates by computing their standard errors. The *z-*statistic in the above plays the same role as *t* statistic in linear regression output. They are calculated from $\hat{\beta_i} / \text{SE}(\hat{\beta_i})$, and so a large(absolute) value of the z-statistic indicates evidence against the null hypothesis $H_0: \beta_1 = 0$. This null hypothesis implies that $p(x) = \frac{e^{\beta_0}}{1 + e^{\beta_0}}$. In other words, that the probability of default does not depend on blaance. p value is very low, we can reject $H_0$; there is relationship between balance and probability of default. Intercept is not important here.

### Making predictions

Once the coefficients have been estimated, we can compute the probability of default for any given credit card balance.

$$
\hat{p}(x) = \frac{e^{\hat{\beta_0} + \hat{\beta_1}x}}{1 +e^{\hat{\beta_0} + \hat{\beta_1}x}} = \frac{e^{-10.6513 + 0.0055 x}}{1 + e^{-10.6513 + 0.0055 x}}
$$

so for example given income \$1,000 the predicted possiblity of default is

```{r}
exp(-10.6513 + 0.0055 * 1000) / (1 + exp(-10.6513 + 0.0055 * 1000))
```

which is below 1%. What about the probability of default for a person with \$2,000 balance

```{r}
exp(-10.6513 + 0.0055 * 2000) / (1 + exp(-10.6513 + 0.0055 * 2000))
```

much higher 58%.

We can get all the predictions for our training data set form

```{r}
default_workflow %>% 
  fit(data=default) %>% 
  augment(new_data = default) %>% print() %>% 
  count(.pred_class)
# we classified 142 cases to default 

# originally 
default %>% 
  count(default)

# 333 cases are actually default
```

We can implement qualitative predictors to the logistic regression using the dummy variable approach.. Lets predict default by only `student` variable

$$
x = 
\begin{cases}
1 & \text{if student} \\
0 & \text{if not student}
\end{cases}
$$

```{r}
logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification") -> log_model

default_recipe <- recipe(default ~ student, data = default)
workflow() %>% 
  add_model(log_model) %>% 
  add_recipe(default_recipe) -> default_workflow
default_workflow %>% 
  fit(data = default) %>% 
  tidy(conf.int = T)
```

The coefficient is positive and the p value is statistically significant. This indicates that students tend to have higher default probabilities than non student:

$$
\widehat{Pr}(default = Yes | student = Yes) = \frac{e^{-3.50 + 0.4049 \times 1}}{1 + e^{-3.50 + 0.4049 \times 1}} = 0.0431
$$

$$
\widehat{Pr}(default = Yes | student = No) = \frac{e^{-3.50 + 0.4049 \times 0}}{1 + e^{-3.50 + 0.4049 \times 0}} = 0.0292
$$

### Multiple Logistic Regression

We now consider the problem of predicting a binary response using a multiple predictors. We can generalize (4.4) as follows:

$$
\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1x_1 + \dots + \beta_p x_p
$$ (4.6)

(4.6) can be written as

$$
p(x) = \frac{e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1x_1 + \dots + \beta_p x_p}}
$$ (4.7)

We again use maximum likelihood method to estimate $\beta_0, \beta_1, \dots, \beta_p$.

Lets use all of our variables in our model

```{r}
logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification") -> log_model

recipe <- recipe(default ~ balance + income + student, data = default)

workflow() %>% 
  add_model(log_model) %>% 
  add_recipe(recipe) -> default_workflow

default_workflow %>% 
  fit(data = default) %>% 
  tidy() 
```

| term        |    estimate | std.error |  statistic |   p.value |
|:------------|------------:|----------:|-----------:|----------:|
| (Intercept) | -10.8690452 | 0.4922555 | -22.080088 | 0.0000000 |
| balance     |   0.0057365 | 0.0002319 |  24.737563 | 0.0000000 |
| income      |   0.0000030 | 0.0000082 |   0.369815 | 0.7115203 |
| studentYes  |  -0.6467758 | 0.2362525 |  -2.737646 | 0.0061881 |

Here there is a suprising result. According to the p-values balance and student variables are associted with the probability of default. However, coefficient for stundet dummy is negative; indicating that students are less likely to default than non-students. But this coefficint was positive in our previous analysis when we regressed probability of default by student: results showed that probability of default is twice as likely for students compared to non students.

![](fig4.3.png)

Check out the fig 4.3. The orange and blue lines show the average default rates for students and non students, respectively, as a function of credit card balance. The nagive coefficient for student in the multiple logistic regression indicates that **for a fixed value of balance and income** a student is less likely to default than a non-student. We observe from the left hand panel that the student default ratge is at or below that of the nun student default rate for every value of balance. But the horizontal broken lines near the base of the plot, which show the default rates for students and non students averaged over all values of balance and income, suggest the opposite effect: the overall student default rate is higher than non student default rate. Consequently there is a positive coefficient for student in the single variable logistic regression output.

The right hand panel provies an explanation for this discrepancy: the varaibles student and blaance are correlated. Students tend to hold higher levels of debt, which is in turn assocaited with higer probability of default. In other words, students are more likely to have large credit card balances, which as we know from the left hand panel of figure 4.3 tend to be assocaited with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall students tend to default at a higher rate than non students. This is an important distinction. A student is riskier than a non student if no information about the student's credit card balance is available. However, that student is less risky than a non student with the same credit card balance.

This simple example illustrates the dangers and subtleties assocaited with performing regressions involving only a single predictor when other predictors may als obe relevant. The results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon seen in Figure 4.3 is known as *confounding*.

Lets put the estimates to our estimated probability function

\$\$ \hat{p}(x) = \frac{
e^{-10.86 + 0.00574 \cdot balance + 3e-6 \cdot income - 0.646 Student}
}

{ 1 + e\^{-10.86 + 0.00574 \cdot balance + 3e-6 \cdot income - 0.646 Student} } \$\$ We can make predictions: a student with a credit card balnce of \$1,500 and an income of \$40,000 has an estimated probability of default:

$$
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 1}
} = 0.05780859
$$

A non student with the same balance and income has an estiamted probability of default of

$$
\hat{p}(x)=\frac{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
}
{
e^{-10.86 + 0.00574 \cdot 1,500 + 3e-6 \cdot 40,000 - 0.646 \cdot 0}
} = 0.1048655
$$

```{r}
default_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = tibble(income = 40000, balance = 1500, student = c("Yes","No")))
```

### Logistic Regression for \> 2 Response Classes

What if our response variable has more than two classes. Like medical condition in the emergency room: `stroke`, `drug overdose`, `epileptic seizure`. In this setting we wish to model both $Pr(Y = stroke | x)$ and $Pr(Y = drug \space overdose | x)$, with the remaining $Pr(Y = epileptic \space seizure |x) = 1 - Pr(Y= stroke | x) - Pr(Y = drug \space overdose | x)$. We can do it my extending logistic regression but *Linear Discriminant Analysis* is much suitable for this task

## Linear Discriminant Analysis

Logistic regression involves directly modeling $Pr(Y = k | X = x)$ using the logistic function, given by (4.7) for the case of two response classes. In statistical jargon, we model the conditional distribution of the response $Y$, given the predictor(s) $X$.

We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors $X$ separately in each of the response classes (i.e. given $Y$), and then use Bayes' theorem to flip these around into estimates for $Pr(Y = k | X = x)$. When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression.

Why do we need another method when we have logistic regression?

-   When the classes are well separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discrimnant analysis does not suffer from this problem.

-   If *n* is small and the distribution of the predictors $X$ is approximately normal in each of the classes the linear discriminant model is again more stable than th elogistic regression model

-   LDA is popular when we have more than two response classes.

### Using Bayes' Theorem for Classification

We want to classify an observation into one of $K$ classes, where $2 \leq K$. So $Y$ can take on $K$ possible distinct and unordered values. Let $\pi_k$ represent the overall or *prior* probability that a randomly chosen observation comes from the $k$th class.; this is the probability that a given observation is associated with the *k*th category of the response variable $Y$. Let $f_k(x) \equiv Pr(X = x | Y = k)$ denote the *density* function of $X$ for an observation that comes from the *k* th class. In other words $f_k(x)$ is relatively large if there is a high probability that an observation in the *k* th class has $X \approx x$, and $f_k(x)$ is small if it is very unlikely that an observation in the *k*th class has $X \approx x$. Then *Bayes's theorem* states that

$$
Pr(Y = k | X = x) = \frac{
\pi_k\,f_k(x)
}
{
\sum^K_{l = 1}\,\pi_l\,f_l(x)
}
$$ (4.10)

We will use the abbreviation $p_k(X) = Pr(Y = k | X)$. This suggests that instead of directly computing $p_k(X)$ as Section 4.3.1, we can simply plug in estimates of $\pi_k$ and $f_k(X)$ into (4.10). In general, estimating $\pi_k$ is easy if we have a random sample of $Y$s from the population: we simply compute the fraction of the training observations that belong to the *k*th class. However, etimating $f_k(X)$ tends to be more challenging, unless we assume some simple forms for these densities. We refer to $p_k(x)$ as the *posterior* probability than an observation $X=x$ belongs to the *k*th class. That is, it is the probability that the observation belongs to the *k*th class, *given* the predictor value for that observation.

We know from Ch.2 that the Bayes classifier, which classifies an observation to the class for which $p_k(X)$ is largest, has the lowest possible error rate out of all classifiers. Therefore, if we can find a way to estimate $f_k(X)$ then we can develop a classifier that approximates the bayes classifier:

### Linear Discriminant Analysis for p=1

Assume we have one predictor. We would like to obtain an estimate for $f_k(x)$ that we can plug into (4.10) in order to estimate $p_k(x)$. We will then classify an observation to the class for which $p_k(x)$ is greatests. In order to estimate $f_k(x)$ we will first make some assumptions about this form.

Suppose we assume $f_k(x)$ is *normal* or *Gaussian*. In the one dimensional setting, the normal density takes the form

$$
f_k(x) = \frac{
1
}
{
\sqrt{2\pi}\,\sigma_k
}\,exp\,\left(-\frac{1}{2\sigma^2_k}(x - \mu_k)^2\right)
$$ (4.11)

where $\mu_k$ and $\sigma^2_k$ are the mean and variance parameters for the *k*th class. For now, let us further assume that $\sigma_1^2=\dots=\sigma^2_K$: that is, there is shared variance term across all $K$ classes, which for simplicty we can denote by $\sigma^2$. Plugging (4.11) into (4.10) we find that

$$
p_k(x) = \frac{
\pi_k\,\frac{1}{\sqrt{2\pi}\,\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_k)^2)
}
{
\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma}\,exp(-\frac{1}{2\sigma^2}(x - \mu_l)^2)
}
$$ (4.12)

The bayes classifier involves assigning an observation $X=x$ to the class for which (4.12) is largest. Taking the log of (4.12) and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which

$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} \,- \frac{\mu_k^2}{2\sigma^2}\,+ \log(\pi_k)
$$ 
(4.13)

is largest. For instance if $K = 2$ and $\pi_1 = \pi_2$, then the Bayes clasffier assigns an obsetvation to classs 1 if $2x(\mu_1 - \mu_2) > \mu_1^2 - \mu_2^2$, ann to class 2 otherwise. In this case, the Bayes desicion boundary corresponds to the point where

$$ 
\begin{align}
\delta_k - delta_j &= 0 \\
x \cdot \frac{\mu_k}{\sigma} - \frac{\mu^2_k}{2\sigma^2} + \log(\pi_k) &- x \cdot \frac{\mu_j}{\sigma}  \frac{\mu_j^2}{2\sigma^2} + \log(\pi_j) = 0 \\
x(\frac{\mu_k - \mu_j}{\sigma}) &+ \frac{\mu_j^2 - \mu_k^2}{2\sigma^2} + \log(\frac{\pi_k}{\pi_j}) = 0 \\
x &= \frac{\frac{\mu^2_k -\mu^2_j}{2\sigma^2} - log(\frac{\pi_k}{\pi_j})}{\frac{\mu_k - \mu_j}{\sigma}}
\end{align}

$$ 
(4.14)
for this spesific case:
$$
x = \frac{\mu_1^2 - \mu_2^2}{2(\mu_1 - \mu_2)}= \frac{\mu_1 + \mu_2}{2}
$$

An example is shown in the left hand panel of fig 4.4

![](fig4.4.png)

The two normal density functions that are displayed, $f_1(x)$ and $f_2(x)$, represent two distinct classes. The mean and the variance parameters for the two desity functions are $\mu_1 = -1.25, \space \mu_2 = 1.25$, and $\sigma_1^2 = \sigma_2^2 = 1$. The two densities overlap, and so given that $X=x$, there is some uncertainty about the class to which observation belongs. If we assume that an observation is equally likely to come from either class--that is $\pi_1 = \pi_2 = 0.5$-- then by inspection of (4.14), we see that the Bayes classfier assigns the observation to class 1 if $x<0$ and class 2 otherwise. Note that in this case, we can compute the Bayes classfier because we know that $X$ is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation we are not able to calcualte Bayes classifier.

In practice, even if we are quite certain of our assumption that $X$ is drawn from a Gaussian distribution within each class, we still have to estimate the parameters $\mu_1, \dots, \mu_K,\space \pi_1,\dots, \pi_K,$ and $\sigma^2$. The *linear discriminant analysis* (LDA) method approximates the bayes classfier vy plugging estimates fro $\pi_k, \mu_k$ and $\sigma^2$ into (4.13). In particular the following estimates are used.:

$$ \begin{align}
\hat{\mu}_k &= \frac{1}{n_k}\sum_{i:y_i=k} x_i \\

\hat{\sigma}^2 &= \frac{1}{n-K}\sum^K_{k=1} \sum_{i:y_i =k}(x_i - \hat{\mu}_k)^2

\end{align} 
$$ 
(4.15) 
**Important**

where *n* is the total number of training observations, and *n_k* is the number of training observations in the *k*th class. The estimate for $\mu_k$ is simply the average of all the training observations from the *k*th class, **while** $\hat{\sigma}^2$ **can be seen as a weighted average of the sample variances for each of the** $K$ **classes**. Sometimes we have knowledge of the class membership probabilities $\pi_1, \dots, \pi_K$, which can be used directly. In the absence of any additional information, LDA estimates $\pi_K$ using the proportion of the training observations that belong to the *k*th class. In other words,

$$
\pi_k = n_k /n
$$ The LDA classifer plugs the estimates given in (4.15) and (4.16) into (4.13), and assigns an observation $X=x$ to the class for which

$$
\hat{\delta}(x) = x \cdot \,\frac{\hat{u}_k}{\hat{\sigma}^2} \,-\,\frac{\hat{u}^2_k}{2\,\hat{\sigma}^2}\,+\,\log(\hat{\pi}_k)
$$ 
(4.17)

is largest. The word *linear* in classfier's name stems from the fact that *discriminant functions* $\hat{\delta}_k(x)$ in (4.17) are linear functions of $x$.

The right hand panel of Fig 4.4 displays a histogram of a random sample $n=20$ observations from each class. To implement LDA, we behan by estimating $\pi_k, \mu_k, \sigma^2$ using (4.15) and (4.16). We then computed the desicion boundy, shown as a black solid line, that results from assigning an observation to the clas for which (4.17) is largest.

```{r}
set.seed(12)
a = rnorm(n=20, mean = -1.25, sd =1)
b = rnorm(n=20, mean=1.25, sd =1)
d = rbind(tibble(x = a, class = 1),tibble(x = b, class =2))

d %>% ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F) 

d %>% 
  mutate(d_x_1 = x * -1.25/1 - (-1.25^2/2) + log(0.5),
         d_x_2 = x * 1.25/1 - (1.25^2/2) + log(0.5),
         .pred.class = ifelse(abs(d_x_2) > abs(d_x_1) ,1,2)) %>% 
  ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F)  
```



```{r}
BDB = ((((-1.25)^2 - 1.25^2)/2) - log(0.5/0.5)) / ((-1.25 - 1.25)/1) # = 0
```

```{r}
d %>% 
  mutate(d_x_1 = x * -1.25/1 - (-1.25^2/2) + log(0.5),
         d_x_2 = x * 1.25/1 - (1.25^2/2) + log(0.5),
         .pred.class = ifelse(abs(d_x_2) > abs(d_x_1) ,1,2)) %>% 
  ggplot() + aes(x = x, color = class, group = class) + geom_density(show.legend = F)  + geom_vline(xintercept = 0, linetype = "dashed")
```

Here in this case bayess classfier assigns the observation to class 1 if For any $x <0$ and 2 if $x>0$. we can compute the Bayess classfier because we know that $X$ is drawn from a gaussian distribution within each class, and we know all of the parameters involved. In real-life we cannot calculate bayes classfier


```{r}
d %>% 
  group_by(class) %>% 
  summarise(mean_hat = mean(x), var_hat = sd(x)^2) 
```

```{r}
d %>% 
  group_by(class) %>% 
  summarise(count = n()) %>% 
  mutate(pi_hat = count/sum(count))
```

```{r}
d %>% 
  mutate(d_k_c1 = x * (-1.58/0.751) - (-1.58^2/(2*0.751)) + log(0.5),
         d_k_c2 = x * (1.35 / 0.781) - (1.35^2/(2*0.781)) + log(0.5)) %>% 
  mutate(pred.class = factor(ifelse(d_k_c1 > d_k_c2,1,2)), class = factor(class)) %>% 
  conf_mat(truth = class, estimate = pred.class) %>% 
  autoplot(type = "heatmap")
```

```{r}
library(discrim)
d %>% mutate(class = factor(class)) -> d

discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS") -> lda_spec

lda_recipe <- recipe(class~x,data =d)

workflow() %>% 
  add_model(lda_spec) %>% 
  add_recipe(lda_recipe) -> lda_workflow

lda_workflow %>% 
  fit(data = d) %>% 
  augment(new_data=d) 
```

To sum up; the LDA classfier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance $\sigma^$, and plugging estimates for these parameters into the Bayes classfier. In section 4.4.4, we will consider a less stringent set of assumptions, by allowing the observations in the *k*th class to have a class specific variance, $\sigma^2_k$


### Linear Discriminant Analysis for p > 1

We are going to extend the LDA classfier to the case of multiple predictors. To do this, we will assume that $X = (x_1, x_2, \dots, x_p)$ is drawn from a *multip variate Gaussian* (or multivariate normal) distribution, with a class-specific meean vector and a common covariance matrix. 

The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional distribution, as in (4.11), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with $p=2$ are shown in Figure 4.5.

![](fig4.5.png)

The height of the surface at any particular point represnts the probability that bot $X_1$ and $X_2$ fall in a small region on that point. In either panel, if the surface is cut along the $X_1$ axis or along the $X_2$ axis, the resulting cross section will have the shape of a one dimensional normal distribution. The left hand panel illustrates an example in which $var(x_1) = var(x_2)$ and $cor(x_1,x_2) = 0$; this surface has a charactersitic *bell shpae*. However, the bell shape will be distorted if the preditors are correlated or have unequal variances, as in illusterated in the right hand panel. In this case, the base of the bell will have an elliptical, rather than circular shape.

To indicate that a *p-dimnensional* random varaible $X$ has a multipvariate Gaussian distribution we write $X \sim N(\mu,\sum)$. Here $E(X) = \mu$ is the mean of $X$ ( a vector with *p* components), and $Cov(X) = \sum$ is the $ \times p$ covariance matrix of $X$. Formally, the multivariate Gaussian density is defined as 

$$
f(x) = \frac{1}{(2\pi)^{p/2}|\sum|^{1/2}} \exp\left(-\frac{1}{2}(x -\mu)^T \textstyle\sum^{-1}(x -\mu) \right)
$$
(4.18)

In this case of $p>1$ predictors, the LDA classfier assumes that the observations in the *k*th class are drawn from a multivariate Gaussian distribution $N(\mu_k,\textstyle\sum)$, where $\mu_k$ is class-specific mean vector, adn $\textstyle\sum$ is a covariance matrix that is common to all $K$ classes. Plugging the density function for the *k*th class, $f_k(X=x)$ into (4.10) and performing a little bit of algebra reveals that the Bayes classfier assigns an observation $X=x$ to the class for which

$$
\delta_k(x) = x^T \textstyle\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\textstyle\sum^{-1}\mu_k + \log \pi_k
$$
(4.19)

is largest. This is the vector/matrix version of 4.13.

An example is shown in the left-hand panel of Figure 4.6

![](fig4.6.png)

Three equally sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represetn regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes desicion boundaries. In other words, they represent the set of values $x$ for which $\delta_k(x) = \delta_l(x)$; i.e.

$$
x^T\textstyle\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\textstyle\sum^{-1} = x^T\textstyle\sum^{-1}\mu_l - \frac{1}{2}\mu_l^T\textstyle\sum^{-1}\mu_l
$$
(4.20)

for $k\neq1$. The $\log \pi_k$ term from 4.19 has dissapeared because each of the three classes has the same number ob observations; i.e. $\pi_k$ is the same for each class. No that there are three lines representing the Bayes desciion boundaries because there are three *pairs of clases* among the three classes. That is, on desicion boundary separets class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classfier will classify an observation according to the region in which it is located.

Once again, we need to estimate the unknown parametrs $\mu_1, \dots, \mu_K, \pi_1, \dots, \pi_K$ and $\sum$; the formulas are similar to those used in the one dimensiona lcase, given in 4.15. To assign a new observation $X=x$, LDA plugs these estiamtes into 4.19 and classfiers to the class for which $\hat{\delta}(x)$ is largest. Note that in 3.19 $\delta_k(x)$ is a linear function of $x$; that is the LDA desicion rule depend on $x$ only through a linear combination of its elements. Once again, this is the reason for the word *linear* in LDA.


In the right hand panel of Fig 4.6, 20 observations drawn from each of the three classes are displayed, and the resulting LDA desicion boundaries are shown as solid black lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test erro rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively. This indicates that LDA is performing well on this data.

We can perfrom LDA on the `default` data in order to predict whether or not an individaul will default on the bassi of credit card balance and student status. The LDA model fit to the 10,000 training samples results in a *training* error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.

-   First of all training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classfier to perform worse if we use it to predict whether or not. anew set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parametsr *p* to number of samples *n*, the more we expect this *overfitting* to play a role. For these data, we don't expect tihs to be a problem, since $p=2$ and $n = 10,000$.

-   Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individaul will not default, regardless of his or her credit car balance and student statsu, will result in an error rate of 3.33%. In other words, the trivial *null* classifier will achive an error rate thatis only a bit higher than the LDA training set error rate.

In pracitice, a binay classifiar such as this one can make two types of errors: it can incorretly assign an individual who default to the *no default* category, or it can incorretly assign an individual who does not defaul to *default* category. It is often of interst to determine which of these two types of errors are being made. A *confusion matrix* show for the `default` data in Table 4.4 is a convinient way to dispaly this information.

```{r}
discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS") -> lda_spec

lda_recipe <- recipe(default ~ balance + student, data = default)

workflow() %>% 
  add_model(lda_spec) %>% 
  add_recipe(lda_recipe) -> lda_workflow

lda_workflow %>% 
  fit(data = default)
```

> one thing we can look in LDA outpit is the group means. We see that defaulted people have 2 times balance than not defaulted. % of students are higher for defaulted people.

```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data =default)
```

Lets have a confusion matrix

```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  conf_mat(truth = default, estimate = .pred_class) %>% print()

lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>%
  sens(truth = default, estimate = .pred_class, estimator = "binary") %>% print() # overall accuracy; specificity; the goal of LDA is to max this

lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>%
  sens(truth = default, estimate = .pred_class,event_level = "second") # accuracy of Yes predictions; 1-0.243 = 0.757 -> sensitivity; the percentage o true defaulters that are identified
```

This confusion matrix table shows 104 people would default. Of those people, 81 is actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorretly labeled. This looks like a pretty low error rate! However, of the 333 individuals who default, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who default is very high. From the perspective of a credit card company that is tryibng to identify high-risk individuals, an error rate of 75.7% among individauls who default may be well unaccapteble.

Class-specific performance is also important in medicine and biology where the terms *sensivity* and *specificty* characterize the performance of a classifer or screening test. In this case the sensivity is the percentage of true defaulters that are identified, a low 34.3% in this case. The specifity is the percentage of nond-defaulters that are correctly identified, here ($1-23/9,667 = 99.8%$)

```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  accuracy(truth = default, estimate = .pred_class)
```

Why does LDA do such a poor job of classfiying the customers who default? In other words, why does it have such a low sensivity? AS we have seen, LDA is trying to approximate the Bayes classfier, which has the lowest *total* error rate out of all classfiers (if the Gaussian model is correct). That is the Bayes classfier will yield the smallest possible total number of misclassifed observaitons, irrespective of which class the errors come from. That is, some misclassfications will result from incorretly assigninga customer who does not default tot he default class, and others will result from incorreclty assigning a customer who defaults tot he non default class. In contrast a credt card company might particualrly wihch to aviod incorrectly classifying an individual who wilkl defualt, where as incorrectly classfiyng an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classfier that better meets the credit card company's needs.


The Bayes classfier works by assignig an observation to the class for which the posterior probability $p_k(X)$ is greatest. In the two-class case, this amounts to assigning an observation to the *default* class if

$$
Pr(default = Yes | X = x) > 0.5
$$

```{r}
lda_workflow %>% 
  fit(data =default) %>% 
  augment(new_data = default) %>% 
  filter(.pred_Yes > 0.45, .pred_Yes < 0.55)
```

Thus the bayes classfier, andby extension LDA, uses a threshold of 50% for the posterior probabiblity of default in order to assign an observation to the *default* class. However, if we are concerned about incorretly predicting the default status for individauls who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the *default* class. In other words, instead of assigning an observation to the *default* class if 4.21 holds, we could instead assign an observation to this class if 

$$
Pr(default = Yes | X = x) > 0.2
$$
```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  mutate(.pred_class = ifelse(.pred_Yes > 0.2, "Yes", "No"), .pred_class = factor(.pred_class)) %>% 
  conf_mat(truth = default, estimate = .pred_class) %>% print()

```

Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4%. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this imporvement comes at a cost: now 235 individauls who do not default are incorrectly classified. As a result, the overall error rate has increased in the total error rate to be a small price to pay for more accurate identification of individauls who do indeed default.

```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  mutate(.pred_class = ifelse(.pred_Yes > 0.2, "Yes", "No"), .pred_class = factor(.pred_class)) %>% 
  sens(truth = default, estimate = .pred_class, event_level = "second") 
```

![](fig4.7.png)

Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. Using a threshold of 0.5 as in 4.21 minimizes th eoverall error rate, shown as a black solid line. This is to be expected since the Bayes classifier uses a threshold of 0.5 and is known to have the lowest overall error rate. But when a threshold of 0.5 is used the error rate among the individuals who default is quite high (blue dashed line). As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individiuals who do not increasess. How can we decide which threshold value is the best? Such a decision must be based on *domain knowledge* such as detailed information about the costs associated with default.

The *ROC curve* is a popular graphic for simultaneously displaying the two types of errors for all possible threshold. The name "ROC" means *receiver operating characteristics*. 


```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  roc_curve(truth = default,.pred_No) %>% 
  autoplot() + labs(x = "False positive rate : 1 - specifity", y = "True positive rate: sensitivity", title = "ROC curve")
#| fig-cap: A ROC curve for the LDA classifer on the default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctşy identified, using a given threshold value. The flase posiitve rate is 1- speficity: the fraction of non defaulters that we classify incorretly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner i dicating a high true positive rate and a low false positive rate. The dotted line represts the no information classifier: this is what we would expect if student statust and credit card balance are not associated with probability of default.

```

The overall performance of a classifier, summarized over all possible thresholds, is given by the *area under the ROC curve (AUC)* An ideal ROC curve will hug the top left corner so the larger the AUC the better the classifier. For this data the AUC is 0.95.


```{r}
lda_workflow %>% 
  fit(data = default) %>% 
  augment(new_data = default) %>% 
  roc_auc(truth = default,.pred_No)
```

which is close to maximum of 1 so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5. ROC curves are useful for comparing different classfiers since they take into account of all possible thresholds. It turns out that the ROC curve for the logistic regression model of section 4.3.4 fit to these data is virtually indistinguaslibe from this one for the LDA model.

As we have seen above varying the classfier threshold changes its true positive and false positive rate. These are also called the *sensivity* and one minus the *specificity* of our classifer. Since there is an almost bewildering array of terms used in this context, we now give a summary.

![](tab4.6.png)

Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemology literature we think of "+" as "disease" that we are trying to detect, and "-" as the "non disease" state. To make the connection to the classical hypothessi testing literatureü, we think of "-" as the null hypothessi and "+". as the alternative (non-null) hypothessi. In the context of the default data, "+" indicates an individual who defaults, and "-" indicates one who does not.




