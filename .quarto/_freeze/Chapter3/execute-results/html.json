{
  "hash": "fe2d43f8921d8f1ce01415595b9b0117",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Linear Regression\n\nWe will predict quantitative response.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertising = read_csv(\"./data/Advertising.csv\") %>% as_tibble %>% select(-1)\nadvertising\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 × 4\n      TV radio newspaper sales\n   <dbl> <dbl>     <dbl> <dbl>\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n# ℹ 190 more rows\n```\n:::\n:::\n\n\nWe are asked to suggest a marketing plan for next year which will yeild high product sales. We may want to inquire the following questions:\n\n1.  *Is there a relationship between advertising budget and sales?*\n\n    First we should determine if there is an association between adveritisng expenditure and sales. If not, no money shpuld be spent on advertising.\n\n2.  *How strong is the relationship between advertising budget and sales?*\n\n    If there is a relationship between advertising and sales, what is the strength of this relationship? Given a certain advertising budget, can we predict sales with a high level of accuracy? =\\> strong relationsihp.\n\n3.  *Which media contribute to sales?*\n\n    Do all variables--tv,radio,newspaper-- contribute to sales, or just one or the two?\n\n4.  *How accurately can we estimate the effect of each medium on sales?*\n\n    For every ollar spent on advertising in a particular medium, by what amount will sales increase? How accuretly can we predict this amount of increase?\n\n5.  *How accurately can we predict future sales?*\n\n    For any given level of media advertisig, what is our prediction for sales, and what is the accuracy of this prediciton?\n\n6.  *Is the relationship linear?*\n\n    If so linear regresion is appropriate tool, if not we may need to transform the predictor or the repsonse so that liner regression can be used.\n\n7.  *Is there synergy among the advertising media?*\n\n    Does the effect of a medium on sales depend on other medium levels? Does dividing advertisement budget to two or three medium yeild a higher sales?\n\nWe can answer each of these questions using Linear regression.\n\n## Simple Linear Regression\n\nPredicting a quantitative response $Y$ on the basis of a single predictor variable $X$.\n\nOur assumption is that there is approximately a linear relationship between $X$ and $Y$; we can write this linear relationship as\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\epsilon\n$$\n\n$$\nY \\approx \\beta_0 + \\beta_1X\n$$ (3.1)\n\nFor example lets say $X$ is `TV`, and $Y$ is `sales`\n\n$$\nsales = \\beta_0 + \\beta_1 \\times TV + \\epsilon\n$$ or\n\n$$\nsales = \\beta_0 + \\beta_1 \\times TV\n$$\n\nOn (3.1) $\\beta_0$ and $\\beta_1$ are unknown constants that represent the *intercept* and *slope* in the linear model. Together they are known as *coefficients* or *parameters*.\n\nWe are going to use or training data to produce estimates for $\\beta_0$ =\\> $\\hat{\\beta_0}$ and $\\beta_1$ =\\> $\\hat{\\beta_1}$. Using these predicted coefficients we can predict sales;\n\n$$\n\\hat{sales} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times TV\n$$\n\nor as in general form\n\n$$\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\n$$ (3.2)\n\n### Estimating the coefficients\n\nSince, $\\beta_0$ and $\\beta_1$ are unknown, before we can use (3.1) to make predictions we must use data to estimate the coefficients. We have $n$ observations :\n\n$$\n(x_1,y_1), (x_2,y_2), \\dots, (x_n,y_n)\n$$\n\nWe want our estimated coefficients to give such predictions that will fit the avaible data as well =\\> $y_i \\approx \\hat{\\beta_0} + \\hat{\\beta_1}x_i$ for $i = 1,\\dots, n$. This coefficients will allow us to draw a regression line and we want this regression line to be close as possible to the $n$ data points we have.\n\nThere are different ways to measure *closeness*. The most common approach is minimizing the *least squares* criterion. Alternative approaches will be considered in Chapter 6.\n\nOur predictions come from $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i$.\n\nThen for each data we have a *residual*: difference between $y$ and $\\hat{y}$:\n\n$$\ne_i = y_i - \\hat{y_i}\n$$\n\nWe need to take the squares to get the distances--because of the negative residuals, and sum them to get the *residual sum of squares*(RSS)\n\n$$\nRSS = e_1^2 + e_2^2 + \\dots + e_n^2\n$$\n\nthis is equal to\n\n$$\nRSS = (y_i - \\hat{beta_0} - \\hat{\\beta_1}x_1)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2) + \\dots + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1}x_n)\n$$ (3.3)\n\nThe least squares approach chooses $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to minimize RSS. These minimizers are\n\n$$\n\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y_i} - \\hat{\\beta_1}\\bar{x}\n\\end{align}\n$$ (3.4)\n\n$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^ny_i$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^nx_i$ are the sample means.\n\nSo (3.4) defines the *least squares coefficient estimates* for simple linear regression.\n\nLets calculate them with R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_1_hat_adv = sum((advertising$TV - mean(advertising$TV)) * ((advertising$sales - mean(advertising$sales)))) / sum((advertising$TV - mean(advertising$TV))^2)\nbeta_1_hat_adv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04753664\n```\n:::\n:::\n\n\nSo; our \\$\\hat{\\beta_1} = 0.0475 \\$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_0_hat_adv = mean(advertising$sales) - beta_1_hat_adv * mean(advertising$TV)\nbeta_0_hat_adv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.032594\n```\n:::\n:::\n\n\nour $\\hat{\\beta_0} = 7.032$\n\nLets compare them with r function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV, data = advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,\tAdjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nCalculating the predicted values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat_adv = beta_0_hat_adv + beta_1_hat_adv * advertising$TV\ny_hat_adv[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 17.970775  9.147974  7.850224 14.234395 15.627218  7.446162  9.765950\n [8] 12.746498  7.441409 16.530414\n```\n:::\n:::\n\n\nOur $y_i$ values are as above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2102.531\n```\n:::\n:::\n\n\nOur $\\text{RSS} = 2102.531$\n\nSo we can draw our regression line\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertising %>%\n  ggplot() + aes(x=TV, y = sales) +  geom_abline(intercept = beta_0_hat_adv, slope = beta_1_hat_adv, color = \"#262B70\", size =1.2) +\n  geom_segment(aes(xend=TV, yend=y_hat_adv), color = \"#939393\") +\n  geom_point(color = \"#AA1D2E\", size =2) + theme_par()\n```\n\n::: {.cell-output-display}\n![For the advertising data, the least squares fit for the regression of sales onto TV. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, adn the fit make a comprimise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot](Chapter3_files/figure-html/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\nSo we have\n\n$$\n\\hat{y_i} = 7.032 + 0.0475x_i\n$$\n\nAccording to this approximation an additional \\$1,000 spent on TV increases sales by 47.5 units.\n\n### Assessing the Accuracy of the Coefficient Estimates\n\nWe assumed that *true* relationship is linear: $Y = f(X) + \\epsilon$. We don't know $f$, and $\\epsilon$ is a mean-zero random error term.\n\nWe said $f$ is approximatly linear, so that $f(X) = \\beta_0 + \\beta_1 X$; which means\n\n$$\nY = \\beta_0 + \\beta_1 X + \\epsilon\n$$ (3.5)\n\nerror term captures: \\* the true relationship may not be linear \\* other variables that affect Y \\* measurement error\n\nand is independent of $X$.\n\n(3.5) is the *population regression line*: the best linar approximation to the true relationship between $X$ and $Y$.\n\n$$\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}X\n$$ is the *least squares line*. They are different of course! But we don't know the population regression line. If we did:\n\nFor example, lets create a data;\n\n-   First we create random x values from 100 random numbers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq(-2,2,length.out=100)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -2.000000 -1.959596 -1.919192 -1.878788 -1.838384 -1.797980 -1.757576\n [8] -1.717172 -1.676768 -1.636364\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\nx = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\nx[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -0.6666667  0.2222222 -1.0303030 -1.3939394 -0.5454545  0.3838384\n [7] -1.5555556  1.3939394  1.4343434  0.4646465\n```\n:::\n:::\n\n\nlets define our $f$--population parameters $\\beta_0$ and $\\beta_1$\n\n$$\nf(X) = 2 + 3\\times X\n$$ Now lets create our $Y$ values from this function but we also want to add random error values as well\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\ny = 2 + 3*x + rnorm(100)\ny[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -0.5910311  2.6932610 -2.6074622 -3.5444715  1.5421255  2.2173638\n [7] -1.3430610  6.8067360  6.2573073  2.3898188\n```\n:::\n:::\n\n\nSo we have a data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = tibble(\n  y = y, x = x\n)\n```\n:::\n\n\nNow lets plot this data points\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot() + aes(x=x, y=y) + geom_point() + theme_par()\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\nNow, even though we already know $f$ and population parameters $\\beta_0 = 2$ and $\\beta_1 = 3$, lets estimate them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y ~ x, data = data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.88077    0.09180   20.49   <2e-16 ***\nx            3.05893    0.07608   40.20   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,\tAdjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nSo our *least squares estimation is*\n\n$$\n\\hat{y_i} = 1.88 + 3.06 x_i\n$$ Lets draw this *least squares regression line to our plot*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept=1.88, slope = 3.06, size = 1.2) + theme_par()\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWhat about the population regression line\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot() + aes(x=x, y=y) + geom_point() + geom_abline(intercept = 1.9, slope = 3.06, size = 1.2) + geom_abline(intercept = 2, slope = 3, color =\"red\") + theme_par()\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThey are not the same! If we were to have another data from the same data generation process other estimates of parameters would result with different *least squares regression lines*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(111)\nx_r1 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r1 = 2 + 3*x_r1 + rnorm(100)\nset.seed(1111)\nx_r2 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r2 = 2 + 3*x_r2 + rnorm(100)\nset.seed(11111)\nx_r3 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r3 = 2 + 3*x_r3 + rnorm(100)\nset.seed(111111)\nx_r4 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r4 = 2 + 3*x_r4 + rnorm(100)\nset.seed(1111111)\nx_r5 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r5 = 2 + 3*x_r5 + rnorm(100)\nset.seed(11111111)\nx_r6 = sample(seq(-2,2,length.out = 100),size = 100, replace = T)\ny_r6 = 2 + 3*x_r6 + rnorm(100)\n```\n:::\n\n\nLets now estimate population parameters for each of these data and plot them\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_r = tibble(\n  y_r1,x_r1,y_r2,x_r2,y_r3,x_r3,y_r4,x_r4, y_r5,x_r5,y_r6,x_r6\n)\ndata_r\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 12\n     y_r1   x_r1   y_r2    x_r2   y_r3   x_r3  y_r4   x_r4   y_r5    x_r5  y_r6\n    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl> <dbl>\n 1  5.99   1.11   0.807 -0.263   8.82   1.80  -3.05 -1.52   7.64   1.68   6.49 \n 2  6.53   1.35   3.74   0.141   6.23   1.92   5.11  1.27   8.71   2      7.75 \n 3  6.28   1.31   2.16  -0.0202 -0.272 -0.707  2.39 -0.101  4.59   0.909  1.21 \n 4  1.52  -0.141 -0.178 -0.990   5.45   0.788 -4.55 -1.84  -1.41  -1.11   0.478\n 5  0.708 -1.03  -0.464 -0.586   0.361 -0.343 -3.31 -1.15   9.59   1.88   2.64 \n 6  2.05   0.343  4.12   0.788  -1.09  -1.11  -5.13 -1.96   0.723 -0.586  4.10 \n 7  4.54   0.747  5.44   1.60    0.479 -0.707  2.27  0.182  2.29  -0.263  3.68 \n 8  1.69  -0.626  8.02   1.80   -2.23  -1.64   1.55  0.222 -1.45  -0.990  5.08 \n 9  2.84   0.869  1.02   0.384   4.04   0.949  3.34 -0.747 -2.21  -0.828  8.33 \n10 -0.933 -0.990  3.52   0.505  -0.471 -0.505 -1.07 -0.869  1.47   0.0606 2.45 \n# ℹ 90 more rows\n# ℹ 1 more variable: x_r6 <dbl>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot() + aes(x,y) + geom_point(size = 0, color = \"white\")  + \n  geom_abline(intercept = lm(y_r1 ~ x_r1)$coefficients[1], slope = lm(y_r1 ~ x_r1)$coefficients[2], color =\"#29019F\") +\n  geom_abline(intercept = lm(y_r2 ~ x_r2)$coefficients[1], slope = lm(y_r2 ~ x_r2)$coefficients[2], color =\"#0A04BF\") +\n  geom_abline(intercept = lm(y_r3 ~ x_r3)$coefficients[1], slope = lm(y_r3 ~ x_r3)$coefficients[2], color =\"#0930DF\") +\n  geom_abline(intercept = lm(y_r4 ~ x_r4)$coefficients[1], slope = lm(y_r4 ~ x_r4)$coefficients[2], color =\"#0E6DFF\") +\n  geom_abline(intercept = lm(y_r5 ~ x_r5)$coefficients[1], slope = lm(y_r5 ~ x_r5)$coefficients[2], color =\"#2BA8FF\") +\n  geom_abline(intercept = lm(y_r6 ~ x_r6)$coefficients[1], slope = lm(y_r6 ~ x_r6)$coefficients[2], color =\"#48D9FF\") +\n  geom_abline(intercept =2, slope =3, color = \"red\") +\n  \n  theme_par()\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nSo, different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.\n\nThis is because we are using a sample, and estimating characteristics of the population. Usually these characteristics are different, but generally sample characteristics will provide a good estimate to the population characteristics.\n\nComputing $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ from different sets of sample data provide different but similar results. And we are trying to estimate population parameters $\\beta_0$ and $\\beta_1$ with these. Some of these $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ will overestimate, some will underestimate $\\beta_0$, and $\\beta_1$. But if we could average all these estimated parameters and take the average, than this average should be equal to population parameters; if this is the case this estimator is called *unbiased estimator*. So an unbiased estimator does not *systematically* over- or under-estimate the true parameter.\n\nOkay but how close $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ are to the true values $\\beta_0$ and $\\beta_1$. We want to compute the standard errors associated with $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$. Standard error telss us the average amount of estimate differes from the actual value.\n\n$$\n\\begin{align}\n\\text{SE}(\\hat{\\beta_0})^2 &= \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x}^2)}\\right] \\\\\n\\text{SE}(\\hat{\\beta_1})^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\end{align}\n$$ (3.8)\n\nWhere $\\sigma^2=\\text{Var}(\\epsilon)$.\n\nNotice that formula of $\\text{SE}(\\hat{\\beta_1})$ is smaller when $x_i$ are more spread out; intutively we have more *leverage* to estimate a slope when this is the case.\n\nIn general $\\sigma^2$ is not known, but can be estimated from the data. The estimate of $\\sigma$ is known as the *residual standard error*, and given by the formula\n\n$$\n\\text{RSE} = \\sqrt{\\text{RSS}/(n-2)}\n$$ So, when $\\sigma^2$ is estimated fro mthe data we should write $\\hat{\\text{SE}}(\\hat{\\beta_1})$ to indicate that an estimate has been made, but usually we drop this extra hat.\n\nStandard errors can be used to compute *confidence intervals*. A 95% confidence interval is defines as a range of values such that with 95% probability, the rage will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta_1$ approximately takes the form\n\n$$\n\\hat{\\beta_1} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_1}) \n$$ (3.9)\n\nSo there is approximately a 95% chance that the interval $$[\\hat{\\beta_1} - 1.96 \\cdot \\text{SE}(\\hat{\\beta_1}), \\hat{\\beta_1} + 1.96 \\cdot \\text{SE}(\\hat{\\beta-1})]$$ (3.10) will contain the true value of $\\beta_1$. Same is true for $\\beta_0$\n\n$$\n\\hat{\\beta_0} \\pm 1.96 \\cdot \\text{SE}(\\hat{\\beta_0})\n$$ (3.11)\n\nLets calculate the confidence intervals for $\\beta_0$ and $\\beta_1$ from our original data and model\n\n$$\n\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\cdot TV\n$$ We first need to calculate RSS and RSE:\n\n$$\n\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRSS = sum((advertising$sales - y_hat_adv)^2)\nRSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2102.531\n```\n:::\n:::\n\n\n$$\n\\text{RSE} = \\sigma = \\sqrt{RSS/(n-2)}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRSE = sqrt((RSS / (length(advertising$sales) -2)))\nRSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.258656\n```\n:::\n:::\n\n\nFor $\\beta_0$\n\n$$\n\\text{SE}(\\hat{\\beta_0}) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\right]\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_beta_0_adv =  sqrt(RSE^2 * (1/length(advertising$sales) + (mean(advertising$TV)^2 / sum((advertising$TV - mean(advertising$TV))^2))))\nse_beta_0_adv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4578429\n```\n:::\n:::\n\n\nSo we can calculate the confidence interval for $\\beta_0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"In the absence of any advertising, sales will on average, fall somewhere between\",beta_0_hat_adv - 1.96 * se_beta_0_adv, \"and\", beta_0_hat_adv + 1.96 * se_beta_0_adv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn the absence of any advertising, sales will on average, fall somewhere between 6.135221 and 7.929966\n```\n:::\n:::\n\n\nFor $\\hat{\\beta_1}$:\n\n$$\n\\text{SE}(\\hat{\\beta_1})^2 =\\frac{\\sigma^2}{\\sum(x_i - \\bar{x})^2}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_beta_1_adv = sqrt(RSE^2 / (sum((advertising$TV - mean(advertising$TV))^2)))\nse_beta_1_adv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.002690607\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"For each $1,000 increase in TV advertising, average increase in sales will be between\",(beta_1_hat_adv - 1.96 * se_beta_1_adv) * 1000, \"and\", (beta_1_hat_adv + 1.96 * se_beta_1_adv)*1000, \"by 95% confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor each $1,000 increase in TV advertising, average increase in sales will be between 42.26305 and 52.81023 by 95% confidence\n```\n:::\n:::\n\n\nLets confirm our results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,\tAdjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm(sales ~ TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n```\n:::\n:::\n\n\nSo standard errors of our estimated parameters tells us the average amount of difference from the true population parameters. And using the confidence intervals we can tell a range of the true population parameters' interval with a percentage (usually 95%).\n\nLets do this for our `data` as well, which we know has the form\n\n$$\ny_i = 2 + 3 x_i + \\epsilon_i\n$$ Lets calculate $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ first\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_1_hat_data = sum((data$x - mean(data$x)) * (data$y - mean(data$y))) / sum((data$x - mean(data$x))^2)\nbeta_1_hat_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.058925\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_0_hat_data = mean(data$y) - beta_1_hat_data * mean(data$x)\nbeta_0_hat_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.880772\n```\n:::\n:::\n\n\n$$\n\\hat{y_i} = 1.88 + 3.05 x_i\n$$\n\nlets confirm this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y~x, data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n      1.881        3.059  \n```\n:::\n:::\n\n\nLets calculate the residual sum of squares, residual sum of errors, standard errors, and confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  mutate(\n    y_hat = beta_0_hat_data + beta_1_hat_data * x\n  ) -> data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 82.28514\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSE_data = sqrt((RSS_data/(length(data$y) -2)))\nRSE_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9163211\n```\n:::\n:::\n\n\nso $\\sigma_{data} = 0.9163$\n\nWe can now compute the standard errors of estiamed coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_beta_0_data = sqrt(((1/length(data$y)) + (mean(data$x)^2 / sum((data$x - mean(data$x))^2))) * RSE_data^2)\nse_beta_0_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.09179903\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nse_beta_1_data = sqrt(RSE_data^2 / (sum((data$x - mean(data$x))^2)))\nse_beta_1_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07608457\n```\n:::\n:::\n\n\nLets confirm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y~x,data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01398 -0.65163 -0.06344  0.60455  2.39869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.88077    0.09180   20.49   <2e-16 ***\nx            3.05893    0.07608   40.20   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9163 on 98 degrees of freedom\nMultiple R-squared:  0.9428,\tAdjusted R-squared:  0.9423 \nF-statistic:  1616 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nSo we can say that on average our $\\hat{\\beta_0}$s are 0.091 differ from $\\beta_0$, and our $\\hat{\\beta_1}$s differ 0.076 from $\\beta_1$. To make more sense of it we can calculate the confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_0_hat_data - 1.96 * se_beta_0_data, \"and\", beta_0_hat_data + 1.96 * se_beta_0_data )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBy 95% confidence we can say that the true beta_0 is between 1.700846 and 2.060698\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"By 95% confidence we can say that the true beta_0 is between\", beta_1_hat_data - 1.96 * se_beta_1_data, \"and\", beta_1_hat_data + 1.96 * se_beta_1_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBy 95% confidence we can say that the true beta_0 is between 2.909799 and 3.208051\n```\n:::\n:::\n\n\nLets confirm this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm(y~x,data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               2.5 %   97.5 %\n(Intercept) 1.698600 2.062944\nx           2.907938 3.209912\n```\n:::\n:::\n\n\nSince standard error tells us the range of the $\\beta$ values via confidence interval, we can infer that if this range does not include 0, than our $\\beta$ values are statistically significant; x is assocaited with y.\n\nOr we can use standard erros to perform *hypothesis tests* on the coefficients. We usually don't care about the intercept, so lets do the hypothesis test on only $\\hat{\\beta_1}$.\n\n$$\n\\begin{align}\nH_0 &: \\beta_1 = 0 \\to \\text{there is no relationship between X and Y} \\\\\nH_1 &: \\beta_1 \\neq 0 \\to \\text{there is some relationship between X and Y}\n\\end{align}\n$$ If the *null-hypothesis* is true =\\> \\$ \\beta\\_1 = 0\\$ =\\> $Y = \\beta_0 + \\epsilon$ =\\> $X$ is not associated with $Y$.\n\nTo test the null-hypothessi, we need to determine whether our estimate $\\hat{\\beta_1}$ is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. How far is enough? This depends on the accuracy of $\\hat{\\beta_1}$--that is it depends on $\\text{SE}(\\hat{\\beta_1})$. If $\\text{SE}(\\hat{\\beta_1})$ is small, then even relatively small values of $\\hat{\\beta_1}$ may provide strong evidence that $\\beta_1 \\neq 0$. If $\\text{SE}(\\hat{\\beta_1})$ is large, then $\\hat{\\beta_1}$ must be large in absolute value in order for us to reject the null hypothessis. In practice we compute a *t-statistic* given by\n\n$$\nt = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}\n$$ (3.14)\n\nwhich measures the number of standard deviations that $\\hat{\\beta_1}$ is away from zero. From the t-statistic we can compute the *p-value*; a small p value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in absence of any real association between the predictor and the response. So if p value is small we infer that there is assocaition between the predictor and the response =\\> we reject the null hypothesis. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1%. When $n=30$ these correspond to tstatsitcs of around 2 and 2.75.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,\tAdjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nHere we see that t statistics are very high, and p values are very low =\\> reject the null hypothesis for both $\\beta$ values; they are statistically significant.\n\n### Assessing the Accuracy of the Model\n\nOnce we concluded the statistically significant variable--rejecting the null hypothesis, we want to quantify *the extend to which the model fits the data*. We can use either\n\n-   *Residual standard error*\n-   $R^2$\n\n*Residual standard error*\n\nRecall from $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$ that associated with each observation is an error term $\\epsilon$. Because of these error terms even if we knew the true regression line, we would not be able to predict $Y$ from $X$. The *RSE* is an estimate of the standard deviation of $\\epsilon$. It is the average amount that the response will deviate from the true regression line, computed by\n\n$$\n\\begin{align}\n\\text{RSE} &= \\sqrt{\\frac{1}{n-2}\\text{RSS}} \\\\\n&= \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i - \\hat{y_i})^2}\n\\end{align}\n$$ (3.15)\n\nIn the advertising data, RSE was 3.26; actual sales in each market deviate from the true regression line by approximately 3,269 units, on average. This also means that; if the model were correct and the true values of the unknown coefficients $\\beta_0$ and $\\beta_1$ were known exaclty, any predcition of sales on the basis of TV advertising would still be off by about 3,260 units on average. Is this prediction error accaptable? Depends on the data: in the `advertising` data set the mean value of `sales` is $\\approx 14,000$ units, and so the percentage error is $3,260 / 14,000 = 23%$.\n\nThe RSE is considered a measure of the *lack of fit* of the model $Y=\\beta_0 + \\beta_1 + \\epsilon$ to the data. If the predictions from the model are very close to the true outcome values--$\\hat{y_i} \\approx y_i$ then RSE will be small, and we can concldue that the model fits the data very well. Otherwise, if $\\hat{y_i}$ is very far from $y_i$ then RSE may be quite large, indicating the model doesn't fit the data well.\n\n$R^2$**Statistic**\n\nThe RSE provides an absolute measure of lack of fit of the model to the data. $R^2$ provides an alternative measure of fit. It takes the form of a *proportion*--the proportion of variance explained--and so its always $0\\leq R^2 \\leq 1$ and is independent of the scale of $Y$--as opposed to RSE.\n\n$$\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n$$ (3.17)\n\nwhere $\\text{TSS} = \\sum(y_i - \\bar{y})^2$ is the *total sun of squares* and $\\text{RSS} = \\sum(y_i - \\hat{y_i})^2$. TSS measures the total variaance in the response $Y$; and can be thought of as the amount of varaiblity ingerent in the response before the regression is performed. RSS measures the amount of varaiblity that is left unexplained after performing the regression. So TSS - RSS measures the amount of variability in the response that is explained by performing the regression, and $R^2$ measures the *proportion of variability in* $Y$ *that can be explained using* $X$. As $R^2$ gets closer to 1, a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variablity in the response; this might occur because the linear model is wrong, or the inherit error $\\sigma^2 = \\text{RSE}^2$ is high, or both.\n\nLets calculate $R^2$ of our estimation on `advertising` data with the model $\\hat{sales_i} = \\hat{\\beta_0} + \\hat{\\beta_1}TV_i$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvertising %>% \n  mutate(sales_hat = beta_0_hat_adv + beta_1_hat_adv * TV) -> advertising\nadvertising\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 × 5\n      TV radio newspaper sales sales_hat\n   <dbl> <dbl>     <dbl> <dbl>     <dbl>\n 1 230.   37.8      69.2  22.1     18.0 \n 2  44.5  39.3      45.1  10.4      9.15\n 3  17.2  45.9      69.3   9.3      7.85\n 4 152.   41.3      58.5  18.5     14.2 \n 5 181.   10.8      58.4  12.9     15.6 \n 6   8.7  48.9      75     7.2      7.45\n 7  57.5  32.8      23.5  11.8      9.77\n 8 120.   19.6      11.6  13.2     12.7 \n 9   8.6   2.1       1     4.8      7.44\n10 200.    2.6      21.2  10.6     16.5 \n# ℹ 190 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSS = sum((advertising$sales - advertising$sales_hat)^2)\nRSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2102.531\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSE = sqrt(RSS/(length(advertising$sales) -2))\nRSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.258656\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTSS = sum((advertising$sales - mean(advertising$sales))^2)\nTSS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5417.149\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nR2 = (TSS - RSS) / TSS\nR2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6118751\n```\n:::\n:::\n\n\n61% of the variablity in `sales` is explained by a linear regression on `TV`.\n\nwhat about our `data`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 3\n        y      x  y_hat\n    <dbl>  <dbl>  <dbl>\n 1 -0.591 -0.667 -0.159\n 2  2.69   0.222  2.56 \n 3 -2.61  -1.03  -1.27 \n 4 -3.54  -1.39  -2.38 \n 5  1.54  -0.545  0.212\n 6  2.22   0.384  3.05 \n 7 -1.34  -1.56  -2.88 \n 8  6.81   1.39   6.14 \n 9  6.26   1.43   6.27 \n10  2.39   0.465  3.30 \n# ℹ 90 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSS_data = sum((data$y - data$y_hat)^2)\nRSS_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 82.28514\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRSE_data = sqrt(RSS_data/(length(data$y) -2))\nRSE_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9163211\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTSS_data = sum((data$y - mean(data$y))^2)\nTSS_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1439.473\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_data = (TSS_data - RSS_data) / TSS_data\nR2_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9428366\n```\n:::\n:::\n\n\n94% of the variablity in `y` is explained by `x`; very good fit of the model.\n\n$R^2$ is better to interpret than RSE.\n\n## Multiple Linear Regression\n\nIn practice we have more than one predictor to explain $Y$.\n\nHow can we extend our analysis of the advertising order to accomodate the other two (`radio` and `newspaper`) additional predictors?\n\n=\\> We can run three separate simple linear regressions, each of which uses a different advertising medium as a predictor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,\tAdjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ radio, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332,\tAdjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,\tAdjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n```\n:::\n:::\n\n\nWe find that on average, \\$1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\n\nWe find that on average, \\$1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\n\nWe find that on average, \\$1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\n\n**However** this approach is not good. First of all it is unclear to make a sinlge prediction of sales given levesl of the three advertising media budgets, since each has their own regression equation. Second, each of these three regression equations ignores the other two medi in forming estimates for the regression coefficients. Especially if these media budgets are correalted, this can lead to very misleading estimates of the individaul media effects on sales.\n\nInstead of the seperate linear regressions for each predictor, better approach is to extend the simple linear regression setting $Y = \\beta_0 + \\beta_1 X$ to\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon\n$$\n\n(3.19)\n\nHere we interpret $B_j$ as the average effect of a one unit increase in $X_j$, *holding all other predictors fixed*.\n\nFor the advertising example;\n\n$$\nsales = \\beta_0 + \\beta_1 TV + \\beta_2 radio + \\beta_3 newspaper + \\epsilon\n$$\n\n### Estimating the Regression Coefficients\n\nAgain, regression coefficients in (3.19) are unknown, and must be estimated from the data. And with these estimates we can make predictions\n\n$$\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\dots + \\hat{\\beta_p}x_p\n$$ (3.21)\n\nThe parameters are estimated using the same least squares approach with simple linear regression. We choose $\\beta_0, \\beta_1, \\dots, \\beta_p$ to minimize the sum of squared residuals\n\n\\$\\$ \\begin{align}\n\n\\text{RSS} &= \\sum_{i = 1}^n(y_i - \\hat{y_i})^2 \\\\\n&= \\sum_{i = 1}^n(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_{i1} - \\beta_2x_{i2} - \\dots - \\beta_px_{ip})^2\n\n\\end{align} \\$\\$ (3.22)\n\n$\\hat{\\beta_0}, \\hat{\\beta_1},\\dots, \\hat{\\beta_p}$ values minimize RSS.\n\nWe are not going to calculate these estimates with our hands, R does that.\n\nLets see our model results with the three predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n*Interpretation:* for a given amount of Tv and newspaper advertising, spending additional \\$1,000 on radio(TV)(newspaper) advertising leads to an increase in sales approximately by 189(46)(-1) units.\n\nIf we compare these effects with one predictor regressions\n\n> We find that on average, \\$1,000 increase in spending on radio advertising is associated with an increase in sales by around 203 units.\n\n> We find that on average, \\$1,000 increase in spending on newspaper advertising is associated with an increase in sales by around 55 units.\n\n> We find that on average, \\$1,000 increase in spending on TV advertising is associated with an increase in sales by around 47 units.\n\nFor tv and radio coefficients are similar, but for **newspaper**: from the simple linear regression coefficient of newspaper was significant, but in multiple linear regression it is not; p value is very high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm(sales ~ TV + radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n```\n:::\n:::\n\n\nIts confidence interval contains 0.\n\nThis difference between simple linear regression and multiple linear regression coefficients stems from the fact that in the simple regression, the slope term represents the average effet of a one dollar increase in newspaper advertising, ignoring other preditors such as tv and radio. In contrsat, in the multiple regression setting, the coefficient for newspaper represents the average effect of increeasing newapper spending by one dollar, while holding tv and radio fixed.\n\nDoes it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite? Yes!\n\nTake a look at this correlation matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(advertising[1:4])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  TV      radio  newspaper     sales\nTV        1.00000000 0.05480866 0.05664787 0.7822244\nradio     0.05480866 1.00000000 0.35410375 0.5762226\nnewspaper 0.05664787 0.35410375 1.00000000 0.2282990\nsales     0.78222442 0.57622257 0.22829903 1.0000000\n```\n:::\n:::\n\n\nWe can also make it a plot out of this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot(cor(advertising[1:4]), method = \"number\")\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nNotice that correlation between radio and newspaper is 0.35. This reveals a tendency to spend more on newspaper advertising in markets where more is spent on radio advertising. Now suppose the multiple regression is correct and newspaper advertising has no direct impact on sales, but radio advertising does increase sales. Then in markets where we spend more on radio, our sales will tend to be higher, adn as our correaltion matrix shows, we also tend to spend more on newspaper advertising in those same markets. Hence, in a simple linaer regresion which only examines sales vs newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising does not actually affect sales. So newspaper sales are proxy for radio advertising; newspaper gets credit for the effect of radio on sales.\n\nThis is a very common issue. Consider running a regression of shark attack versus ice cream sales for data collected at a given beach community. We would see a positive relationship, similar to that seen between sales and newspaper. Of course ice creams doesnt cause shark attacks. In reality higher temperatures cause more people to visit the beach in trun results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature revals that, the former predictr is no longer significant after adjusting for temperature.\n\n### Some important Questions\n\nWhen we perform MLR, we usually are interested answering a few important questions\n\n1.  *Is at least one of the predictors* $x_1, x_2, \\dots, x_p$ *useful in predicting the response?*\n\n2.  *Do all predictors help to explain* $Y$, *or is only a subset of the predictors useful?*\n\n3.  *How well does the model fit the data?*\n\n4 *Given a set of predictor values, what response value should we predict, and how accurate is our prediction?*\n\nLets answer these questions:\n\n1.  *Is at least one of the predictors* $x_1, x_2, \\dots, x_p$ *useful in predicting the response?*\n\n    In SLR we simply checked whether $\\beta_1 = 0$ or not. In MLR, we need to ask whether all of the regression coefficients are zero $\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$. So our null hypothesis is\n\n    $$\n     \\begin{align}\n     H_0 &: \\beta_1 = \\beta_2 = \\dots = \\beta_o = 0 \\\\\n     H_\\alpha &: \\text{at least one} \\space B_j \\space \\text{is non-zero}\n     \\end{align}\n     $$ This hypothesis test is performed by computing the *F-statistic*,\n\n    $$\n     F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\n     $$ (3.23)\n\n    So, if there is no relationship between the resposne and predictors, we expect F-statistic to take on value close to 1. if $H_\\alpha$ is true then $F$ should be greater than 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nF statistic is 570 and is far from 1. But it is best to have a look at the p-value of the F statistic which is also very small.\n\nThis means that at least one of the media is associated with increase sales.\n\nIn (3.23) we are testing $H_0$ that all the coefficients are zero. Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponesd to a null hypothesis\n\n$$\n  H_0 : \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_p\n  $$\n\nIn this case we fit a second model that uses all the variables *except* those last $q$. suppose that residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is\n\n$$\n  F = \\frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}\n  $$ (3.24)\n\nOn advertising MLR we saw that newspaper is not significant from its p value. Then why do we need to look at the overall F-statistic? after all, it seems likely that if any one of the p-values for the individual variables is very small, then *at least one of the predictors is realted to the respose*. This is not true usually, especially when $p$ is large.\n\nSo after estimating the model first look at the F-statistic, than to the individual t statistic p values.\n\n2.  *Do all predictors help to explain* $Y$, *or is only a subset of the predictors useful?* =\\> **Deciding on important variables**\n\n    After lookig at the F statistic, we can look at the individual p values. But if \\*p\\$ is large, we are going to make false discoveries.\n\n    Usually not all predictors are associated with the response. This task of determining which predictors are associated with the response in order to fit a single model involving only those predictors is refered to as *variable selection*. Check out Chapter 6 for more detail. But here is a breif outline of some of the classical approaches.\n\n    Ideally we want to perform variable selection by trying out a lot of different models, each containing different subset of the predictors. For instance if our $p=2$ then we can consider four models\n\n    -   \n\n        (1) a model containing no variables\n\n    -   \n\n        (2) a model containing $x_1$ only\n\n    -   \n\n        (3) a model containnig $x_2$ only\n\n    -   \n\n        (4) a model containing $x1$ and $x_2$.\n\n    We can then select the *best* model out of all the models by looking at some statistics we can use to judge the quality of the model. These are\n\n    -   *Mallow*'s $C_p$\n    -   *Akaike information creterion* (AIC)\n    -   *Bayesian information criterion*(BIC)\n    -   *adjusted* $R^2$\n\n    These are discussed in more detail in chapter 6.\n\n    We can also determine which model is the best by plotting various model outputs, such as the residuals, in order to search for patterns.\n\n    But we cannot consider all models, especially when $p$ is high. There are three classical approaches for this task:\n\n    -   *Forward selection*\n        -   begin with *null model* a model that contains an intercept but no predictors.\n\n        -   Then fit *p* simple linear regressions and add to the null model the variable that results in the lowest RSS.\n\n        -   Then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n    -   *Backward selection*\n        -   Put all varaibles in the model.\n        -   remove the least statistically significant predictor.\n        -   estimate the new regression with $p-1$ variable, remove the largest p-value predictor. This procedure continues until a stopping rule is reached =\\> stop after all remaining variables have p value \\< 0.02\n    -   *Mixed selection*\n        -   Combination of forward selection and backward selection\n        -   Start with no variables in the model\n        -   add the varaible that provides the best fit\n        -   add varaibles one-by-one\n        -   at one point if the p-value for one of the variables in the model rises above a certain treshold, then we remove that variabel from the model.\n        -   Continue untill all variables have sufficiently low p value, and all vairables in the model woudl have a large p-value if added to the model\n\n    Backwar slecetion cannot be used if $p>n$, forward selection can always be used.\n\n3.  *How well does the model fit the data?* **Model Fit**\n\nTwo of the most common numerical measures of model fit are RSE and $R^2$.\n\nIn SLR $R^2$ is equal to $cor(Y,X)$. In MLR $R^2 = cor(Y,\\hat{Y})$.\n\n$R^2$ will always increase as you add more variable, even though that varaible is not statistically significant. This is because adding another variable must allow us to fit the trainig data(not necessarly test data) more accurately. But this increase in $R^2$ after adding a statistically not-significant varible is very low =\\> evidence that you can drop the not significant variable. Check out the $R^2$ variables of the following models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.92110    0.29449   9.919   <2e-16 ***\nTV           0.04575    0.00139  32.909   <2e-16 ***\nradio        0.18799    0.00804  23.382   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThey are almost the same.\n\nBut lets see the $R^2$ of the model containing only Tv\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,\tAdjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nit is 0.611.\n\nIf we add radio\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.92110    0.29449   9.919   <2e-16 ***\nTV           0.04575    0.00139  32.909   <2e-16 ***\nradio        0.18799    0.00804  23.382   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIt increaes dramatically. This implies that model that uses TV and radio to predict sales is better than only using Tv. also radio is statistically signifiacnt.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6231 -1.7346 -0.0948  1.8926  8.4512 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5.774948   0.525338  10.993  < 2e-16 ***\nTV          0.046901   0.002581  18.173  < 2e-16 ***\nnewspaper   0.044219   0.010174   4.346 2.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.121 on 197 degrees of freedom\nMultiple R-squared:  0.6458,\tAdjusted R-squared:  0.6422 \nF-statistic: 179.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNot with newspaper though.\n\nOr the opposite\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ radio, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ radio, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332,\tAdjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales~radio+TV, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ radio + TV, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.92110    0.29449   9.919   <2e-16 ***\nradio        0.18799    0.00804  23.382   <2e-16 ***\nTV           0.04575    0.00139  32.909   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ radio + newspaper, data = advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5289  -2.1449   0.7315   2.7657   7.9751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 9.188920   0.627672  14.640   <2e-16 ***\nradio       0.199045   0.021870   9.101   <2e-16 ***\nnewspaper   0.006644   0.014909   0.446    0.656    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.284 on 197 degrees of freedom\nMultiple R-squared:  0.3327,\tAdjusted R-squared:  0.3259 \nF-statistic: 49.11 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWhat about RSE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.92110    0.29449   9.919   <2e-16 ***\nTV           0.04575    0.00139  32.909   <2e-16 ***\nradio        0.18799    0.00804  23.382   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(sales ~ TV + radio + newspaper, advertising))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,\tAdjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nadding newspaper increased the RSE =\\> no need to add newspaper. Adding newspaper increases RSE because\n\n$$\nRSE = \\sqrt{\\frac{1}{n-p-1}RSS}\n$$\n\nModels with more variables can have higher RSE if the decrease in RSS is small relative to the increase in $p$.\n\nSo we can look at both the RSE and $R^2$.\n\n**Four: Predictions**\n\nAfter fitting the model we can predict $Y$ => $\\hat{y}$ with estimated coefficients. However, there are three sorts of uncertainty associated with this prediction:\n\n1. The coefficient esstimates $\\hat{\\beta_0}, \\hat{\\beta_1},\\dots,\\hat{\\beta_p}$ are estimates for $\\beta_0, \\beta_1, \\dots, \\beta_p$:\n\n    That is, the *least squares plane*\n\n$$\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\dots +  \\hat{\\beta_p}x_p\n$$\n\n  which is only an estimate for the *true population regression plane*\n$$\nf(X) = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p\n$$\n\nSo there is an inaccuracy in the coefficient estimates => this is the *reducible error* from Chapter 2. We can compute a *confidence interval* to determine how close $\\hat{y}$ will be to $f(X)$.\n\n2. Assuming a linear model for $f(X)$ is almost always an aaproximation of reality (usually relationships are not linear), so ther is an additional source of potentially reducible error => this is the *model bias*.\n\n    When we are using a linear model, we are in fact estimating the best linear approximation to the true surface. However, we will ignore this discrepancy and operate as if the linear model is correct\n    \n3. Even if we knew $f(X)$--that is even if we knew the true values of $\\beta$--the response value cannot be predicted perfectly because of the random error $\\epsilon$ in the model => *irreducable error*. How much will $Y$ vary from $\\hat{y}$ => we use *prediction intervals* to answer this question. \n\n    Predicion intervals are always wider than confidence intervals, because they contain both the *reducible error*(error from estimating coefficients of $f(X)$) and irreducible error.\n    \n\nWe use a *confidence interval* to quantify the uncertainty surrounding the *average* `sales` over a large number of cities. For example given that \\$100,000 is spent on `TV` advertising and \\$20,000 is spent on `radio` advertising in each city, the 95% confidence interval is $[10,985, 11,528]$. We interpret this to mean that 95% of intervals of this form will contain the true value of $f(X)$. \n\nOn the other hand, *a prediction interval* can be used to quantify the uncertainty surrounding `sales` for a *particular* city. Given that \\$100,000 is spent on `TV` advertising and \\$20,000 is spent on `radio` advertising in that city the 95% prediction interval is $[7,930, 14,580]$. We interpret this to mean that 95% of intervals of this form will contain the true value of $Y$ for this city. Note that both intervals are centered at 11,256, but that the prediction intervaş is substantially wider than the confidence interval, reflecting the increased uncertainty about `sales` for a given city in comparison to the average `sales` over many locations.\n\n## Other Considerations in the Regression Model\n\n### Qualitative Predictors\n\nIn practice not all variables are *quantitative*; some predictors are *qualitative*.\n\nCheck out the `Credit` data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit = read.csv(\"./data/Credit.csv\") %>% as_tibble\nCredit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 400 × 12\n      ID Income Limit Rating Cards   Age Education Gender   Student Married\n   <int>  <dbl> <int>  <int> <int> <int>     <int> <chr>    <chr>   <chr>  \n 1     1   14.9  3606    283     2    34        11 \" Male\"  No      Yes    \n 2     2  106.   6645    483     3    82        15 \"Female\" Yes     Yes    \n 3     3  105.   7075    514     4    71        11 \" Male\"  No      No     \n 4     4  149.   9504    681     3    36        11 \"Female\" No      No     \n 5     5   55.9  4897    357     2    68        16 \" Male\"  No      Yes    \n 6     6   80.2  8047    569     4    77        10 \" Male\"  No      No     \n 7     7   21.0  3388    259     2    37        12 \"Female\" No      No     \n 8     8   71.4  7114    512     2    87         9 \" Male\"  No      No     \n 9     9   15.1  3300    266     5    66        13 \"Female\" No      No     \n10    10   71.1  6819    491     3    41        19 \"Female\" Yes     Yes    \n# ℹ 390 more rows\n# ℹ 2 more variables: Ethnicity <chr>, Balance <int>\n```\n:::\n:::\n\n\nLets do a scatterplot of all variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% select(where(is.numeric)) %>% pairs(.)\n```\n\n::: {.cell-output-display}\n![Fig 3.6 The credit data set contains infortmation about blaance, age, cards, education income, limit and rating for a number of potential customers](Chapter3_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n:::\n\n\n**Predictors with Only Two Levels**\n\nWe want to investigate differences in credit card balance between maels and females, ignoring other variables for the moment. If a qualitative predictors (also known as *factor*) only has two *levels*, then incorporating it into a regression model is very simple. We create a *dummy variable* that takes on two possible *numerical* values. For example based on `Gender` varaible, we can create a new varaible that takes the form\n\n$$\nx_i = \n\\begin{cases}\n1 & \\text{if}\\space i\\text{th} \\space\\text{person is female} \\\\\n0 & \\text{if}\\space i\\text{th} \\space\\text{person is male}\n\\end{cases}\n$$\n(3.26)\n\nand use this variable as a predictor in the regression equation. This results in the model\n\n$$\nY_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & i\\text{th} \\space\\text{person is female} \\\\\n\\beta_0 + \\epsilon_i & i\\text{th} \\space\\text{person is male}\n\\end{cases}\n$$\n(3.27)\n\nNow $\\beta_0$ can be interpreted as the average credit card balance among males, $\\beta_0 + \\beta_1$ as the average credit card among females, and $\\beta_1$ as the average difference in credit card balance between females and males.\n\nHere is the regression results:\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit$Gender <- factor(Credit$Gender, levels = c(\" Male\",\"Female\"))\nlinear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") %>% \n  fit(Balance ~ Gender, Credit) %>% \n  pluck(\"fit\") %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = Balance ~ Gender, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    509.80      33.13  15.389   <2e-16 ***\nGenderFemale    19.73      46.05   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611,\tAdjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n```\n:::\n:::\n\n\nR converts all *\"Female\"* values to 1 automatically. If we were to do this manually\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% \n  mutate(Gender = as.character(Gender)) %>% \n  mutate(Gender = ifelse(Gender == \"Female\",1,0)) %>% \n  lm(Balance ~ Gender, .) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   509.80      33.13  15.389   <2e-16 ***\nGender         19.73      46.05   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611,\tAdjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n```\n:::\n:::\n\n\nThis means that average credit card debt for males is estimated to be \\$509.80, whereas females are estimated to carry \\$19.73 in additional debt for a total of $\\$509.80 + \\$19.73 = \\$529.53$. However, the coefficient of the dummy variable is not significant; there is no statistical evidence of a difference in average credit card balance between the genders.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% \n  ggplot() + aes(x=Gender, y =Balance, fill = Gender) + geom_boxplot(show.legend = F) + geom_point(data =(Credit %>% group_by(Gender) %>% summarise(Balance = mean(Balance))),shape = 4, show.legend = F) + theme_clean()\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n\nThe desicion to code females as 1 and males as 0 in (3.27) is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we had coded males as 1 and females as 0:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% \n  mutate(Gender = factor(Gender, levels = c(\"Female\", \" Male\"))) %>% \n  lm(Balance ~ Gender,.) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   529.54      31.99  16.554   <2e-16 ***\nGender Male   -19.73      46.05  -0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611,\tAdjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n```\n:::\n:::\n\nThen we would say that estimated average debt for females is \\$529.54, and for males is $\\$529.52 - \\$19.73 = \\$509.80$.\n\nAlternatively, instead of 0/1 coding scheme, we could create a dummy variable\n\n$$\nx_i = \n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is female} \\\\\n-1 & \\text{if}\\space i\\text{th}\\space \\text{person is male}\n\\end{cases}\n$$\nand use this variable in the regression equation. This results in the model\n\n$$\nY_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is female} \\\\\n\\beta_0 - \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is male} \n\\end{cases}\n$$\nNow $\\beta_0$ can be interpreted as the overall average credit card balance (ignoring the gender effect), and $\\beta_1$ is the amount that females are above the average and males are below the average.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% \n  mutate(Gender = as.character(Gender)) %>% \n  mutate(Gender = ifelse(Gender == \"Female\",1,-1)) %>% \n  lm(Balance ~ Gender,.) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Balance ~ Gender, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-529.54 -455.35  -60.17  334.71 1489.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  519.670     23.026  22.569   <2e-16 ***\nGender         9.867     23.026   0.429    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.2 on 398 degrees of freedom\nMultiple R-squared:  0.0004611,\tAdjusted R-squared:  -0.00205 \nF-statistic: 0.1836 on 1 and 398 DF,  p-value: 0.6685\n```\n:::\n:::\n\n\nNow $\\beta_0$ is \\$ 519.670 which is the halfway between the male and female averages of \\$509.80 and \\$529.53. The estimate for $\\beta_1$ is \\$9.865, which is half of \\$19.74, the average difference between females and males. \n\n**Qualitative Predictors with More than Two levels**\n\nIn this case we need to create an additional dummy. For example have a look at the `Ethnicity` variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% select(Ethnicity) %>% unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n  Ethnicity       \n  <chr>           \n1 Caucasian       \n2 Asian           \n3 African American\n```\n:::\n:::\n\n\nHas three possible values. Then we need to create two dummies\n\n$$\nx_{i1} = \n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is Asian} \\\\\n0 & \\text{if}\\space i\\text{th}\\space \\text{person is not Asian} \n\\end{cases}\n$$\n\nand \n\n$$\nx_{i2} = \n\\begin{cases}\n1 & \\text{if}\\space i\\text{th}\\space \\text{person is Caucasian} \\\\\n0 & \\text{if}\\space i\\text{th}\\space \\text{person is not Caucasian} \n\\end{cases}\n$$\n\nThen both these varaibles can be used in the regression equation, in order to obtain the model\n\n$$\nY_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i = \n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is Asian} \\\\\n\\beta_0 + \\beta_2 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is Caucasian} \\\\\n\\beta_0 + \\epsilon_i & \\text{if}\\space i\\text{th}\\space \\text{person is African American} \n\\end{cases}\n$$\nNow $\\beta_0$ can be interpreted as the average credit card balance for African Americans, $\\beta_1$ can be interpreted as the difference in the average balance between Asian and African american categories, and $\\beta_2$ can be interpreted as the difference in average balance between the Caucasian and African American categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") %>% \n  fit(Balance ~ Ethnicity, Credit) %>% \n  pluck(\"fit\") %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = Balance ~ Ethnicity, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-531.00 -457.08  -63.25  339.25 1480.50 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          531.00      46.32  11.464   <2e-16 ***\nEthnicityAsian       -18.69      65.02  -0.287    0.774    \nEthnicityCaucasian   -12.50      56.68  -0.221    0.826    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.9 on 397 degrees of freedom\nMultiple R-squared:  0.0002188,\tAdjusted R-squared:  -0.004818 \nF-statistic: 0.04344 on 2 and 397 DF,  p-value: 0.9575\n```\n:::\n:::\n\n\nThere will always be one fewer dummy variable than the number of levels. The level with no dummy variable--African American in this example--is known as the *baseline*.\n\nFrom the regression results we see that the estimated `balance` for the baseline, African American, is \\$531.00. It is estimated that Asian category will have \\$18.69 less debt than the African American category on average, and the Caucasian category will have \\$12.50 less debt that the African American category. However, p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities. The coefficients will change bassed on the baseline and coding.\n\nRather than relying on the individual coefficients, we can use F-test to test $H_0 : \\beta_1 = \\beta_2 = 0$; this does not depend on the coding. The F-test has a p-value of 0.9575, indicating that we cannot reject the null hypothesis that there is no relationship between `balance` and `ethnicity`.\n\nUsing this dummy variable approach presents no difficulties when using both quantitative and qualitative predictors. For example, to regress `balance` on both a quantitative varaible such as `income` and a qualitative variable such as `student`, we must simply create a dummy varaible for `student` and then fit a multiple linear regression model using `income` and the dummy variable as the predictors for credit card balance.\n\n### Extensions of the Linear Model\n\nLinear regression model is very interpretable and works quite well on many real-world problems. However, it makes several highly restrictive assumptions that are ofthen violated in practice. Two of them ost important assumptions state that the relationship between the predictors and response are *additive* and *linear*. \n\nThe *additive* assumption means that the effect of changes in a predictor $x_j$ on the response $Y$ is independent of the vlaues of the other predictors.\n\nThe *linear* assumption means thatt the change in the response $Y$ due to one-unit change in $x_j$ is constant, regardless of the vlaue of $x_j$. \n\nWe can relax these assumptions. Here some classical approaches to do that\n\n**Removing the Additive Assumption**\n\nFrom `Advertising` data, we concluded that both `TV` and `radio` seem to be associated with `sales`. Our model was linear; the effect of `TV` and `radio` advertising spending on sales is independent of each other, and their effect is constant no matter the level of spending. \n\n$$\nY_i = \\beta_0 + \\beta_1 TV_i + \\beta_2 radio_i + \\epsilon_i\n$$\nthis model means that, the average effect on sales of a one unit increase in tv is always $\\beta_1$ regardless of the amount spent on radio.\n\nHowever, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for `TV` should increase as `radio` increases. In this situation, given a fixed budget \\$100,000, spending half on `radio` and half on `TV` may increase `sales` more than allocating the entire amount to either `TV` or to `radio`. In marketing this is known as the *synergy effect*, in statistics *interaction effect*.\n\nConsider the standard linear regression with two variables,\n\n$$\nY = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\n$$\nHere if we increaes $x_1$ by one unit, $Y$ will increase by an average of $\\beta_1$ units. The presence of $x_2$ does not alter this statement-regardless of the value of $x_2$, a one-unit increase in $x_1$ will lead to $\\beta_1$ unit increae in $Y$.\n\nWe can extend this model by allowing interaction effects by including a third predictor, called an *interaction term*, which is constructed by computing the product of $x_1$ and $x_2$:\n\n$$\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\n$$\n(3.31)\n\nWe can write this equation\n\n$$\n\\begin{align}\nY &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2x_2 +\\epsilon \\\\\n&= \\beta_0 + \\tilde{\\beta_1}x_1 + \\beta_2x_2 + \\epsilon\n\\end{align}\n$$\nwhere $\\tilde{\\beta_1} = \\beta_1 + \\beta_3x_2$. Since $\\tilde{\\beta_1}$ changes with $x_2$, the effect of $x_1$ on $Y$ is no longer constant: adjusting $x_2$ will change the impact of $x_1$ on $Y$.\n\n::: {.callout-note}\n# Productivy of a factory\n\nSuppose that we are interested in studying the productivity of a factory. We want to predict the number of `units` produced on the basis of the number of production `lines` and the total number of `workers`. Probably the effect of increasing `lines` on `units` produced will depend on the number of `workers`, since if no workers are available to operate the lines, then increasing the number of lines will not increase the production. This suggest to include an interaction term between `lines` and `workers` in a linear model to predict `units`.\n\nSuppose when we fit the model, we obtain\n\n$$\n\\begin{align}\nunits &\\approx 1.2 + 3.4 \\times lines + 0.22 \\times workers + 1.4 \\times (lines \\times workers) \\\\\n&= 1.2 + (3.4 + 1.4 \\times workers) \\times lines + 0.22 \\times workers\n\\end{align}\n$$\nAdding an addtional line will increase the number of units produced by $3.4 + 1.4 \\times workers$. Hence, the more `workers` we have, the stronger will be the effect of `lines`.\n\n:::\n\nFor the advitising, a linear model that uses `radio`, `TV` and interaction between the two to predict `sales` takes the form\n\n$$\n\\begin{align}\nsales &= \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times radio + \\beta_3 \\times(radio \\times TV) + \\epsilon \\\\\n&= \\beta_0 + (\\beta_1 + \\beta_3 \\times radio)\\times Tv + \\beta_2 \\times radio + \\epsilon\n\\end{align}\n$$\n(3.33) \nWe can interpret $\\beta_3$ as the increase in the effectiveness of TV advertising for a one unit increse in radio advertising (or vice versa). The coefficients that result from fitting the model (3.33) are\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") %>% \n  fit(sales ~ TV + radio + TV*radio, advertising) %>% \n  pluck(\"fit\") %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = sales ~ TV + radio + TV * radio, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.750e+00  2.479e-01  27.233   <2e-16 ***\nTV          1.910e-02  1.504e-03  12.699   <2e-16 ***\nradio       2.886e-02  8.905e-03   3.241   0.0014 ** \nTV:radio    1.086e-03  5.242e-05  20.727   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,\tAdjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nAdding the interaction term increased the $R^2$ significantly from 0.8972 to 0.9678, RSE from 1.681 to 0.9435.\n\nThe p-value for the interaction term is very low; $H_\\alpha : \\beta_3 \\neq 0$. This means that the relationship is not additive.\n\nTo interpret the coefficients:\n\n\\$1,000 increse in `TV` advertising results with $\\hat{\\beta_1} +\\hat{\\beta_3}\\times radio = \\$19.1 + \\$1.09 \\times radio$ increase in sales on average.\n\n\\$1,000 invrease in `radio` advertising results with $\\hat{\\beta_2} + \\hat{\\beta_3} \\times TV = \\$28.9 + \\$1.09 \\times TV$ increase in sales on average.\n\nAll p-values are significant => all three varibles should be included in the model. \n\nSometimes interaction term has a very small pvalue but the associated main effects(in this case `TV` and `radio`) do not. The *hieararchial principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.*\n\nHere both our varaibles were quantitative. However, the consept of interaction applies to qualitative varaibles, or combination of qualitative and quantitative varibles. Actually interaction between a quantitative and qualitative variable has a particularly nice interpreateion:\n\nConsider the `Credit` data set. Suppose we want to predict `balance` using `income`(quantitative) and `student`(qualitative) varaibles. In the absence of an interaction term the model takes the form\n\n$$\nbalance_i &\\approx \\beta_0 + \\beta_1 \\times income_i + \n\\begin{cases}\n\\beta_2 & \\text{if} \\space i\\text{th} \\space \\text{person is student} \\\\\n0& \\text{if} \\space i\\text{th} \\space \\text{person is not student}\n\\end{cases}\n$$\n$$\nbalance_i = \\beta_1 \\times income + \n\\begin{cases}\n\\beta_0 + \\beta_2 &\\text{if} \\space i\\text{th} \\space \\text{person is student} \\\\\n\\beta_0 &\\text{if} \\space i\\text{th} \\space \\text{person is student}\n\\end{cases}\n$$\n(3.34)\n\nNotice that this amounts to fitting two parallel lines to the data, one for students and one for non-students. The lines for students and non students have different intercepts, $\\beta_0 + \\beta_2$ versus $\\beta_0$, but the slope $\\beta_1$ is the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\") %>% \n  fit(Balance ~ Income + Student, Credit) %>% \n  pluck(\"fit\") %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::lm(formula = Balance ~ Income + Student, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-762.37 -331.38  -45.04  323.60  818.28 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 211.1430    32.4572   6.505 2.34e-10 ***\nIncome        5.9843     0.5566  10.751  < 2e-16 ***\nStudentYes  382.6705    65.3108   5.859 9.78e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 391.8 on 397 degrees of freedom\nMultiple R-squared:  0.2775,\tAdjusted R-squared:  0.2738 \nF-statistic: 76.22 on 2 and 397 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nCredit %>% \n  ggplot() + aes(y=Balance, x=Income) + geom_point(shape =NA) + geom_abline(intercept = 211.1430, slope = 5.9843, color = \"black\", size=1.2) + geom_abline(intercept = 593.8135, slope = 5.9843, color = \"darkred\", size= 1.2) + coord_cartesian(xlim = c(0,150), ylim=c(200,1400)) + scale_y_continuous(breaks = seq(0,1400,400)) + theme_clean() + geom_text(data = tibble(x = c(80,80), y =c(1130,640), z = c(\"Student\",\"non-student\")), inherit.aes = F, aes(x=x, y=y, label=z), angle=25.9)\n```\n\n::: {.cell-output-display}\n![](Chapter3_files/figure-html/unnamed-chunk-80-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Chapter3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}